<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
    <info>
        <title>Boost Asynchronous</title>
        <author>
            <personname>Christophe Henry</personname>
            <email>christophe.j.henry@gmail.com</email>
        </author>
        <copyright>
            <year>2017</year>
            <holder>
                <phrase> Distributed under the Boost Software License, Version 1.0. (See
                    accompanying file LICENSE_1_0.txt or copy at <link
                        xlink:href="http://www.boost.org/LICENSE_1_0.txt"
                        >http://www.boost.org/LICENSE_1_0.txt</link> ) </phrase>
            </holder>
        </copyright>
    </info>
    <preface>
        <title>Introduction</title>
        <para>
            <emphasis role="underline">Note</emphasis>: Asynchronous is not part of the Boost
            library. It is planed to be offered for Review at the beginning in 2017. </para>
        <para>Asynchronous is first of all an architecture tool. It allows organizing a complete
            application into Thread Worlds, each world having the possibility to use the same or
            different threadpools for long-lasting tasks. The library provides an implementation of
            the Active Object pattern, extended to allow many Active Objects to live in the same
            World. It provides several Threadpools and many parallel algorithms making use of it.
            And most important of all, it allows simple, blocking-free asynchronous programming
            based on thread-safe callbacks.</para>
        <para>This is particularly relevant for Designers who often have headaches bringing the
            notion of threads into class diagrams. These usually do not mix well. Asynchronous
            solves this problems: it allows representing a Thread World as a Component or Package,
            objects of this Component or Package living into the corresponding thread.</para>
        <para>Why do we care? Herb Sutter wrote <link
                xlink:href="http://www.gotw.ca/publications/concurrency-ddj.htm">in an
                article</link> "The Free Lunch Is Over", meaning that developpers will be forced to
            learn to develop multi-threaded applications. The reason is that we now get our extra
            power in the form of more cores. The problem is: multithreading is hard! It's full of
            ugly beasts waiting hidden for our mistakes: races, deadlocks, crashes, all kinds of
            subtle timing-dependent bugs. Worse yet, these bugs are hard to find because they are
            never reproducible when we are looking for them, which leaves us with backtrace
            analysis, and this is when we are lucky enough to have a backtrace in the first
            place.</para>
        <para>This is not even the only danger. CPUs are a magnitude faster than memory, I/O
            operations, network communications, which all stall our programms and degrade our
            performance, which means long sessions with coverage or analysis tools.</para>
        <para>Trying to solve these problems with tools of the past (mutexes, programmer-managed
            threads) is a dead-end. It's just too hard. This is where Boost Asynchronous is helping.
            Let us forget what mutexes, atomics and races are!</para>
        <para>There are existing solutions for asynchronous or parallel programming. To name a
            few:</para>
        <para>
            <itemizedlist>
                <listitem>
                    <para>std/boost::async.</para>
                </listitem>
                <listitem>
                    <para>Intel TBB.</para>
                </listitem>
                <listitem>
                    <para>N3428.</para>
                </listitem>
            </itemizedlist>
        </para>
        <para>TBB is a wonderful parallel library. But it's not asynchronous as parallel calls are
            blocking.</para>
        <para>std::async will return a future. But what to do with it? Wait for it? This would be
            synchronous. Collect them and then wait for all? This would also be synchronous. Collect
            them, do something else, then check whether they are ready? This would be wasted
            opportunity for more calculations.</para>
        <para>To solve these problems, NB3428 is an attempt at continuations. Let's have a quick
            look at code using futures and .then (taken from N3428):</para>
        <para>
            <programlisting>future&lt;int> f1 = async([]() { return 123; });
future&lt;string> f2 = f1.then([](future&lt;int> f) {return f.get().to_string();}); // here .get() wonâ€™t block
f2.get(); // just a "small get" at the end?</programlisting>
        </para>
        <para>Saying that there is only a "small get" at the end is, for an application with
            real-time constraints, equivalent to saying at a lockfree conference something like
            "what is all the fuss about? Can't we just add a small lock at the end?". Just try
            it...</para>
        <para>Worse yet, it clutters the code, makes it hard to debug and understand. The author,
            also being the author of Boost Meta State Machine sees no way to use this paradigm with
            state machines.</para>
        <para>Asynchronous supports this programming model too, though it is advised to use it only
            for simple programs, quick prototyping unit tests, or as a step to the more powerful
            tools offered by the library. std::async can be replaced by
            boost::asynchronous::post_future:</para>
        <programlisting>auto pool = boost::asynchronous::make_shared_scheduler_proxy&lt;
                  boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                        boost::asynchronous::lockfree_queue&lt;>>>(8); // create a pool with 8 threads
std::future&lt;int> fu = boost::asynchronous::post_future(pool,
    []()
    {
        return 123;
    });
f1.get();</programlisting>
        <para>Instead of an ugly future.then, Asynchronous supports continuations as coded into the
            task itself. We will see later how to do it. For the moment, here is a quick example.
            Let's say we want to modify a vector in parallel, then reduce it, also in parallel,
            without having to write synchronization points:</para>
        <programlisting>std::future&lt;int> fu = boost::asynchronous::post_future(pool, // pool as before
    [this]()
    {
        return boost::asynchronous::parallel_reduce(                   // reduce will be done in parallel after for
            boost::asynchronous::parallel_for(std::move(this->m_data), // our data, a std::vector&lt;int> will be moved, transformed, then reduced and eventually destroyed
                                              [](int&amp; i)
                                              {
                                                  i += 2;              // transform all elements in parallel
                                              }, 1024),                // cutoff (when to go sequential. Will be explained later)
            [](int const&amp; a, int const&amp; b)                             // reduce function
            {
                return a + b;
            }, 1024);                                                  // reduce cutoff
    });
int res = fu.get();</programlisting>
        <para>But this is just the beginning. It is not really asynchronous. More important, Boost
            Asynchronous is a library which can play a great role in making a thread-correct
            architecture. To achieve this, it offers tools for asynchronous designs: ActiveObject,
            safe callbacks, threadpools, servants, proxies, queues, algorithms, etc. </para>
        <para>Consider the following example showing us why we need an architecture tool:</para>
        <para>
            <programlisting>struct Bad : public boost::signals::trackable
{
   int foo();
};
boost::shared_ptr&lt;Bad> b;
future&lt;int> f = async([b](){return b->foo()});          </programlisting>
        </para>
        <para>Now we have the ugly problem of not knowing in which thread Bad will be destroyed. And
            as it's pretty hard to have a thread-safe destructor, we find ourselves with a race
            condition in it. </para>
        <para>Asynchronous programming has the advantage of allowing to design of code, which is
            nonblocking and single-threaded while still utilizing parallel hardware at full
            capacity. And all this while forgetting what a mutex is. </para>
        <para>This brings us to a central point of Asynchronous: if we build a system with strict
            real-time constraints, there is no such thing as a small blocking get(). We need to be
            able to react to any event in the system in a timely manner. And we can't afford to have
            lots of functions potentially waiting too long everywhere in our code. Therefore,
            .then() is only good for an application of a few hundreds of lines. What about using a
            timed_wait instead? Nope. This just limits the amount of time we waste waiting. Either
            we wait too long before handling an error or result, or we wait not enough and we poll.
            In any case, while waiting, our thread cannot react to other events and wastes
            time.</para>
        <para>An image being more worth than thousand words, the following story will explain in a
            few minutes what Asynchronous is about. Consider some fast-food restaurant:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor1.jpg"/>
                </imageobject>
            </inlinemediaobject>
        </para>
        <para>This restaurant has a single employee, Worker, who delivers burgers through a burger
            queue and drinks. A Customer comes. Then another, who waits until the first customer is
            served.</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>To keep customers happy by reducing waiting time, the restaurant owner hires a second
            employee:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor3.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Unfortunately, this brings chaos in the restaurant. Sometimes, employes fight to get a
            burger to their own customer first:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-RC.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>And sometimes, they stay in each other's way:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-DL.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>This clearly is a not an optimal solution. Not only the additional employee brings
            additional costs, but both employees now spend much more time waiting. It also is not a
            scalable solution if even more customers want to eat because it's lunch-time right now.
            Even worse, as they fight for resources and stay in each other's way, the restaurant now
            serves people less fast than before. Customers flee and the restaurant gets bankrupt. A
            sad story, isn't it? To avoid this, the owner decides to go asynchronous. He keeps a
            single worker, who runs in zero time from cash desk to cash desk:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>The worker never waits because it would increase customer's waiting time. Instead, he
            runs from cash desks to the burger queue, beverage machine using a self-made strategy: <itemizedlist>
                <listitem>
                    <para>ask what the customer wants and keep an up-to-date information of the
                        customer's state.</para>
                </listitem>
                <listitem>
                    <para>if we have another customer at a desk, ask what he wants. For both
                        customers, remember the state of the order (waiting for customer choice,
                        getting food, getting drink, delivering, getting payment, etc.)</para>
                </listitem>
                <listitem>
                    <para>as soon as some new state is detected (customer choice, burger in the
                        queue, drink ready), handle it.</para>
                </listitem>
                <listitem>
                    <para>priorities are defined: start the longest-lasting tasks first, serve
                        angry-looking customers first, etc.</para>
                </listitem>
            </itemizedlist></para>
        <para>The following diagram shows us the busy and really really fast worker in
            action:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Of course the owner needs a worker who runs fast, and has a pretty good memory so he
            can remember what customers are waiting for. </para>
        <para>This is what Asynchronous is for. A worker (thread) runs as long as there are waiting
            customers, following a precisely defined algorithm, and lots of state machines to manage
            the asynchronous behaviour. In case of customers, we could have a state machine: Waiting
            -> PickingMenu -> WaitingForFood -> Paying.</para>
        <para>We also need some queues (Burger queue, Beverage glass positioning) and some
            Asynchronous Operation Processor (for example a threadpool made of workers in the
            kitchen), event of different types (Drinks delivery). Maybe we also want some work
            stealing (someone in the kitchen serving drinks as he has no more burger to prepare. He
            will be slower than the machine, but still bring some time gain).</para>
        <para><emphasis role="bold">To make this work, the worker must not block, never,
                ever</emphasis>. And whatever he's doing has to be as fast as possible, otherwise
            the whole process stalls.</para>
    </preface>
    <part>
        <title>Concepts</title>
        <chapter>
            <title>Related designs: std::async, Active Object, Proactor</title>
            <sect1>
                <title>std::async</title>
                <subtitle>What is wrong with it</subtitle>
                <para>The following code is a classical use of std::async as it can be found in
                    articles, books, etc.</para>
                <programlisting>std::future&lt;int> f = std::async([](){return 42;}); // executes asynchronously
int res = f.get(); // wait for result, block until ready</programlisting>
                <para>It looks simple, easy to use, and everybody can get it. The problem is, well,
                    that it's not really asynchronous. True, our lambda will execute in another
                    thread. Actually, it's not even guaranteed either. But then, what do we do with
                    our future? Do we poll it? Or call get() as in the example? But then we will
                    block, right? And if we block, are we still asynchronous? If we block, we cannot
                    react to any event happening in our system any more, we are unresponsive for a
                    while (are we back to the old times of freezing programs, the old time before
                    threads?). We also probably miss some opportunities to fully use our hardware as
                    we could be doing something more useful at the same time, as in our fast-food
                    example. And diagnostics are looking bad too as we are blocked and cannot
                    deliver any. What is left to us is polling. And if we get more and more futures,
                    do we carry a bag of them with us at any time and check them from time to time?
                    Do we need some functions to, at a given point, wait for all futures or any of
                    them to be ready? </para>
                <para>Wait, yes they exist, <code>wait_for_all</code> and
                    <code>wait_for_any</code>... </para>
                <para>And what about this example from an online documentation?</para>
                <para>
                    <programlisting>{ 
   std::async(std::launch::async, []{ f(); }); 
   std::async(std::launch::async, []{ g(); });
}</programlisting>
                </para>
                <para>Every std::async returns you a future, a particularly mean one which blocks
                    upon destruction. This means that the second line will not execute until f()
                    completes. Now this is not only not asynchronous, it's also much slower than
                    calling sequentially f and g while doing the same.</para>
                <para>No, really, this does not look good. Do we have alternatives?</para>
            </sect1>
            <sect1>
                <title>N3558 / N3650</title>
                <para>Of course it did not go unnoticed that std::async has some limitations. And so
                    do we see some tries to save it instead of giving it up. Usually, it goes around
                    the lines of blocking, but later.</para>
                <para>
                    <programlisting>future&lt;int> f1 = async([]() { return 123; }); 
future&lt;string> f2 = f1.then([](future&lt;int> f) 
{ 
  return f.get().to_string(); // here .get() wonâ€™t block 
});
// and here?
string s= f2.get();</programlisting>
                </para>
                <para>The idea is to make std::async more asynchronous (this already just sounds
                    bad) by adding something (.then) to be called when the asynchronous action
                    finishes. It still does not fly:<itemizedlist>
                        <listitem>
                            <para>at some point, we will have to block, thus ending our asynchronous
                                behavior</para>
                        </listitem>
                        <listitem>
                            <para>This works only for very small programs. Do we imagine a 500k
                                lines program built that way?</para>
                        </listitem>
                    </itemizedlist></para>
                <para>And what about the suggestion of adding new keywords, async and await, as in
                    N3650? Nope. First because, as await suggests, someone will need, at some point,
                    to block waiting. Second because as we have no future, we also lose our polling
                    option.</para>
            </sect1>
            <sect1>
                <title>Active Object</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/ActiveObject.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This simplified diagram shows a possible design variation of an Active Object
                    pattern.</para>
                <para>A thread-unsafe Servant is hidden behind a Proxy, which offers the same
                    members as the Servant itself. This Proxy is called by clients and delivers a
                    future object, which will, at some later point, contain the result of the
                    corresponding member called on the servant. The Proxy packs a MethodRequest
                    corresponding to a Servant call into the ActivationQueue. The Scheduler waits
                    permanently for MethodRequests in the queue, dequeues them, and executes them.
                    As only one scheduler waits for requests, it serializes access to the Servant,
                    thus providing thread-safety.</para>
                <para>However, this pattern presents some liabilities:<itemizedlist>
                        <listitem>
                            <para>Performance overhead: depending on the system, data moving and
                                context switching can be a performance drain.</para>
                        </listitem>
                        <listitem>
                            <para>Memory overhead: for every Servant, a thread has to be created,
                                consuming resources.</para>
                        </listitem>
                        <listitem>
                            <para>Usage: getting a future gets us back to the non-asynchronous
                                behaviour we would like to avoid.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Proactor</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/Proactor.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This is the design pattern behind Boost.Asio. See: <link
                        xlink:href="http://www.boost.org/doc/libs/1_57_0/doc/html/boost_asio/overview/core/async.html"
                        >Boost.Asio documentation</link> for a full explanation. Boost Asynchronous
                    is very similar. It supports enqueueing asynchronous operations and waiting for
                    callbacks, offering extensions: safe callbacks, threadpools, proxies,
                    etc.</para>
            </sect1>
        </chapter>
        <chapter>
            <title>Features of Boost.Asynchronous</title>
            <sect1>
                <title>Thread world</title>
                <subtitle>Extending Active Objects with more servants within a thread
                    context</subtitle>
                <para>A commonly cited drawback of Active Objects is that they are awfully
                    expensive. A thread per object is really a waste of ressources.
                    Boost.Asynchronous extends this concept by allowing an unlimited number of
                    objects to live within a single thread context, thus amortizing the costs. It
                    even provides a way for n Active Objects to share m threads while still being
                    called single thread. This allows tuning thread usage.</para>
                <para>As many objects are potentially living in a thread context, none should be
                    allowed to process long-lasting tasks as it would reduce reactivity of the whole
                    component. In this aspect, Asynchronous' philosophy is closer to a
                    Proactor.</para>
                <para>As long-lasting tasks do happen, Boost.Asynchronous provides several
                    implementations of threadpools and the needed infrastructure to make it safe to
                    post work to threadpools and get aynchronously a safe callback. It also provides
                    safe mechanisms to shutdown Thread worlds and threadpools.</para>
            </sect1>
            <sect1>
                <title>Better Architecture</title>
                <para>We all learned in our design books that a software should be organized into
                    layers. This is, however, easier said than done, single-threaded, but much worse
                    when layers are having their own threads. Let's say, layer A is on top and
                    basing itself on layer B. A creates B and keeps it alive as long as it lives
                    itself. A and B are each composed of hundreds of classes / objects. Our standard
                    communication is A => B, meaning A gives orders to B, which executes them. This
                    is the theory. Unfortunately, B needs to give answers, usually delayed, to A.
                    Unfortunately, A and B live in different threads. This means mutexes. Ouch. Now
                    we are forced to check every class of A and protect it. Worse, the object of A
                    getting an answer might have long been destroyed. Ouch again. What to do? We
                    could keep the object of A alive in the callback of B. But then we have a
                    dependency B -> A. Ouch again, bad design. We can also hide the dependency using
                    some type erasure mechanism. We still have a logical one as B keeps its owner,
                    A, alive. Then, we can use a weak_ptr so that B does not keep A alive. But when
                    we lock, we do keep A alive. It's for a short time, but what if A is shutting
                    down? It's lost, our layered design is broken.</para>
                <para>Asynchronous is more that a library providing a better std::async or some
                    parallel algorithms, it's first of all an architectural tool. In the above case,
                    we will decide that every layer will live in its own thread(s), called
                    schedulers in Asynchronous language. Deciding in which thread an object "lives"
                    is a key point of a good design. Then the top layer, A, will make a request to
                    B, aking a future as a result, or much better, providing a callback.
                    Asynchronous offers a callback safe in two ways: thread-safe and checking the
                    lifetime of the callback target. This callback is provided by
                        <code>make_safe_callback</code>. This simple tool is a major help in making
                    a safe and efficient design.</para>
            </sect1>
            <sect1>
                <title>Shutting down</title>
                <para>Shutting down a thread turns out to be harder in practice than expected, as
                    shown by several posts of surprise on the Boost mailing lists when Boost.Thread
                    tried to match the C++ Standard. Asynchronous hides all these ugly details. What
                    users see is a scheduler proxy object, which can be shared by any number of
                    objects, and running any number of threads, managed by a scheduler. The
                    scheduler proxy object manages the lifetime of the scheduler. </para>
                <para>When the last instance of the scheduler object is destroyed, the scheduler
                    thread is stopped. When the last instance of a scheduler proxy is destroyed, the
                    scheduler thread is joined. It's as simple as that. This makes threads shared
                    objects. </para>
            </sect1>
            <sect1>
                <title>Object lifetime</title>
                <para>There are subtle bugs when living in a multithreaded world. Consider the
                    following class:</para>
                <para>
                    <programlisting>struct Unsafe
{
  void foo()
  {
    m_mutex.lock();
    // call private member
    m_mutex.unlock();
  }
private:
  void foobar()
  {
    //we are already locked when called, do something while locked
  }
  boost::mutex m_mutex;
};            </programlisting>
                </para>
                <para>This is called a thread-safe interface pattern. Public members lock, private
                    do not. Simple enough. Unfortunately, it doesn't fly.</para>
                <para>First one has the risk of deadlock if a private member calls a public one
                    while being called from another public member. If we forget to check one path of
                    execution within a class implementation, we get a deadlock. We'll have to test
                    every single path of execution to prove our code is correct. And this at every
                    commit.</para>
                <para>Usually, for any complex class, where there's a mutex, there is a race or a
                    deadlock...</para>
                <para>But even worse, the principle itself is not correct in C++. It supposes that a
                    class can protect itself. Well, no, it can't. Why? One cannot protect the
                    destructor. If the object (and the mutex) gets destroyed when a thread waits for
                    it in foo(), we get a crash or an exception. We can mitigate this with the use
                    of a shared_ptr, then we have no destructor call while someone waits for the
                    mutex. Unfortunately, we still have a risk of a signal, callback, etc. all those
                    things mixing badly with threads. And if we use too many shared_ptr's, we start
                    having lifetime issues or leaks. </para>
                <para>There are more lifetime issues, even without mutexes or threads. If you have
                    ever used Boost.Asio, a common mistake and an easy one is when a callback is
                    called in the proactor thread after an asynchronous operation, but the object
                    called is long gone and the callback invalid. Asynchronous provides <command
                        xlink:href="#trackable_servant">trackable_servant</command> which makes sure
                    that a callback is not called if the object which called the asynchronous
                    operation is gone. It also prevents a task posted in a threadpool to be called
                    if this condition occurs, which improves performance. Asynchronous also provides
                    a safe callback for use as Boost.Asio or similar asynchronous libraries.</para>
            </sect1>
            <sect1>
                <title>Servant Proxies</title>
                <para>Asynchronous offers <code>servant_proxy</code>, which makes the outside world
                    call members of a servant as if it was not living in an ActiveObject. It looks
                    like a thread-safe interface, but safe from deadlock and race conditions. </para>
            </sect1>
            <sect1>
                <title>Interrupting</title>
                <subtitle>Or how to catch back if you're drowning. </subtitle>
                <para>Let's say you posted so many tasks to your threadpool that all your cores are
                    full, still, your application is slipping more and more behind plan. You need to
                    give up some tasks to catch back a little.</para>
                <para>Asynchronous can give us an interruptible cookie when we post a task to a
                    scheduler, and we can use it to <command xlink:href="#interrupting_tasks">stop a
                        posted task</command>. If not running yet, the task will not start, if
                    running, it will stop at the next interruption point, in the sense of the <link
                        xlink:href="http://www.boost.org/doc/libs/1_54_0/doc/html/thread/thread_management.html#thread.thread_management.tutorial.interruption"
                        >Boost.Thread documentation</link>. Diagnostics will show that the task was
                    interrupted.</para>
            </sect1>
            <sect1>
                <title>Diagnostics</title>
                <para>Finding out how good your software is doing is not an easy task. Developers
                    are notoriously bad at it. You need to add lots of logging to find out which
                    function call takes too long and becomes a bottleneck. Finding out the minimum
                    required hardware to run your application is even harder.</para>
                <para>Asynchronous design helps here too. By logging the required time and the
                    frequency of tasks, it is easy to find out how many cores are needed.
                    Bottlenecks can be found by logging what the Thread world is doing and how long.
                    Finally, designing the asynchronous Thread world as state machines and logging
                    state changes will allow a better understanding of your system and make visible
                    potential for concurrency. Even for non-parallel algorithms, finding out, using
                    a state machine, the earliest point a task can be thrown to a threadpool will
                    give some low-hanging-fruit concurrency. Throw enough tasks to the threadpool
                    and manage this with a state machine and you might use your cores with little
                    effort. Parallelization can then be used later on by logging which tasks are
                    worth parallelized.</para>
                <para>Asynchronous offers <command
                    xlink:href="#html_diags">tools generating nice HTML outputs</command> for every schedulers,
                    including waiting and execution times of tasks, histograms, etc.</para>
            </sect1>
            <sect1>
                <title>Continuations</title>
                <para>Callbacks are great when you have a complex flow of operations which require a
                    state machine for management, however there are cases where callbacks are not an
                    ideal solution. Either because your application would require a constant
                    switching of context between single-threaded and parallel schedulers, or because
                    the single-threaded scheduler might be busy, which would delay completion of the
                    algorithm. A known example of this is a parallel fibonacci. In this case, one
                    can register a <command xlink:href="#continuations">continuation</command>,
                    which is to be executed upon completion of one or several tasks. </para>
                <para>This mechanism is flexible so that you can use it with futures coming from
                    another library, thus removing any need for a
                        <code>wait_for_all(futures...)</code> or a
                        <code>wait_for_any(futures...)</code>.</para>
            </sect1>
            <sect1>
                <title>Want more power? What about extra machines?</title>
                <para>What to do if your threadpools are using all of your cores but there simply
                    are not enough cores for the job? Buy more cores? Unfortunately, the number of
                    cores a single-machine can use is limited, unless you have unlimited money. A
                    dual 6-core Xeon, 24 threads with hyperthreading will cost much more than 2 x
                    6-core i7, and will usually have a lesser clock frequency and an older
                    architecture. </para>
                <para>The solution could be: start with the i7, then if you need more power, add
                    some more machines which will steal jobs from your threadpools using <command
                        xlink:href="#distributing">TCP</command>. This can be done quite easily with
                    Asynchronous.</para>
                <para>Want to build your own hierarchical network of servers? It's hard to make it
                    easier.</para>
            </sect1>
            <sect1>
                <title>Parallel algorithms</title>
                <para>The library also comes with <command xlink:href="#parallel_algos">non-blocking
                        algorithms</command> with iterators or ranges, partial support for TCP,
                    which fit well in the asynchronous system, with more to come. If you want to
                    contribute some more, be welcome. At the moment, the library offers:<itemizedlist>
                        <listitem>
                            <para>most STL algorithms</para>
                        </listitem>
                        <listitem>
                            <para>parallel_for / parallel_for_each</para>
                        </listitem>
                        <listitem>
                            <para>parallel_reduce</para>
                        </listitem>
                        <listitem>
                            <para>parallel_extremum</para>
                        </listitem>
                        <listitem>
                            <para>parallel_find_all</para>
                        </listitem>
                        <listitem>
                            <para>parallel_invoke</para>
                        </listitem>
                        <listitem>
                            <para>parallel_sort , parallel_quicksort</para>
                        </listitem>
                        <listitem>
                            <para>parallel_scan</para>
                        </listitem>
                        <listitem>
                            <para>parallelized Boost.Geometry algorithms for polygons
                                (parallel_union, parallel_intersection,
                                parallel_geometry_intersection_of_x,
                                parallel_geometry_union_of_x)</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Task Priority</title>
                <para>Asynchronous offers this possibility for all schedulers at low performance
                    cost. This means you not only have the possibility to influence task execution
                    order in a threadpool but also in Active Objects.</para>
                <para>This is achieved by posting a task to the queue with the corresponding
                    priority. It is also possible to get it even more fine-grained by using a
                    sequence of queues, etc.</para>
            </sect1>
            <sect1>
                <title>Integrating with Boost.Asio</title>
                <para>Asynchronous offers a Boost.Asio based <command xlink:href="#asio_scheduler">scheduler</command> allowing you to easily write
                    a Servant using Asio, or an Asio based threadpool. An advantage is that you get
                    safe callbacks and easily get your Asio application to scale. Writing a server
                    has never been easier.</para>
                <para>Asynchronous also uses Boost.Asio to provide a timer with callbacks.</para>
            </sect1>
            <sect1>
                <title>Integrating with Qt</title>
                <para>What about getting the power of Asynchronous within a Qt application? Use
                    Asynchronous' threadpools, algorithms and other cool features easily.</para>
            </sect1>
            <sect1>
                <title>Work Stealing</title>
                <para>Work stealing is supported both within the threads of a threadpool but also
                    between different threadpools. Please have a look at Asynchronous' composite
                    scheduler.</para>
            </sect1>
            <sect1>
                <title>Extending the library</title>
                <para>Asynchronous has been written with the design goal of allowing anybody to
                    extend the library. In particular, the authors are hoping to be offered the
                    following extensions:<itemizedlist>
                        <listitem>
                            <para>More schedulers, threadpools</para>
                        </listitem>
                        <listitem>
                            <para>Queues</para>
                        </listitem>
                        <listitem>
                            <para>Parallel algorithms</para>
                        </listitem>
                        <listitem>
                            <para>Integration with other libraries</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Design Diagrams</title>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/AsynchronousDesign.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This diagram shows an overview of the design behind Asynchronous. One or more
                    Servant objects live in a single-theaded world, communicating with the outside
                    world only through one or several queues, from which the single-threaded
                    scheduler pops tasks. Tasks are pushed by calling a member on a proxy
                    object.</para>
                <para>Like an Active Object, a client uses a proxy (a shared object type), which
                    offers the same members as the real servant, with the same parameters, the only
                    difference being the return type, a std::future&lt;R>, with R being the return
                    type of the servant's member. All calls to a servant from the client side are
                    posted, which includes the servant constructor and destructor. When the last
                    instance of a servant is destroyed, be it used inside the Thread world or
                    outside, the servant destructor is posted.</para>
                <para>any_shared_scheduler is the part of the Active Object scheduler living inside
                    the Thread world. Servants do not hold it directly but hold an
                    any_weak_scheduler instead. The library will use it to create a posted callback
                    when a task executing in a worker threadpool is completed.</para>
                <para>Shutting down a Thread world is done automatically by not needing it. It
                    happens in the following order:<itemizedlist>
                        <listitem>
                            <para>While a servant proxy is alive, no shutdown</para>
                        </listitem>
                        <listitem>
                            <para>When the last servant proxy goes out of scope, the servant
                                destructor is posted.</para>
                        </listitem>
                        <listitem>
                            <para>if jobs from servants are running in a threadpool, they get a
                                chance to stop earlier by running into an interruption point or will
                                not even start.</para>
                        </listitem>
                        <listitem>
                            <para>threadpool(s) is (are) shut down.</para>
                        </listitem>
                        <listitem>
                            <para>The Thread world scheduler is stopped and its thread
                                terminates.</para>
                        </listitem>
                        <listitem>
                            <para>The last instance of any_shared_scheduler_proxy goes out of scope
                                with the last servant proxy and joins.</para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>It is usually accepted that threads are orthogonal to an OO design and
                    therefore are hard to manage as they don't belong to an object. Asynchronous
                    comes close to this: threads are not directly used, but instead owned by a
                    scheduler, in which one creates objects and tasks.</para>
            </sect1>
        </chapter>
    </part>
    <part>
        <title>User Guide</title>
        <chapter>
            <title>Using Asynchronous</title>
            <sect1>
                <title>Definitions</title>                
                <sect2>
                    <title>Scheduler</title>
                    <para>Object having 0..n threads, executing jobs or callbacks. Stops owned
                        threads when destroyed.</para>
                </sect2>
                <sect2>
                    <title>Thread World (also known as Appartment)</title>
                    <para>A "thread world" is a world defined by a (single threaded) scheduler and
                        all the objects which have been created, are living and destroyed within
                        this context. It is usually agreed on that objects and threads do not mix
                        well. Class diagrams fail to display both as these are orthogonal concepts.
                        Asynchronous solves this by organizing objects into worlds, each living
                        within a thread. This way, life cycles issues and the question of thread
                        access to objects are solved. It is similar to the Active Object pattern,
                        but with n Objects living within a thread. </para>
                </sect2>
                <sect2>
                    <title>Weak Scheduler</title>
                    <para>A weak_ptr to a shared scheduler. Does not keep the Scheduler
                        alive.</para>
                </sect2>
                <sect2>
                    <title>Trackable Servant</title>
                    <para>Object living in a (single-threaded) scheduler, starting tasks and
                        handling callbacks.</para>
                </sect2>
                <sect2>
                    <title>Queue</title>
                    <para>Holds jobs for a scheduler to execute. Used as communication mean between
                        Schedulers / Worlds</para>
                </sect2>
                <sect2>
                    <title>Servant Proxy</title>
                    <para>A thread-safe object looking like a Servant and serializing calls to
                        it.</para>
                </sect2>
                <sect2>
                    <title>Scheduler Shared Proxy</title>
                    <para>Object holding a scheduler and interfacing with it. The last instance
                        joins the threads of the scheduler.</para>
                </sect2>
                <sect2>
                    <title>Posting</title>
                    <para>Enqueueing a job into a Scheduler's queue.</para>
                </sect2>
            </sect1>
            <sect1>
                <title>Hello, asynchronous world</title>
                <para>The following code shows a very basic usage (a complete example <link
                        xlink:href="examples/example_post_future.cpp">here</link>), this is not
                    really asynchronous yet:</para>
                <programlisting>#include &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp>
#include &lt;boost/asynchronous/queue/lockfree_queue.hpp>
#include &lt;boost/asynchronous/scheduler_shared_proxy.hpp>
#include &lt;boost/asynchronous/post.hpp>
struct void_task
{
    void operator()()const
    {
        std::cout &lt;&lt; "void_task called" &lt;&lt; std::endl;
    }
};
struct int_task
{
    int operator()()const
    {
        std::cout &lt;&lt; "int_task called" &lt;&lt; std::endl;
        return 42;
    }
};  

// create a threadpool scheduler with 3 threads and communicate with it using a threadsafe_list
// we use auto as it is easier than boost::asynchronous::any_shared_scheduler_proxy&lt;>
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
std::future&lt;void> fuv = boost::asynchronous::post_future(scheduler, void_task());
fuv.get();
// post a simple task and wait for result
std::future&lt;int> fui = boost::asynchronous::post_future(scheduler, int_task());
int res = fui.get();
  </programlisting>
                <para>Of course this works with C++11 lambdas:</para>
                <programlisting>auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
std::future&lt;void> fuv = boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "void lambda" &lt;&lt; std::endl;});
fuv.get();
// post a simple task and wait for result
std::future&lt;int> fui = boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "int lambda" &lt;&lt; std::endl;return 42;});
int res = fui.get();   </programlisting>
                <para>boost::asynchronous::post_future posts a piece of work to a threadpool
                    scheduler with 3 threads and using a lockfree_queue. We get a std::future&lt;the
                    type of the task return type>.</para>
                <para>This looks like much std::async, but we're just getting started. Let's move on
                    to something more asynchronous.</para>
            </sect1>
            <sect1>
                <title>A servant proxy</title>
                <para>We now want to create a single-threaded scheduler, populate it with some
                    servant(s), and exercise some members of the servant from an outside thread. We
                    first need a servant:</para>
                <programlisting>struct Servant
{
    Servant(int data): m_data(data){}
    int doIt()const
    {
        std::cout &lt;&lt; "Servant::doIt with m_data:" &lt;&lt; m_data &lt;&lt; std::endl;
        return 5;
    }
    void foo(int&amp; i)const
    {
        std::cout &lt;&lt; "Servant::foo with int:" &lt;&lt; i &lt;&lt; std::endl;
        i = 100;
    }
    void foobar(int i, char c)const
    {
        std::cout &lt;&lt; "Servant::foobar with int:" &lt;&lt; i &lt;&lt; " and char:" &lt;&lt; c &lt;&lt;std::endl;
    }
    int m_data;
}; </programlisting>
                <para>We now create a proxy type to be used in other threads:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>
{
public:
    // forwarding constructor. Scheduler to servant_proxy, followed by arguments to Servant.
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>(s, data)
    {}
    // the following members must be available "outside"
    // foo and foobar, just as a post (no interesting return value)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foo</emphasis>)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foobar</emphasis>)
    // for doIt, we'd like a future
    BOOST_ASYNC_FUTURE_MEMBER(<emphasis role="bold">doIt</emphasis>)
};</programlisting>
                <para>Let's use our newly defined proxy:</para>
                <programlisting>int something = 3;
{
    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >);

    {
        // arguments (here 42) are forwarded to Servant's constructor
        ServantProxy proxy(scheduler,42);
        // post a call to foobar, arguments are forwarded.
        proxy.foobar(1,'a');
        // post a call to foo. To avoid races, the reference is ignored.
        proxy.foo(something);
        // post and get a future because we're interested in the result.
        std::future&lt;int> fu = proxy.doIt();
        std::cout&lt;&lt; "future:" &lt;&lt; fu.get() &lt;&lt; std::endl;
    }// here, Servant's destructor is posted and waited for
}// scheduler is gone, its thread has been joined
std::cout&lt;&lt; "something:" &lt;&lt; something &lt;&lt; std::endl; // something was not changed as it was passed by value. You could use a boost::ref if this is not desired.</programlisting>
                <para>We can call members on the proxy, almost as if they were called on Servant.
                    The library takes care of the posting and forwarding the arguments. When
                    required, a future is returned. Stack unwinding works, and when the servant
                    proxy goes out of scope, the servant destructor is posted. When the scheduler
                    goes out of scope, its thread is stopped and joined. The queue is processed
                    completely first. Of course, as many servants as desired can be created in this
                    scheduler context. Please have a look at <link
                        xlink:href="examples/example_simple_servant.cpp">the complete
                    example</link>.</para>
            </sect1>
            <sect1>
                <title>Using a threadpool from within a servant</title>
                <para>If you remember the principles of Asynchronous, blocking a single-thread
                    scheduler is taboo as it blocks the thread doing all the management of a system.
                    But what to do when one needs to execute long tasks? Asynchronous provides a
                    whole set of threadpools. A servant posts something to a threadpool, provides a
                    callback, then gets a result. Wait a minute. Callback? Is this not
                    thread-unsafe? Why not threadpools with futures, like usual? Because in a
                    perfectly asynchronous world, waiting for a future means blocking a servant
                    scheduler. One would argue that it is possible not to block on the future, and
                    instead ask if there is a result. But frankly, polling is not a nice solution
                    either.</para>
                <para>And what about thread-safety? Asynchronous takes care of this. A callback is
                    never called from a threadpool, but instead posted back to the queue of the
                    scheduler which posted the work. All the servant has to do, is to do nothing and
                    wait until the callback is executed. Note that this is not the same as a
                    blocking wait, the servant can still react to events.</para>
                <para>Clearly, this brings some new challenges as the flow of control gets harder to
                    follow. This is why a servant is often written using state machines. The
                    (biased) author suggests to have a look at the <link
                        xlink:href="http://www.boost.org/doc/libs/1_59_0/libs/msm/doc/HTML/index.html"
                        > Meta State Machine library </link> , which plays nicely with
                    Asynchronous.</para>
                <para>But what about the usual proactor issues (crashes) when the servant has long
                    been destroyed when the callback is posted. Gone. Asynchronous <command
                        xml:id="trackable_servant"/><code>trackable_servant</code> post_callback
                    ensures that a callback is not called if the servant is gone. Better even, if
                    the servant has been destroyed, an unstarted posted task will not be
                    executed.</para>
                <para>What about another common issue? If one posts a task, say a lambda, which
                    captures a shared_ptr to an object per value, and this object is a
                    boost::signal? Then when the task object has been executed and is destroyed, one
                    could have a race on the signal deregistration. But again no. Asynchronous
                    ensures that a task created within a scheduler context gets destroyed in this
                    context.</para>
                <para>This is about the best protection one can get. What Asynchronous cannot
                    protect from are self-made races within a task (if you post a task with a
                    pointer to the servant, you're on your own and have to protect your servant). A
                    good rule of thumb is to consider data passed to a task as moved or passed by
                    value. To support this, Asynchronous does not copy tasks but moves them.</para>
                <para>Armed with these protections, let's give a try to a threadpool, starting with
                    the most basic one, <code>threadpool_scheduler</code> (more to come):</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler)
        : boost::asynchronous::trackable_servant&lt;>(scheduler,
                                               // <emphasis role="bold">threadpool with 3 threads</emphasis> and a lockfree_queue
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   new <emphasis role="bold">boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis></emphasis>&lt;
                                                           boost::asynchronous::lockfree_queue&lt;> >(<emphasis role="bold">3</emphasis>))){}
    // call to this is posted and executes in our (safe) single-thread scheduler
    void start_async_work()
    {
       //ok, let's post some work and wait for an answer
       <emphasis role="bold">post_callback</emphasis>(
                    [](){std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;}, // work, do not use "this" here
                    [/*this*/](boost::asynchronous::expected&lt;void>){...}// callback. Safe to use "this" as callback is only called if Servant is alive
        );
    }
};</programlisting>
                <para>We now have a servant, living in its own thread, which posts some long work to
                    a three-thread-threadpool and gets a callback, but only if still alive.
                    Similarly, the long work will be executed by the threadpool only if Servant is
                    alive by the time it starts. Everything else stays the same, one creates a proxy
                    for the servant and posts calls to its members, so we'll skip it for
                    conciseness, the complete example can be found <link
                        xlink:href="examples/example_post_trackable_threadpool.cpp"
                    >here</link>.</para>
            </sect1>
            <sect1>
                <title>A servant using another servant proxy</title>
                <para>Often, in a layered design, you'll need that a servant in a single-threaded
                    scheduler calls a member of a servant living in another one. And you'll want to
                    get a callback, not a future, because you absolutely refuse to block waiting for
                    a future (and you'll be very right of course!). Ideally, except for main(), you
                    won't want any of your objects to wait for a future. There is another
                    servant_proxy macro for this, <code>BOOST_ASYNC_UNSAFE_MEMBER</code>(unsafe
                    because you get no thread-safety from if and you'll take care of this yourself,
                    or better, <code>trackable_servant</code> will take care of it for you, as
                    follows):</para>
                <para>
                    <programlisting>// Proxy for a basic servant 
class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s, data)
    {}
    BOOST_ASYNC_UNSAFE_MEMBER(foo)
    BOOST_ASYNC_UNSAFE_MEMBER(foobar)
};   </programlisting>
                    <programlisting>// Servant using the first one
struct Servant2 : boost::asynchronous::trackable_servant&lt;>
{
    Servant2(boost::asynchronous::any_weak_scheduler&lt;> scheduler,ServantProxy worker)
        :boost::asynchronous::trackable_servant&lt;>(scheduler)
        ,m_worker(worker) // the proxy allowing access to Servant
    void doIt()    
    {                 
         call_callback(m_worker.get_proxy(), // Servant's outer proxy, for posting tasks
                       m_worker.foo(), // what we want to call on Servant
                      // callback functor, when done.
                      [](boost::asynchronous::expected&lt;int> result){...} );// expected&lt;return type of foo> 
    }
};</programlisting>
                </para>
                <para>Call of <code>foo()</code> will be posted to <code>Servant</code>'s scheduler,
                    and the callback lambda will be posted to <code>Servant2</code> when completed.
                    All this thread-safe of course. Destruction is also safe. When
                        <code>Servant2</code> goes out of scope, it will shutdown
                        <code>Servant</code>'s scheduler, then will his scheduler be shutdown
                    (provided no more object is living there), and all threads joined. The <link
                        xlink:href="examples/example_two_simple_servants.cpp">complete example
                    </link> shows a few more calls too.</para>
                <para>Asynchronous offers a different syntax to achieve the same result. Which one
                    you use is a matter of taste, both are equivalent. The second method is with
                    BOOST_ASYNC_MEMBER_UNSAFE_CALLBACK(_LOG if you need logging). It takes a
                    callback as argument, other arguments are forwarded. Combined with
                    <code>make_safe_callback</code>, one gets the same effect (safe call) as above.</para>
                <programlisting>// Proxy for a basic servant 
class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s, data)
    {}
    BOOST_ASYNC_MEMBER_UNSAFE_CALLBACK(foo) // say, foo takes an int as argument
};   </programlisting>
                <programlisting>// Servant using the first one
struct Servant2 : boost::asynchronous::trackable_servant&lt;>
{
    Servant2(boost::asynchronous::any_weak_scheduler&lt;> scheduler,ServantProxy worker)
        :boost::asynchronous::trackable_servant&lt;>(scheduler)
        ,m_worker(worker) // the proxy allowing access to Servant
    void doIt()    
    {                 
         m_worker.foo(make_safe_callback([](boost::asynchronous::expected&lt;void> res) // expected&lt;return type of foo> 
                                        {/* callback code*/}), 
                      42 /* arguments of foo*/); 
    }
};</programlisting>
            </sect1>
            <sect1>
                <title><command xml:id="interrupting_tasks"/>Interrupting tasks</title>
                <para>Let's imagine that a manager object (a state machine for example) posted some
                    long-lasting work to a threadpool, but this long-lasting work really takes too
                    long. As we are in an asynchronous world and non-blocking, the manager object
                    realizes there is a problem and decides the task must be stopped otherwise the
                    whole application starts failing some real-time constraints (how would we do if
                    we were blocked, waiting for a future?). This is made possible by using another
                    form of posting, getting a handle, on which one can require interruption. As
                    Asynchronous does not kill threads, we'll use one of Boost.Thread predefined
                    interruption points. Supposing we have well-behaved tasks, they will be
                    interrupted at the next interruption point if they started, or if they did not
                    start yet because they are waiting in a queue, then they will never start. In
                    this <link xlink:href="examples/example_interrupt.cpp">example</link>, we have
                    very little to change but the post call. We use <code>interruptible_post_callback</code>
                    instead of <code>post_callback</code>. We get an <code>any_interruptible object</code>, which offers a
                    single <code>interrupt()</code> member.</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
     ... // as usual
    void start_async_work()
    {
        // start long interruptible tasks
        // we get an interruptible handler representing the task
        <emphasis role="bold">boost::asynchronous::any_interruptible</emphasis> interruptible =
        <emphasis role="bold">interruptible_post_callback</emphasis>(
                // interruptible task
               [](){
                    std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;
                    boost::this_thread::sleep(boost::posix_time::milliseconds(1000));}, // sleep is an interrupting point
               // callback functor.
               [](boost::asynchronous::expected&lt;void> ){std::cout &lt;&lt; "Callback will most likely not be called" &lt;&lt; std::endl;}
        );
        // let the task start (not sure but likely)
        // if it had no time to start, well, then it will never.
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // actually, we changed our mind and want to interrupt the task
        interruptible.<emphasis role="bold">interrupt()</emphasis>;
        // the callback will likely never be called as the task was interrupted
    }
};                </programlisting>
            </sect1>
            <sect1>
                <title><command xml:id="logging_tasks"/>Logging tasks</title>
                <para>Developers are notoriously famous for being bad at guessing which part of
                    their code is inefficient. This is bad in itself, but even worse for a control
                    class like our post-callback servant as it reduces responsiveness. Knowing how
                    long a posted tasks or a callback lasts is therefore very useful. Knowing how
                    long take tasks executing in the threadpools is also essential to plan what
                    hardware one needs for an application(4 cores? Or 100?). We need to know what
                    our program is doing. Asynchronous provides logging per task to help there.
                    Let's have a look at some code. It's also time to start using our template
                    parameters for <code>trackable_servant</code>, in case you wondered why they are
                    here.</para>
                <programlisting>// we will be using loggable jobs internally
typedef boost::asynchronous::any_loggable&lt;std::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;
// the type of our log
typedef std::map&lt;std::string,std::list&lt;boost::asynchronous::diagnostic_item&lt;std::chrono::high_resolution_clock> > > <emphasis role="bold">diag_type</emphasis>;

// we log our scheduler and our threadpool scheduler (both use servant_job)
struct Servant : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job</emphasis>,<emphasis role="bold">servant_job</emphasis>>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;servant_job> scheduler) //servant_job is our job type
        : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job,servant_job</emphasis>>(scheduler,
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   // threadpool with 3 threads and a simple threadsafe_list queue
                                                   // Furthermore, it logs posted tasks
                                                   new boost::asynchronous::threadpool_scheduler&lt;
                                                           //servant_job is our job type
                                                           boost::asynchronous::lockfree_queue&lt; <emphasis role="bold">servant_job</emphasis> > >(3))){}
    void start_async_work()
    {
         post_callback(
               // task posted to threadpool
               [](){...}, // will return an int
               [](boost::asynchronous::expected&lt;int> res){...},// callback functor.
               // the task / callback name for logging
               <emphasis role="bold">"int_async_work"</emphasis>
        );
    }
    // we happily provide a way for the outside world to know what our threadpool did.
    // get_worker is provided by trackable_servant and gives the proxy of our threadpool
    diag_type get_diagnostics() const
    {
        return (*get_worker()).get_diagnostics();
    }
};</programlisting>
                <para>The proxy is also slightly different, using a _LOG macro and an argument
                    representing the name of the task.</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>(s)
    {}
    // the _LOG macros do the same as the others, but take an extra argument, the logged task name
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(start_async_work,<emphasis role="bold">"proxy::start_async_work"</emphasis>)
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(get_diagnostics,<emphasis role="bold">"proxy::get_diagnostics"</emphasis>)
};               </programlisting>
                <para> We now can get diagnostics from both schedulers, the single-threaded and the
                    threadpool (as external code has no access to it, we ask Servant to help us
                    there through a get_diagnostics() member).</para>
                <programlisting>// create a scheduler with logging
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                    boost::asynchronous::lockfree_queue&lt;servant_job> >);
// create a Servant                    
ServantProxy proxy(scheduler); 
...
// let's ask the single-threaded scheduler what it did.
diag_type single_thread_sched_diag = scheduler.get_diagnostics(); 
for (auto mit = single_thread_sched_diag.begin(); mit != single_thread_sched_diag.end() ; ++mit)
{
     std::cout &lt;&lt; "job type: " &lt;&lt; (*mit).first &lt;&lt; std::endl;
     for (auto jit = (*mit).second.begin(); jit != (*mit).second.end();++jit)
     {
          std::cout &lt;&lt; "job waited in us: " &lt;&lt; std::chrono::nanoseconds((*jit).get_started_time() - (*jit).<emphasis role="bold">get_posted_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job lasted in us: " &lt;&lt; std::chrono::nanoseconds((*jit).get_finished_time() - (*jit).<emphasis role="bold">get_started_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job interrupted? "  &lt;&lt; std::boolalpha &lt;&lt; (*jit).<emphasis role="bold">is_interrupted()</emphasis> &lt;&lt; std::endl;
          std::cout &lt;&lt; "job failed? "  &lt;&lt; std::boolalpha &lt;&lt; (*jit).<emphasis role="bold">is_failed()</emphasis> &lt;&lt; std::endl; // did this job throw an exception?
     }
}              </programlisting>
                <para>It goes similarly with the threapool scheduler, with the slight difference
                    that we ask the Servant to deliver diagnostic information through a proxy
                    member. The <link xlink:href="examples/example_log.cpp">complete example</link>
                    shows all this, plus an interrupted job.</para>                
            </sect1>
            <sect1>
                <title><command xml:id="html_diags"/>Generating HTML diagnostics</title>
                <para>We just saw how to programmatically get diagnostics from schedulers. This is
                    very useful, but nobody likes to do it manually, so the authors went the extra
                    mile and provide an HTML formatter for convenience. The <link
                        xlink:href="examples/example_html_diagnostics.cpp">included example</link>
                    shows how to use it. In this example, we have a Servant, living in its own
                    single-threaded scheduler called "Servant". It uses a threadpool call
                    "Threadpool". When the Servant's foo() method is called, it executes a
                    parallel_reduce(parallel_for(...)), or whatever you like. These operations are
                    named accordingly. We also create a third scheduler, called "Formatter
                    scheduler", which will be used by the formatter code. Yes, even this scheduler
                    will be logged too. The example creates a Servant, calls foo() on the proxy,
                    sleeps for a while (how long is passed to the example as argument), then
                    generates <link xlink:href="examples/in_progress.html">a first output
                        statistics</link>. Depending on the sleep time, the parallel work might or
                    might not be finished, so this is an intermediate result.</para>
                <para>We then wait for the tasks to finish, destroy the servant, so that its
                    destructor is logged too, and we generate a <link
                        xlink:href="examples/final.html">final diagnostics</link>.</para>
                <para>The HTML pages display the statistics for all schedulers, including the
                    formatter. It shows with different colors the waiting times of tasks (called
                    Scheduling time), the execution times, successful or failed separately, and the
                    added total time for each task, with max min, average duration. One can also
                    display the full list of tasks and even histograms. As this is a lot of
                    information, it is possible to hide part of it using checkboxes.</para>
                <para>One also gets the very useful information of how long are the different
                    scheduler queues, which gives a very good indication of how busy the system
                    is.</para>
            </sect1>
            <sect1>
                <title>Queue container with priority</title>
                <para>Sometimes, all jobs posted to a scheduler do not have the same priority. For
                    threadpool schedulers, <code>composite_threadpool_scheduler</code> is an option.
                    For a single-threaded scheduler, Asynchronous does not provide a priority queue
                    but a queue container, which itself contains any number of queues, of different
                    types if needed. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>Priority is defined simply by posting to the queue with the
                                desired priority, so there is no need for expensive priority
                                algorithms.</para>
                        </listitem>
                        <listitem>
                            <para>Reduced contention if many threads of a threadpool post something
                                to the queue of a single-threaded scheduler. If no priority is
                                defined, one queue will be picked, according to a configurable
                                policy, reducing contention on a single queue.</para>
                        </listitem>
                        <listitem>
                            <para>It is possible to mix queues.</para>
                        </listitem>
                        <listitem>
                            <para>It is possible to build a queue container of queue containers,
                                etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Note: This applies to any scheduler. We'll start with single-threaded
                    schedulers used by managing servants for simplicity, but it is possible to have
                    composite schedulers using queue containers for finest granularity and least
                    contention.</para>
                <para>First, we need to create a single-threaded scheduler with several queues for
                    our servant to live in, for example, one threadsafe list and three lockfree
                    queues:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
                           boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                        boost::asynchronous::any_queue_container&lt;> >
                        (boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::<emphasis role="bold">threadsafe_list</emphasis>&lt;> >(<emphasis role="bold">1</emphasis>),
                         boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::<emphasis role="bold">lockfree_queue</emphasis>&lt;> >(<emphasis role="bold">3</emphasis>,100)
                         ));</programlisting>
                <para><code>any_queue_container</code> takes as constructor arguments a variadic
                    sequence of <code>any_queue_container_config</code>, with a queue type as
                    template argument, and in the constructor the number of objects of this queue
                    (in the above example, one <code>threadsafe_list</code> and 3
                        <code>lockfree_queue</code> instances, then the parameters that these queues
                    require in their constructor (100 is the capacity of the underlying
                        <code>boost::lockfree_queue</code>). This means, that our
                        <code>single_thread_scheduler</code> has 4 queues:<itemizedlist>
                        <listitem>
                            <para>a threadsafe_list at index 1</para>
                        </listitem>
                        <listitem>
                            <para>lockfree queues at indexes 2,3,4</para>
                        </listitem>
                        <listitem>
                            <para>>= 4 means the queue with the least priority.</para>
                        </listitem>
                        <listitem>
                            <para>0 means "any queue" and is the default</para>
                        </listitem>
                    </itemizedlist></para>
                <para>The scheduler will handle these queues as having priorities: as long as there
                    are work items in the first queue, take them, if there are no, try in the
                    second, etc. If all queues are empty, the thread gives up his time slice and
                    sleeps until some work item arrives. If no priority is defined by posting, a
                    queue will be chosen (by default randomly, but this can be configured with a
                    policy). This has the advantage of reducing contention of the queue, even when
                    not using priorities. The servant defines the priority of the tasks it provides.
                    While this might seem surprising, it is a design choice to avoid that the coder
                    using a servant proxy interface would have to think about it, as you will see in
                    the second listing. To define a priority for a servant proxy, there is a second
                    field in the macros:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s)
    {}
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_CTOR(3)</emphasis>
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_DTOR(4)</emphasis>
    BOOST_ASYNC_FUTURE_MEMBER(start_async_work,<emphasis role="bold">1</emphasis>)
};</programlisting>
                <para>BOOST_ASYNC_FUTURE_MEMBER and other similar macros can be given an optional
                    priority parameter, in this case 1, which is our threadsafe list. Notice how you
                    can then define the priority of the posted servant constructor and
                    destructor.</para>
                <programlisting>ServantProxy proxy(scheduler);
std::future&lt;std::future&lt;int>> fu = proxy.start_async_work();</programlisting>
                <para>Calling our proxy member stays unchanged because the macro defines the
                    priority of the call.</para>
                <para>We also have an extended version of <code>post_callback</code>, called by a
                    servant posting work to a threadpool:</para>
                <programlisting>post_callback(
       [](){return 42;},// work
       [this](boost::asynchronous::expected&lt;int> res){}// callback functor.
       ,"",
       <emphasis role="bold">2</emphasis>, // work prio
       <emphasis>2</emphasis>  // callback prio
);</programlisting>
                <para>Note the two added priority values: the first one for the task posted to the
                    threadpool, the second for the priority of the callback posted back to the
                    servant scheduler. The string is the log name of the task, which we choose to
                    ignore here.</para>
                <para>The priority is in any case an indication, the scheduler is free to ignore it
                    if not supported. In the <link xlink:href="examples/example_queue_container.cpp"
                        >example</link>, the single threaded scheduler will honor the request, but
                    the threadpool has a normal queue and cannot honor the request, but a threadpool
                    with an <code>any_queue_container</code> or a
                        <code>composite_threadpool_scheduler</code> can. The <link
                        xlink:href="examples/example_queue_container_log.cpp">same example</link>
                    can be rewritten to also support logging.</para>
                <para><code>any_queue_container</code> has two template arguments. The first, the
                    job type, is as always by default, a callable (<code>any_callable</code>) job.
                    The second is the policy which Asynchronous uses to find the desired queue for a
                    job. The default is <code>default_find_position</code>, which is as described
                    above, 0 means any position, all other values map to a queue, priorities >=
                    number of queues means last queue. Any position is by default random
                        (<code>default_random_push_policy</code>), but you might pick
                        <code>sequential_push_policy</code>, which keeps an atomic counter to post
                    jobs to queues in a sequential order.</para>
                <para>If you plan to build a queue container of queue containers, you'll probably
                    want to provide your own policy.</para>
            </sect1>
            <sect1>
                <title>Multiqueue Schedulers' priority</title>
                <para>A multiqueue_... threadpool scheduler has a queue for each thread. This
                    reduces contention, making these faster than single queue schedulers, like
                    threadpool_scheduler. Furthermore, these schedulers support priority: the
                    priority given in post_future or post_callback is the (1-based) position of the
                    queue we want to post to. 0 means "any queue". A queue of priority 1 has a
                    higher priority than a queue with priority 2, etc. </para>
                <para>Each queue is serving one thread, but threads steal from each other's queue,
                    according to the priority.</para>
            </sect1>
            <sect1>
                <title>Threadpool Schedulers with several queues</title>
                <para>A queue container has advantages (different queue types, priority for single
                    threaded schedulers) but also disadvantages (takes jobs from one end of the
                    queue, which means potential cache misses, more typing work). If you don't need
                    different queue types for a threadpool but want to reduce contention, multiqueue
                    schedulers are for you. A normal <code>threadpool_scheduler</code> has x threads
                    and one queue, serving them. A <code>multiqueue_threadpool_scheduler</code> has
                    x threads and x queues, each serving a worker thread. Each thread looks for work
                    in its queue. If it doesn't find any, it looks for work in the previous one,
                    etc. until it finds one or inspected all the queues. As all threads steal from
                    the previous queue, there is little contention. The construction of this
                    threadpool is very similar to the simple
                    <code>threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                // 4 threads and 4 lockfree queues of 10 capacity
                new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> >(4,10));</programlisting>
                <para>The first argument is the number of worker threads, which is at the same time
                    the number of queues. As for every scheduler, if the queue constructor takes
                    arguments, they come next and are forwarded to the queue.</para>
                <para>This is the <emphasis role="underline">advised</emphasis> scheduler for
                    standard cases as it offers lesser contention and task stealing between the
                    queues it uses for task transfer.</para>
                <para><emphasis role="italic">Limitation:</emphasis> these schedulers cannot have 0
                    thread like their single-queue counterparts.</para>
            </sect1>
            <sect1>
                <title>Composite Threadpool Scheduler</title>
                <sect2>
                    <title>Usage</title>
                
                <para>When a project becomes more complex, having a single threadpool for the whole
                    application does not offer enough flexibility in load planning. It is pretty
                    hard to avoid either oversubscription (more busy threads than available hardware
                    threads) or undersubscription. One would need one big threadpool with exactly
                    the number of threads available in the hardware. Unfortunately, if we have a
                    hardware with, say 12 hardware threads, parallelizing some work using all 12
                    might be slowlier than using only 8. One would need different threadpools of
                    different number of threads for the application. This, however, has the serious
                    drawback that there is a risk that some threadpools will be in overload, while
                    others are out of work unless we have work stealing between different
                    threadpools.</para>
                <para>The second issue is task priority. One can define priorities with several
                    queues or a queue container, but this ensures that only highest priority tasks
                    get executed if the system is coming close to overload. Ideally, it would be
                    great if we could decide how much compute power we give to each task
                    type.</para>
                <para>This is what <code>composite_threadpool_scheduler</code> solves. This pool
                        supports, like any other pool, the
                        <code>any_shared_scheduler_proxy</code>concept so you can use it in place of
                        the ones we used so far. The pool is composed of other pools
                            (<code>any_shared_scheduler_proxy</code> pools). It implements work
                        stealing between pools if a) the pools support it and b) the queue of a pool
                        also does. For example, we can create the following worker pool made of 3
                        sub-pools:</para>
                <para>
                    <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 1 thread, with a lockfree_queue of capacity 100. 
// This scheduler does not steal from other schedulers, but will lend its queue for stealing
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (1,100));

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp2 = boost::asynchronous::create_shared_scheduler_proxy( 
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;> > (3));

// a multiqueue_threadpool_scheduler, 4 threads, each with a lockfree_spsc_queue of capacity 100
// this is safe because there will be no stealing as the queue does not support it, and only the servant single-thread scheduler will be the producer
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp3 = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_spsc_queue&lt;> > (4,100));

// create a composite pool made of the 3 previous ones
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp_worker =
             boost::make_shared&lt;boost::asynchronous::composite_threadpool_scheduler&lt;> > (tp,tp2,tp3);
                    </programlisting>
                </para>
                <para>We can use this pool:<itemizedlist>
                        <listitem>
                            <para>As a big worker pool. In this case, the priority argument we use
                                for posting refers to the (1-based) index of the subpool
                                (post_callback(func1,func2,"task name",<emphasis role="bold"
                                    >1</emphasis>,0);). "1" means post to the first pool. But
                                another pool could steal the work.</para>
                        </listitem>
                        <listitem>
                            <para>As a pool container, but different parts of the code will get to
                                see only the subpools. For example, the pools tp, tp2 and tp3 can
                                still be used independently as a worker pool. Calling
                                composite_threadpool_scheduler&lt;>::get_scheduler(std::size_t
                                index_of_pool) will also give us the corresponding pool (1-based, as
                                always).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Another example of why to use this pool is reusing threads allocated to an
                        asio-based communication for helping other schedulers. Addng an asio
                        scheduler to a composite pool will allow the threads of this scheduler to
                        help (steal) other pools when no communication is currently happening. </para>
                <para>Stealing is done with priority. A stealing pool first tries to steal from the
                        first pool, then from the second, etc.</para>
                <para>The <link xlink:href="examples/example_composite_threadpool.cpp">following
                        example</link> shows a complete servant implementation, and the <command
                        xlink:href="#asio_scheduler">ASIO section</command> will show how an ASIO
                    pool can steal.</para>
                <para>The threadpool schedulers we saw so far are not stealing from other pools. The
                        single-queue schedulers are not stealing, and the multiqueue schedulers
                        steal from the queues of other threads of the same pool. The
                        scheduler-stealing schedulers usually indicate this by appending a
                            <code>stealing_</code> to their name:<itemizedlist>
                            <listitem>
                                <para><code>stealing_threadpool_scheduler</code> is a
                                        <code>threadpool_scheduler</code> which steals from other
                                    pools.</para>
                            </listitem>
                            <listitem>
                                <para><code>stealing_multiqueue_threadpool_scheduler</code> is a
                                        <code>multiqueue_threadpool scheduler</code> which steals
                                    from other pools.</para>
                            </listitem>
                            <listitem>
                                <para><code>asio_scheduler steals</code>.</para>
                            </listitem>
                        </itemizedlist></para>
                <para>The only difference with their not stealing equivalent is that they steal from
                        other schedulers. To achieve this, they need a composite_scheduler to tell
                        them from which schedulers they can steal.</para>
                <para>Not all schedulers offer to be stolen from. A
                            <code>single_thread_scheduler</code> does not as it would likely bring
                        race conditions to active objects.</para>
                <para>Another interesting usage will be when planning for extra machines to help a
                        threadpool by processing some of the work: work can be stolen from a
                        threadpool by a <command xlink:href="#distributing"
                            >tcp_server_scheduler</command> from which other machines can get it.
                        Just pack both pools in a <code>composite_threadpool_scheduler</code> and
                        you're ready to go.</para>
                </sect2>
                <sect2>
                    <title>Priority</title>
                    <para>A composite supports priority. The first pool passed in the constructor of
                        the composite pool has priority 1, the second 2, etc. 0 means "any pool" and
                        n where n > number of pools will me modulo-ed.</para>
                    <para>Posting to this scheduler using post_future or post_callback using a given
                        priority will post to the according pool. If a pool supports stealing from
                        other pools (stealing_... pools), it will try to steal from other pools,
                        starting with the highest priority, but only if the to be stolen from pools
                        supports it. For example, we try to post to the first pool, callback to any
                        queue.</para>
                    <programlisting>post_callback(
               [](){},// work
               [this](boost::asynchronous::expected&lt;int>){},// callback functor.
               "", // task and callback name
               1,  // work priority, highest
               0   // callback anywhere
);</programlisting>
                </sect2>
            </sect1>
            <sect1>
                <title>More flexibility in dividing servants among threads</title>
                <para>TODO example and code. We saw how to assign a servant or several servants to a
                    single thread scheduler. We can also create schedulers and divide servants among
                    them. This is very powerful but still has some constraints:<itemizedlist>
                        <listitem>
                            <para>We need to assign servants to schedulers while what we want is to
                                assign them to threads. We also have to consider how many schedulers
                                to create. This is not very flexible.</para>
                        </listitem>
                        <listitem>
                            <para>If a servant is taking too long, it blocks all other servants
                                living inside this thread context. This increases latency.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>We can increase the flexibility and reduce latency by using a
                        <code>multiple_thread_scheduler</code>. This scheduler takes as first
                    argument a number of threads to use and a maximum number of client "worlds"
                    (clients living logically in the same thread context). What it does, is to
                    assign any of its threads to different client worlds, but only one thread can
                    service a world at a time. This means that the thread safety of servants is
                    preserved. At the same time, having any number of threads decreases latency
                    because if a servant keeps its thread busy, it does not block other servants
                    from being serviced. As we can choose the number of threads this scheduler will
                    use, we achieve very fine granularity in planing our thread resources.</para>
                <para>Another interesting characteristics of this scheduler is that its threads
                    service its servants in order. If a thread serviced servant x, it next tries to
                    service servant x+1. This makes for good pipelining capabilities as it increases
                    the odds that task is koved from a pipeline stage to the next one by the same
                    thread and will be hot in its cache.</para>
            </sect1>
            <sect1>
                <title>Processor binding</title>
                <para>TODO example and code.On many systems, it can improve performance to bind
                    threads to a processor: better cache usage is likely as the OS does not move
                    threads from core to core. Mostly for threadpools this is an option you might
                    want to try.</para>
                <para>Usage is very simple. One needs to call
                        <code>processor_bind(core_index)</code> on a scheduler proxy. This function
                    takes a single argument, the core to which the first thread of the pool will be
                    bound. The second thread will be bound to core+1, etc.</para>
            </sect1>
            <sect1>
                <title><command xml:id="asio_scheduler"/>asio_scheduler</title>
                <para>Asynchronous supports the possibility to use Boost.Asio as a threadpool
                    provider. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>asio_scheduler is delivered with a way to access Asio's io_service
                                from a servant object living inside the scheduler.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler handles the necessary work for creating a pool of
                                threads for multithreaded-multi-io_service communication.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler threads implement work-stealing from other
                                Asynchronous schedulers. This allows communication threads to help
                                other threadpools when no I/O communication is happening. This helps
                                reducing thread oversubscription.</para>
                        </listitem>
                        <listitem>
                            <para>One has all the usual goodies of Asynchronous: safe callbacks,
                                object tracking, servant proxies, etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Let's create a simple but powerful example to illustrate its usage. We want to
                    create a TCP client, which connects several times to the same server, gets data
                    from it (in our case, the Boost license will do), then checks if the data is
                    coherent by comparing the results two-by-two. Of course, the client has to be
                    perfectly asynchronous and never block. We also want to guarantee some threads
                    for the communication and some for the calculation work. We also want to
                    communication threads to "help" by stealing some work if necessary.</para>
                <para>Let's start by creating a TCP client using Boost.Asio. A slightly modified
                    version of the async TCP client from the Asio documentation will do. All we
                    change is pass it a callback which it will call when the requested data is
                    ready. We now pack it into an Asynchronous trackable servant:</para>
                <programlisting>// Objects of this type are made to live inside an asio_scheduler,
// they get their associated io_service object from Thread Local Storage
struct AsioCommunicationServant : boost::asynchronous::trackable_servant&lt;>
{
    AsioCommunicationServant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,
                             const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_client(*<emphasis role="bold">boost::asynchronous::get_io_service&lt;>()</emphasis>,server,path)
    {}
    void test(std::function&lt;void(std::string)> cb)
    {
        // just forward call to asio asynchronous http client
        // the only change being the (safe) callback which will be called when http get is done
        m_client.request_content(cb);
    }
private:
    client m_client; //client is from Asio example
};</programlisting>
                <para>The main noteworthy thing to notice is the call to <emphasis role="bold"
                        >boost::asynchronous::get_io_service&lt;>()</emphasis>, which, using
                    thread-local-storage, gives us the io_service associated with this thread (one
                    io_service per thread). This is needed by the Asio TCP client. Also noteworthy
                    is the argument to <code>test()</code>, a callback when the data is available. </para>
                <para>Wait a minute, is this not unsafe (called from an asio worker thread)? It is
                    but it will be made safe in a minute.</para>
                <para>We now need a proxy so that this communication servant can be safely used by
                    others, as usual:</para>
                <programlisting>class AsioCommunicationServantProxy: public boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >
{
public:
    // ctor arguments are forwarded to AsioCommunicationServant
    template &lt;class Scheduler>
    AsioCommunicationServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >(s,server,path)
    {}
    // we offer a single member for posting
    BOOST_ASYNC_POST_MEMBER(test)
};                   </programlisting>
                <para>A single member, <code>test</code>, is used in the proxy. The constructor
                    takes the server and relative path to the desired page. We now need a manager
                    object, which will trigger the communication, wait for data, check that the data
                    is coherent:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_check_string_count(0)
    {
        // as worker we use a simple threadpool scheduler with 4 threads (0 would also do as the asio pool steals)
        auto worker_tp = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (4));

        // for tcp communication we use an asio-based scheduler with 3 threads
        auto asio_workers = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(3));

        // we create a composite pool whose only goal is to allow asio worker threads to steal tasks from the threadpool
        m_pools = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::composite_threadpool_scheduler&lt;> (worker_tp,asio_workers));

        set_worker(worker_tp);
        // we create one asynchronous communication manager in each thread
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
    }
... //to be continued                 
                </programlisting>
                <para>We create 3 pools:<itemizedlist>
                        <listitem>
                            <para>A worker pool for calculations (page comparisons)</para>
                        </listitem>
                        <listitem>
                            <para>An asio threadpool with 3 threads in which we create 3
                                communication objects.</para>
                        </listitem>
                        <listitem>
                            <para>A composite pool which binds both pools together into one stealing
                                unit. You could even set the worker pool to 0 thread, in which case
                                the worker will get its work done when the asio threads have nothing
                                to do. Only non- multiqueue schedulers support this. The worker pool
                                is now made to be the worker pool of this object using
                                    <code>set_worker()</code>.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>We then create our communication objects inside the asio pool.</para>
                <para><emphasis role="underline">Note</emphasis>: asio pools can steal from other
                    pools but not be stolen from. Let's move on to the most interesting part:</para>
                <programlisting>void get_data()
{
    // provide this callback (executing in our thread) to all asio servants as task result. A string will contain the page
    std::function&lt;void(std::string)> f =            
...
    m_asio_comm[0].test(make_safe_callback(f));
    m_asio_comm[1].test(make_safe_callback(f));
    m_asio_comm[2].test(make_safe_callback(f));
}</programlisting>
                <para>We skip the body of f for the moment. f is a task which will be posted to each
                    communication servant so that they can do the same work:<itemizedlist>
                        <listitem>
                            <para>call the same http get on an asio servants</para>
                        </listitem>
                        <listitem>
                            <para>at each callback, check if we got all three callbacks</para>
                        </listitem>
                        <listitem>
                            <para>if yes, post some work to worker threadpool, compare the returned
                                strings (should be all the same)</para>
                        </listitem>
                        <listitem>
                            <para>if all strings equal as they should be, cout the page</para>
                        </listitem>
                    </itemizedlist></para>
                <para>All this will be doine in a single functor. This functor is passed to each
                    communication servant, packed into a make_safe_callback, which, as its name
                    says, transforms the unsafe functor into one which posts this callback functor
                    to the manager thread and also tracks it to check if still alive at the time of
                    the callback. By calling <code>test()</code>, we trigger the 3 communications,
                    and f will be called 3 times. The body of f is:</para>
                <programlisting>std::function&lt;void(std::string)> f =
                [this](std::string s)
                {
                   this->m_requested_data.push_back(s);
                   // poor man's state machine saying we got the result of our asio requests :)
                   if (this->m_requested_data.size() == 3)
                   {
                       // ok, this has really been called for all servants, compare.
                       // but it could be long, so we will post it to threadpool
                       std::cout &lt;&lt; "got all tcp data, parallel check it's correct" &lt;&lt; std::endl;
                       std::string s1 = this->m_requested_data[0];
                       std::string s2 = this->m_requested_data[1];
                       std::string s3 = this->m_requested_data[2];
                       // this callback (executing in our thread) will be called after each comparison
                       auto cb1 = [this,s1](boost::asynchronous::expected&lt;bool> res)
                       {
                          if (res.get())
                              ++this->m_check_string_count;
                          else
                              std::cout &lt;&lt; "uh oh, the pages do not match, data not confirmed" &lt;&lt; std::endl;
                          if (this->m_check_string_count ==2)
                          {
                              // we started 2 comparisons, so it was the last one, data confirmed
                              std::cout &lt;&lt; "data has been confirmed, here it is:" &lt;&lt; std::endl;
                              std::cout &lt;&lt; s1;
                          }
                       };
                       auto cb2=cb1;
                       // post 2 string comparison tasks, provide callback where the last step will run
                       this->post_callback([s1,s2](){return s1 == s2;},std::move(cb1));
                       this->post_callback([s2,s3](){return s2 == s3;},std::move(cb2));
                   }
                };        
                </programlisting>
                <para> We start by checking if this is the third time this functor is called (this,
                    the manager, is nicely serving as holder, kind of poor man's state machine
                    counting to 3). If yes, we prepare a call to the worker pool to compare the 3
                    returned strings 2 by 2 (cb1, cb2). Again, simple state machine, if the callback
                    is called twice, we are done comparing string 1 and 2, and 2 and 3, in which
                    case the page is confirmed and cout'ed. The last 2 lines trigger the work and
                    post to our worker pool (which is the threadpool scheduler, or, if stealing
                    happens, the asio pool) two comparison tasks and the callbacks.</para>
                <para>Our manager is now ready, we still need to create for it a proxy so that it
                    can be called from the outside world asynchronously, then create it in its own
                    thread, as usual:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s,server,path)
    {}
    // get_data is posted, no future, no callback
    BOOST_ASYNC_POST_MEMBER(get_data)
};
...              
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;> >);
{
   ServantProxy proxy(scheduler,"www.boost.org","/LICENSE_1_0.txt");
   // call member, as if it was from Servant
   proxy.get_data();
   // if too short, no problem, we will simply give up the tcp requests
   // this is simply to simulate a main() doing nothing but waiting for a termination request
   boost::this_thread::sleep(boost::posix_time::milliseconds(2000));
}
                </programlisting>
                <para> As usual, <link xlink:href="examples/example_asio_http_client.cpp">here the
                        complete, ready-to-use example</link> and the implementation of the <link
                        xlink:href="examples/asio/asio_http_async_client.hpp">Boost.Asio HTTP
                        client</link>. </para>
            </sect1>
            <sect1>
                <title>Timers</title>
                <para>Very often, an Active Object servant acting as an asynchronous dispatcher will
                    post tasks which have to be done until a certain point in the future, or which
                    will start only at a later point. State machines also regularly make use of a
                    "time" event.</para>
                <para>For this we need a timer, but a safe one:<itemizedlist>
                        <listitem>
                            <para>The timer callback has to be posted to the Active Object thread to
                                avoid races.</para>
                        </listitem>
                        <listitem>
                            <para>The timer callback shall not be called if the servant making the
                                request has been deleted (it can be an awfully long time until the
                                callback).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Asynchronous itself has no timer, but Boost.Asio does, so the library provides
                    a wrapper around it and will allow us to create a timer using an
                    asio::io_service running in its own thread or in an asio threadpool, provided by
                    the library.</para>
                <sect2>
                    <title>Constructing a timer</title>
                    <para>One first needs an <code>asio_scheduler</code> with at least one
                        thread:</para>
                    <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> asio_sched = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(1));               
                    </programlisting>
                    <para>The Servant living in its ActiveObject thread then creates a timer (as
                        attribute to keep it alive) using this scheduler and a timer value:</para>
                    <programlisting> boost::asynchronous::asio_deadline_timer_proxy m_timer (asio_sched,boost::posix_time::milliseconds(1000));                   
                    </programlisting>
                    <para>It can now start the timer using <code>trackable_servant</code> (its base
                            class)<code>::async_wait</code>, passing it a functor call when timer
                        expires / is cancelled:</para>
                    <programlisting> async_wait(m_timer,
            [](const ::boost::system::error_code&amp; err)
            {
                std::cout &lt;&lt; "timer expired? "&lt;&lt; std::boolalpha &lt;&lt; (bool)err &lt;&lt; std::endl; //true if expired, false if cancelled
            } 
            );                  </programlisting>
                    <para>Canceling or recreating the timer means destroying (and possibly
                        recreating) the timer object:</para>
                    <programlisting> m_timer =  boost::asynchronous::asio_deadline_timer_proxy(get_worker(),boost::posix_time::milliseconds(1000));                                   
                    </programlisting>
                    <para>Alternatively, asio_deadline_timer_proxy offers a reset(duration) member,
                        which is more efficient than recreating a proxy. The <link
                            xlink:href="examples/example_asio_deadline_timer.cpp">following example
                        </link> displays a servant using an asio scheduler as a thread pool and
                        creating there its timer object. Note how the timer is created using the
                        worker scheduler of its owner.</para>
                </sect2>
            </sect1>
            <sect1>
                <title><command xml:id="callback_continuations"/>Continuation tasks</title>
                <para>A common limitation of threadpools is support for recursive tasks: tasks start
                    other tasks, which start other tasks and wait for them to complete to do a merge
                    of the part-results. Unfortunately, all threads in the threadpool will soon be
                    busy waiting and no task will ever complete. One can achieve this with a
                    controller object or state machine in a single-threaded scheduler waiting for
                    callbacks, but for very small tasks, using callbacks might just be too
                    expensive. In such cases, Asynchronous provides continuations: a task executes,
                    does something then creates a continuation which will be excuted as soon as all
                    child tasks complete.</para>
                <sect2>
                    <title>General</title>
                    <para>The Hello World of recursive tasks is a parallel fibonacci. The naive
                        algorithm creates a task calculating fib(n). For this it will start a fib(n-1)
                        and fib(n-2) and block until both are done. These tasks will start more tasks,
                        etc. until a cutoff number, at which point recursion stops and fibonacci is
                        calculated serially. This approach has some problems: to avoid thread explosion,
                        we would need fibers, which are not available in Boost at the time of this
                        writing. Even with fibers, tasks would block, which means interrupting them is
                        not possible, and a stack will have to be paid for both. Performance will also
                        suffer. Furthermore, blocking simply isn't part of the asynchronous philosophy
                        of the library. Let's have a look how callback continuation tasks let us
                        implement a parallel fibonacci.</para>
                    <para>First of all, we need a serial fibonacci when n is less than the cutoff. This
                        is a classical one:</para>
                    <programlisting> long serial_fib( long n ) {
    if( n&lt;2 )
        return n;
    else
        return serial_fib(n-1)+serial_fib(n-2);
}</programlisting>
                    <para> We now need a recursive-looking fibonacci task: </para>
                    <programlisting>// our recursive fibonacci tasks. Needs to inherit continuation_task&lt;value type returned by this task>
struct fib_task : public <emphasis role="bold">boost::asynchronous::continuation_task&lt;long></emphasis>
{
    fib_task(long n,long cutoff):n_(n),cutoff_(cutoff){}
    // called inside of threadpool
    void operator()()const
    {
        // the result of this task, will be either set directly if &lt; cutoff, otherwise when taks is ready
        boost::asynchronous::<emphasis role="bold">continuation_result&lt;long> task_res = this_task_result()</emphasis>;
        if (n_&lt;cutoff_)
        {
            // n &lt; cutoff => execute immediately
            task_res.set_value(serial_fib(n_));
        }
        else
        {
            // n>= cutoff, create 2 new tasks and when both are done, set our result (res(task1) + res(task2))
            boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
                        // called when subtasks are done, set result of the calling task
                        [task_res](std::tuple&lt;boost::asynchronous::expected&lt;long>,boost::asynchronous::expected&lt;long> > res) mutable
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        // recursive tasks
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_));
        }
    }
    long n_;
    long cutoff_;
};             </programlisting>
                    <para> Our task need to inherit
                        <code>boost::asynchronous::continuation_task&lt;R></code> where R is the
                        returned type. This class provides us with <code>this_task_result()</code> where
                        we set the task result. This is done either immediately if n &lt; cutoff (first
                        if clause), or (else clause) using a continuation.</para>
                    <para>If n>= cutoff, we create a continuation. This is a sleeping task, which will
                        get activated when all required tasks complete. In this case, we have two
                        fibonacci sub tasks. The template argument is the return type of the
                        continuation. We create two sub-tasks, for n-1 and n-2 and when they complete,
                        the completion functor passed as first argument is called.</para>
                    <para>Note that <code>boost::asynchronous::create_continuation</code> is a variadic
                        function, there can be any number of sub-tasks. The completion functor takes as
                        single argument a tuple of <code>expected</code>, one for each subtask. The
                        template argument of the future is the template argument of
                        <code>boost::asynchronous::continuation_task</code> of each subtask. In this
                        case, all are of type long, but it's not a requirement.</para>
                    <para>When this completion functor is called, we set our result to be result of
                        first task + result of second task. </para>
                    <para>The main particularity of this solution is that a task does not block until
                        sub-tasks complete but instead provides a functor to be called asynchronously as
                        soon as subtasks complete.</para>
                    <para>All what we still need to do is create the first task. In the tradition of
                        Asynchronous, we show it inside an asynchronous servant which posts the first
                        task and waits for a callback, but the same is of course possible using
                        <code>post_future</code>:</para>
                    <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
...
   void calc_fibonacci(long n,long cutoff)
   {
      post_callback(
            // work
            [n,cutoff]()
            {
                // a top-level continuation is the first one in a recursive serie.
                // Its result will be passed to callback
                <emphasis role="bold">return</emphasis> boost::asynchronous::<emphasis role="bold">top_level_callback_continuation&lt;long></emphasis>(fib_task(n,cutoff));
            },
            // callback with fibonacci result.
            [](boost::asynchronous::expected&lt;long> res){...}// callback functor.
        );                                 
   }  
};          </programlisting>
                    <para> We call <code>post_callback</code>, which, as usual, ensures that the
                        callback is posted to the right thread and the servant lifetime is tracked.
                        The posted task calls
                            <code>boost::asynchronous::top_level_callback_continuation&lt;task-return-type></code>
                        to create the first, top-level continuation, passing it a first fib_task.
                        This is non-blocking, a special version of <code>post_callback</code>
                        recognizes a continuation and will call its callback (with a
                            <code>expected&lt;task-return-type></code>) only when the calculation is
                        finished, not when the "work" lambda returns. For this to work, <emphasis
                            role="bold">it is essential not to forget the return
                            statement</emphasis>. Without it, the compiler will unhappily remark
                        that an <code>expected&lt;void></code> cannot be casted to an
                            <code>expected&lt;long></code>, or worse if one expects an
                            <code>expected&lt;void></code>, the callback would be called to
                        early.</para>
                    <para>As usual, calling get() on the expected is non-blocking, one gets either the
                        result or an exception if thrown by a task.</para>
                    <para>Please have a look at the <link xlink:href="examples/example_fibonacci.cpp"
                        >complete example</link>.</para>
                </sect2>
                <sect2>
                    <title>Logging</title>
                    <para>What about logging? We don't want to give up this feature of course and
                        would like to know how long all these fib_task took to complete. This is
                        done through minor changes. As always we need a job:</para>
                    <programlisting>typedef boost::asynchronous::any_loggable&lt;std::chrono::high_resolution_clock> servant_job;                                                 </programlisting>
                    <para> We give the logged name of the task in the constructor of fib_task, for
                        example fib_task_xxx:</para>
                    <programlisting>fib_task(long n,long cutoff)
        : boost::asynchronous::continuation_task&lt;long>("fib_task_" + boost::lexical_cast&lt;std::string>(n))
        ,n_(n),cutoff_(cutoff){}                                                </programlisting>
                    <para>And call <code>boost::asynchronous::create_continuation_job</code> instead of
                        <code>boost::asynchronous::create_continuation</code>:</para>
                    <programlisting>boost::asynchronous::<emphasis role="bold">create_callback_continuation_job</emphasis>&lt;servant_job>(
                        [task_res](std::tuple&lt;boost::asynchronous::expected&lt;long>,boost::asynchronous::expected&lt;long> > res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_)
);                                              </programlisting>
                    <para> Inside the servant we might optionally want the version of post_callback with
                        name, and we need to use <code>top_level_continuation_job</code> instead of
                        <code>top_level_continuation</code>:</para>
                    <programlisting>post_callback(
              [n,cutoff]()
              {
                   return boost::asynchronous::<emphasis role="bold">top_level_callback_continuation_job</emphasis>&lt;long,servant_job>(fib_task(n,cutoff));
              },// work
              // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
              [this](boost::asynchronous::expected&lt;long> res){...},// callback functor.
              <emphasis role="bold">"calc_fibonacci"</emphasis>
        );                             
                </programlisting>
                    <para> The previous example has been <link
                        xlink:href="examples/example_fibonacci_log.cpp">rewritten with logs and a
                        display of all tasks</link> (beware, with higher fibonacci numbers, this can
                        become a long list).</para>
                    <para><emphasis role="underline">Limitation</emphasis>: in the current
                        implementation, tasks are logged, but the continuation callback is not. If it
                        might take long, one should post a (loggable) task.</para>
                    <para><emphasis role="underline">Note</emphasis>: to improve performance, the last
                        task passed to <emphasis role="bold"
                            >create_callback_continuation(_job)</emphasis> is not posted but executed
                        directly so it will execute under the name of the task calling <emphasis
                            role="bold">create_callback_continuation(_job)</emphasis>.</para>
                    <para><emphasis role="bold"><emphasis role="underline">Important note about
                        exception safety</emphasis></emphasis>. The passed <emphasis role="bold"
                            >expected</emphasis> contains either a result or an exception. Calling get()
                        will throw contained exceptions. You should catch it, in the continuation
                        callback and in the task itself. Asynchronous will handle the exception, but it
                        cannot set the <emphasis role="bold">continuation_result</emphasis>, which will
                        never be set and the callback part of post_callback never called. This simple
                        example does not throw, so we save ourselves the cost, but more complicated
                        algorithms should take care of this.</para>
                </sect2>
                <sect2>
                    <title>Creating a variable number of tasks for a continuation</title>
                    <para>It is sometimes not possible to know at compile-time the number of tasks
                        or even the types of tasks used in the creation of a continuation. In this
                        cases, Asynchronous provides more possibilities: <itemizedlist>
                            <listitem>
                                <para>Pack all subtasks of a same type into a std::vector, then pass
                                    it to <code>create_callback_continuation or
                                        create_callback_continuation_job</code>. In this case, we
                                    know that these subtasks all have the same type, so our
                                    continuation is called with a
                                        <code>vector&lt;expected&lt;return_type>></code>:</para>
                                            <programlisting>struct sub_task : public boost::asynchronous::continuation_task&lt;long>
{
    // some task with long as result type
};
struct main_task : public boost::asynchronous::continuation_task&lt;long>
{
  void operator()()
  {
     boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
     <emphasis role="bold">std::vector&lt;sub_task></emphasis> subs;
     subs.push_back(sub_task());
     subs.push_back(sub_task());
     subs.push_back(sub_task()); 
                                         
     boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
          [task_res](<emphasis role="bold">std::vector&lt;boost::asynchronous::expected&lt;long>></emphasis> res)
          {
             long r = res[0].get() + res[1].get() + res[2].get();
             task_res.set_value(r);
          },
          <emphasis role="bold">std::move(subs)</emphasis>);
   }
};</programlisting>
                            </listitem>
                            <listitem>
                                <para>If the subtasks have different type, but a common result type,
                                    we can pack them into a
                                    <code>std::vector&lt;boost::asynchronous::any_continuation_task&lt;return_type>></code>
                                    instead, the rest of the code staying the same:</para>
                                        <programlisting><emphasis role="bold">#include &lt;boost/asynchronous/any_continuation_task.hpp></emphasis>

struct sub_task : public boost::asynchronous::continuation_task&lt;long>
{
    // some task with long as result type
};
struct main_task2 : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()
    {
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        <emphasis role="bold">std::vector&lt;boost::asynchronous::any_continuation_task&lt;long>></emphasis> subs;
        subs.push_back(sub_task());
        subs.push_back(sub_task2());
        subs.push_back(sub_task3());

        boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
             [task_res](<emphasis role="bold">std::vector&lt;boost::asynchronous::expected&lt;long>></emphasis> res)
             {
                 long r = res[0].get() + res[1].get() + res[2].get();
                  task_res.set_value(r);
             },
             <emphasis role="bold">std::move(subs)</emphasis>);
    }
};</programlisting>
                            </listitem>
                            <listitem>
                                <para>Of course, if we have continuations in the first place,
                                    returned by
                                        <code>top_level_callback_continuation&lt;task-return-type></code>
                                    or
                                        <code>top_level_callback_continuation&lt;task-return-type></code>,
                                    as all of Asynchronous' algorithms do, these can be packed into
                                    a vector as well:</para>
                                <programlisting>struct main_task3 : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()
    {
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        <emphasis role="bold">std::vector&lt;boost::asynchronous::detail::callback_continuation&lt;long>></emphasis> subs;
        std::vector&lt;long> data1(10000,1);
        std::vector&lt;long> data2(10000,1);
        std::vector&lt;long> data3(10000,1);
        subs.<emphasis role="bold">push_back</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(std::move(data1),
                                                            [](long const&amp; a, long const&amp; b)
                                                            {
                                                              return a + b;
                                                            },1000));
        subs.<emphasis role="bold">push_back</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(std::move(data2),
                                                            [](long const&amp; a, long const&amp; b)
                                                            {
                                                              return a + b;
                                                            },1000));
        subs.<emphasis role="bold">push_back</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(std::move(data3),
                                                            [](long const&amp; a, long const&amp; b)
                                                            {
                                                              return a + b;
                                                            },1000));

        boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
                        [task_res](<emphasis role="bold">std::vector&lt;boost::asynchronous::expected&lt;long>></emphasis> res)
                        {
                            long r = res[0].get() + res[1].get() + res[2].get();
                            task_res.set_value(r);
                        },
                        <emphasis role="bold">std::move(subs)</emphasis>);
    }
};</programlisting>                                
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title>Creating a continuation from a simple functor</title>
                    <para>For very simple tasks, it is in a post C++11 world annoying to have to
                        write a functor class like our above sub_task. For such cases, Asynchronous
                        provides a simple helper function:</para>
                    <para><code>auto make_lambda_continuation_wrapper(functor f, std::string
                            const&amp; name="")</code> where auto will be a
                            <code>continuation_task</code>. We can replace our first case above by a
                        more concise:</para>
                    <programlisting>struct main_task4 : public boost::asynchronous::continuation_task&lt;int>
{
    void operator()()
    {
        // 15, 22,5 are of type int
        boost::asynchronous::continuation_result&lt;<emphasis role="bold">int</emphasis>> task_res = this_task_result();
        <emphasis role="bold">std::vector&lt;boost::asynchronous::any_continuation_task&lt;int>></emphasis> subs;
        subs.push_back(boost::asynchronous::<emphasis role="bold">make_lambda_continuation_wrapper</emphasis>([](){return 15;}));
        subs.push_back(boost::asynchronous::<emphasis role="bold">make_lambda_continuation_wrapper</emphasis>([](){return 22;}));
        subs.push_back(boost::asynchronous::<emphasis role="bold">make_lambda_continuation_wrapper</emphasis>([](){return 5;}));

        boost::asynchronous::create_callback_continuation(
                        [task_res](std::vector&lt;boost::asynchronous::expected&lt;int>> res)
                        {
                            int r = res[0].get() + res[1].get() + res[2].get();
                            task_res.set_value(r);
                        },
                        <emphasis role="bold">std::move(subs)</emphasis>);
    }
};</programlisting>
                </sect2>
            </sect1>
            <sect1>
                <title><command xml:id="continuations"/>Future-based continuations</title>              
                <para>The continuations shown above are the fastest offered by Asynchronous.
                    Sometimes, however, we are forced to use libraries returning us only a future.
                    In this case, Asynchronous also offers "simple" continuations, which are
                    future-based. Consider the following trivial example. We consider we have a
                    task, called sub_task. We will simulate the future-returning library using
                        <code>post_future</code>. We want to divide our work between sub_task
                    instances, getting a callback when all complete. We can create a continuation
                    using these futures:</para>
                <programlisting>// our main algo task. Needs to inherit continuation_task&lt;value type returned by this task>
struct main_task : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()const
    {
        // the result of this task
       <emphasis role="bold"> boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();</emphasis>

        // we start calculation, then while doing this we see new tasks which can be posted and done concurrently to us
        // when all are done, we will set the result
        // to post tasks, we need a scheduler
        boost::asynchronous::any_weak_scheduler&lt;> weak_scheduler = boost::asynchronous::get_thread_scheduler&lt;>();
        boost::asynchronous::any_shared_scheduler&lt;> locked_scheduler = weak_scheduler.lock();
        if (!locked_scheduler.is_valid())
            // ok, we are shutting down, ok give up
            return;
        // simulate algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        std::future&lt;int> fu1 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        // simulate more algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        std::future&lt;int> fu2 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        // simulate algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        std::future&lt;int> fu3 = boost::asynchronous::post_future(locked_scheduler,sub_task());

        // our algo is now done, wrap all and return
        boost::asynchronous::<emphasis role="bold">create_continuation</emphasis>(
                    // called when subtasks are done, set our result
                    [task_res](std::tuple&lt;std::future&lt;int>,std::future&lt;int>,std::future&lt;int> > res)
                    {
                        try
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get()+ std::get&lt;2>(res).get();
                            <emphasis role="bold">task_res.set_value(r);</emphasis>
                        }
                        catch(...)
                        {
                            <emphasis role="bold">task_res.set_exception(std::current_exception());</emphasis>
                        }
                    },
                    // future results of recursive tasks
                    <emphasis role="bold">std::move(fu1),std::move(fu2),std::move(fu3)</emphasis>);
    }
    };                                               </programlisting>
                <para>Please have a look at <link xlink:href="examples/example_continuation_algo.cpp">the complete
                    example</link></para>
                <para>Our tasks starts by posting 3 instances of sub_task, each time getting a
                    future. We then call <emphasis role="bold">create_continuation(_job)</emphasis>,
                    passing it the futures. When all futures are ready (have a value or an
                    exception), the callback is called, with 3 futures containing the result.</para>
                <para>Advantage:<itemizedlist>
                        <listitem>
                            <para>can be used with any library returning a std::future</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Drawbacks:<itemizedlist>
                        <listitem>
                            <para>lesser performance</para>
                        </listitem>
                        <listitem>
                            <para>the thread calling <emphasis role="bold"
                                    >create_continuation(_job)</emphasis> polls until all futures
                                are set. If this thread is busy, the callback is delayed.</para>
                        </listitem>
                </itemizedlist></para>
                <para><emphasis role="bold"><emphasis role="underline">Important
                        note</emphasis></emphasis>: Like for the previous callback continuations,
                    tasks and continuation callbacks should catch exceptions.</para>
                <para><emphasis role="bold">create_continuation(_job)</emphasis> has a wider
                    interface. It can also take a vector of futures instead of a variadic
                    version, for example:</para>
                <programlisting>// our main algo task. Needs to inherit continuation_task&lt;value type returned by this task>
struct main_task : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()const
    {
        // the result of this task
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();

        // we start calculation, then while doing this we see new tasks which can be posted and done concurrently to us
        // when all are done, we will set the result
        // to post tasks, we need a scheduler
        boost::asynchronous::any_weak_scheduler&lt;> weak_scheduler = boost::asynchronous::get_thread_scheduler&lt;>();
        boost::asynchronous::any_shared_scheduler&lt;> locked_scheduler = weak_scheduler.lock();
        if (!locked_scheduler.is_valid())
            // ok, we are shutting down, ok give up
            return;
        // simulate algo work
        std::vector&lt;std::future&lt;int> > fus;
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        std::future&lt;int> fu1 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        fus.emplace_back(std::move(fu1));
        // simulate more algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        std::future&lt;int> fu2 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        fus.emplace_back(std::move(fu2));
        // simulate algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        std::future&lt;int> fu3 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        fus.emplace_back(std::move(fu3));

        // our algo is now done, wrap all and return
        boost::asynchronous::<emphasis role="bold">create_continuation</emphasis>(
                    // called when subtasks are done, set our result
                    [task_res](std::vector&lt;std::future&lt;int>> res)
                    {
                        try
                        {
                            long r = res[0].get() + res[1].get() + res[2].get();
                            task_res.set_value(r);
                        }
                        catch(...)
                        {
                            task_res.set_exception(std::current_exception());
                        }
                    },
                    // future results of recursive tasks
                    <emphasis role="bold">std::move(fus)</emphasis>);
    }
    };                                            </programlisting>
                <para>The drawback is that in this case, all futures must be of the same type.
                    Please have a look at <link xlink:href="examples/example_continuation_algo2.cpp"
                        >the complete example</link></para>
            </sect1>
            <sect1>
                <title><command xml:id="distributing"/>Distributing work among machines</title>
                <para>At the time of this writing, a core i7-3930K with 6 cores and 3.2 GHz will
                    cost $560, so say $100 per core. Not a bad deal, so you buy it. Unfortunately,
                    some time later you realize you need more power. Ok, there is no i7 with more
                    cores and an Extreme Edition will be quite expensive for only a little more
                    power so you decide to go for a Xeon. A 12-core E5-2697v2 2.7GHz will go for
                    almost $3000 which means $250 per core, and for this you also have a lesser
                    frequency. And if you need later even more power, well, it will become really
                    expensive. Can Asynchronous help us use more power for cheap, and at best, with
                    little work? It does, as you guess ;-)</para>
                <para>Asynchronous provides a special pool, <code>tcp_server_scheduler</code>, which
                    will behave like any other scheduler but will not execute work itself, waiting
                    instead for clients to connect and steal some work. The client execute the work
                    on behalf of the <code>tcp_server_scheduler</code> and sends it back the
                    results. </para>
                <para>For this to work, there is however a condition: jobs must be (boost)
                    serializable to be transferred to the client. So does the returned value.</para>
                <para>Let's start with a <link xlink:href="examples/example_tcp_server.cpp">simplest
                        example</link>:</para>
                <programlisting>// notice how the worker pool has a different job type
struct Servant : boost::asynchronous::trackable_servant&lt;boost::asynchronous::any_callable,<emphasis role="bold">boost::asynchronous::any_serializable</emphasis>>
{
  Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler)
        : boost::asynchronous::trackable_servant&lt;boost::asynchronous::any_callable,<emphasis role="bold">boost::asynchronous::any_serializable</emphasis>>(scheduler)
  {
        // let's build our pool step by step. First we need a worker pool
        // possibly for us, and we want to share it with the tcp pool for its serialization work
        boost::asynchronous::any_shared_scheduler_proxy&lt;> workers = boost::asynchronous::make_shared_scheduler_proxy&lt;
                                                                            boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;>>>(3);

        // we use a tcp pool using the 3 worker threads we just built
        // our server will listen on "localhost" port 12345
        auto pool= boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::tcp_server_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>>>
                                (workers,"localhost",12345);
        // and this will be the worker pool for post_callback
        set_worker(pool);
  }
};</programlisting>
                <para>We start by creating a worker pool. The <code>tcp_server_scheduler</code> will
                    delegate to this pool all its serialization / deserialization work. For maximum
                    scalability we want this work to happen in more than one thread.</para>
                <para>Note that our job type is no more a simple callable, it must be
                    (de)serializable too (<emphasis role="bold"
                        >boost::asynchronous::any_serializable</emphasis>).</para>
                <para>Then we need a <code>tcp_server_scheduler</code> listening on, in this case,
                    localhost, port 12345. We now have a functioning worker pool and choose to use
                    it as our worker pool so that we do not execute jobs ourselves (other
                    configurations will be shown soon). Let's exercise our new pool. We first need a
                    task to be executed remotely:</para>
                <programlisting>struct dummy_tcp_task : public boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>
{
    dummy_tcp_task(int d):boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>(<emphasis role="bold">"dummy_tcp_task"</emphasis>),m_data(d){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    int operator()()const
    {
        std::cout &lt;&lt; "dummy_tcp_task operator(): " &lt;&lt; m_data &lt;&lt; std::endl;
        boost::this_thread::sleep(boost::posix_time::milliseconds(2000));
        std::cout &lt;&lt; "dummy_tcp_task operator() finished" &lt;&lt; std::endl;
        return m_data;
    }
    int m_data;
};</programlisting>
                <para>This is a minimum task, only sleeping. All it needs is a
                        <code>serialize</code> member to play nice with Boost.Serialization and it
                    must inherit <code>serializable_task</code>. Giving the task a name is essential
                    as it will allow the client to deserialize it. Let's post to our TCP worker pool
                    some of the tasks, wait for a client to pick them and use the results:</para>
                <programlisting>// start long tasks in threadpool (first lambda) and callback in our thread
for (int i =0 ;i &lt; 10 ; ++i)
{
    std::cout &lt;&lt; "call post_callback with i: " &lt;&lt; i &lt;&lt; std::endl;
    post_callback(
           dummy_tcp_task(i),
           // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
           [this](boost::asynchronous::expected&lt;int> res){
                  try{
                        this->on_callback(res.get());
                  }
                  catch(std::exception&amp; e)
                  {
                       std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                       this->on_callback(0);
                  }
            }// callback functor.
    );
}</programlisting>
                <para>We post 10 tasks to the pool. For each task we will get, at some later
                    undefined point (provided some clients are around), a result in form of a
                    (ready) expected, possibly an exception if one was thrown by the task.</para>
                <para>Notice it is safe to use <code>this</code> in the callback lambda as it will
                    be only called if the servant still exists.</para>
                <para>We still need a client to execute the task, this is pretty straightforward (we
                    will extend it soon):</para>
                <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12346";
    int threads = (argc>3) ? strtol(argv[3],0,0) : 4;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

    auto scheduler = boost::asynchronous::make_shared_scheduler_proxy&lt;boost::asynchronous::asio_scheduler&lt;>>()
    {
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> 
        executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="dummy_tcp_task")
            {
                dummy_tcp_task t(0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_task</emphasis>(t,resp,when_done);
            }
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };

        auto pool = boost::asynchronous::make_shared_scheduler_proxy&lt;
                          boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>>>(threads);
        boost::asynchronous::tcp::<emphasis role="bold">simple_tcp_client_proxy proxy</emphasis>(scheduler,pool,server_address,server_port,executor,
                                                                    0/*ms between calls to server*/);
        std::future&lt;std::future&lt;void> > fu = proxy.run();
        std::future&lt;void> fu_end = fu.get();
        fu_end.get();
    }
    return 0;
}</programlisting>
                <para>We start by taking as command-line arguments the server address and port and
                    the number of threads the client will use to process stolen work from the
                    server. </para>
                <para>We create a single-threaded <code>asio_scheduler</code> for the communication
                    (in our case, this is sufficient, your case might vary) to the server.</para>
                <para>The client then defines an executor function. This function will be called
                    when work is stolen by the client. As Asynchronous does not know what the work
                    type is, we will need to "help" by creating an instance of the task using its
                    name. Calling <code>deserialize_and_call_task</code> will, well, deserialize the
                    task data into our dummy task, then call it. We also choose to return an
                    exception is the task is not known to us.</para>
                <para>Next, we need a pool of threads to execute the work. Usually, you will want
                    more than one thread as we want to use all our cores.</para>
                <para>The simplest client that Asynchronous offers is a
                        <code>simple_tcp_client_proxy</code> proxy. We say simple, because it is
                    only a client. Later on, we will see a more powerful tool.
                        <code>simple_tcp_client_proxy</code> will require the asio pool for
                    communication, the server address and port, our executor and a parameter telling
                    it how often it should try to steal work from a server.</para>
                <para>We are now done, the client will run until killed.</para>
                <para>Let's sum up what we got in these few lines of code:<itemizedlist>
                        <listitem>
                            <para>a pool behaving like any other pool, which can be stolen
                                from</para>
                        </listitem>
                        <listitem>
                            <para>a server which does no work itself, but still scales well as
                                serialization is using whatever threads it is given</para>
                        </listitem>
                        <listitem>
                            <para>a trackable servant working with <code>post_callback</code>, like
                                always</para>
                        </listitem>
                        <listitem>
                            <para>a multithreaded client, which can be tuned precisely to use a
                                given pool for the communication and another (or the same btw.) for
                                work processing.</para>
                        </listitem>
                </itemizedlist></para>
                <para>Interestingly, we have a very versatile client. It is possible to reuse the
                    work processing and communication pools, within the same client application, for
                    a different <code>simple_tcp_client_proxy</code> which would be connecting to another
                    server.</para>
                <para>The server is also quite flexible. It scales well and can handle as many
                    clients as one wishes.</para>
                <para>This is only the beginning of our distributed chapter.</para>
                <sect2>
                    <title>A distributed, parallel Fibonacci</title>
                    <para>Lets's revisit our parallel Fibonacci example. We realize that with higher
                        Fibonacci numbers, our CPU power doesn't suffice any more. We want to
                        distribute it among several machines while our main machine still does some
                        calculation work. To do this, we'll start with our previous example, and
                        rewrite our Fibonacci task to make it distributable.</para>
                    <para>We remember that we first had to call
                            <code>boost::asynchronous::top_level_continuation</code> in our
                        post_callback to make Asynchronous aware of the later return value. The
                        difference now is that even this one-liner lambda could be serialized and
                        sent away, so we need to make it a <code>serializable_task</code>:</para>
                    <programlisting>struct serializable_fib_task : public boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>
{
    serializable_fib_task(long n,long cutoff):boost::asynchronous::<emphasis role="bold">serializable_task("serializable_fib_task")</emphasis>,n_(n),cutoff_(cutoff){}
    template &lt;class Archive>
    <emphasis role="bold">void serialize(Archive &amp; ar, const unsigned int /*version*/)</emphasis>
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    auto operator()()const
        -> decltype(boost::asynchronous::top_level_continuation_log&lt;long,boost::asynchronous::any_serializable>
                    (tcp_example::fib_task(long(0),long(0))))
    {
        auto cont =  boost::asynchronous::top_level_continuation_job&lt;long,boost::asynchronous::any_serializable>
                (tcp_example::fib_task(n_,cutoff_));
        return cont;
    }
    long n_;
    long cutoff_;
};</programlisting>
                <para>We need to make our task serializable and give it a name so that the client
                        application can recognize it. We also need a serialize member, as required
                        by Boost.Serialization. And we need an operator() so that the task can be
                        executed. There is in C++11 an ugly decltype, but C++14 will solve this if
                        your compiler supports it. We also need a few changes in our Fibonacci
                        task:</para>
                    <programlisting>// our recursive fibonacci tasks. Needs to inherit continuation_task&lt;value type returned by this task>
struct fib_task : public boost::asynchronous::continuation_task&lt;long>
                <emphasis role="bold">, public boost::asynchronous::serializable_task</emphasis>
{
    fib_task(long n,long cutoff)
        :  boost::asynchronous::continuation_task&lt;long>()
        <emphasis role="bold">, boost::asynchronous::serializable_task("serializable_sub_fib_task")</emphasis>
        ,n_(n),cutoff_(cutoff)
    {
    }
    <emphasis role="bold">template &lt;class Archive>
    void save(Archive &amp; ar, const unsigned int /*version*/)const
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    template &lt;class Archive>
    void load(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    BOOST_SERIALIZATION_SPLIT_MEMBER()</emphasis>
    void operator()()const
    {
        // the result of this task, will be either set directly if &lt; cutoff, otherwise when taks is ready
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        if (n_&lt;cutoff_)
        {
            // n &lt; cutoff => execute ourselves
            task_res.set_value(serial_fib(n_));
        }
        else
        {
            // n>= cutoff, create 2 new tasks and when both are done, set our result (res(task1) + res(task2))
            boost::asynchronous::create_callback_continuation_job&lt;boost::asynchronous::any_serializable>(
                        // called when subtasks are done, set our result
                        [task_res](std::tuple&lt;std::future&lt;long>,std::future&lt;long> > res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        // recursive tasks
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_));
        }
    }
    long n_;
    long cutoff_;
};</programlisting>
                <para>The few changes are highlighted. The task needs to be a serializable task with
                        its own name in the constructor, and it needs serialization members. That's
                        it, we're ready to distribute!</para>
                    <para>As we previously said, we will reuse our previous TCP example, using
                            <code>serializable_fib_task</code> as the main posted task. This gives
                        us <link xlink:href="examples/example_tcp_server_fib.cpp">this example</link>.</para>
                    <para>But wait, we promised that our server would itself do some calculation
                        work, and we use as worker pool only a <code>tcp_server_scheduler</code>.
                        Right, let's do it now, throwing in a few more goodies. We need a worker
                        pool, with as many threads as we are willing to offer:</para>
                    <programlisting>// we need a pool where the tasks execute
auto <emphasis role="bold">pool</emphasis> = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis>&lt;
                    boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable> >(<emphasis role="bold">threads</emphasis>));</programlisting>
                 <para>This pool will get the fibonacci top-level task we will post, then, if our
                        clients connect after we start, it will get the first sub-tasks. </para>
                    <para>To make it more interesting, let's offer our server to also be a job
                        client. This way, we can build a cooperation network: the server offers
                        fibonacci tasks, but also tries to steal some, thus increasing homogenous
                        work distribution. We'll talk more about this in the next chapter.</para>
                    <programlisting>// a client will steal jobs in this pool
auto cscheduler = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">asio_scheduler</emphasis>&lt;>);
// jobs we will support
std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,
                   std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_callback_continuation_task</emphasis>(fib,resp,when_done);
            }
            else if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_top_level_callback_continuation_task</emphasis>(fib,resp,when_done);
            }
            // else whatever functor we support
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };
boost::asynchronous::tcp::simple_tcp_client_proxy client_proxy(cscheduler,pool,server_address,server_port,executor,
                                                               10/*ms between calls to server*/);</programlisting>
                <para>Notice how we use our worker pool for job serialization / deserialization.
                        Notice also how we check both possible stolen jobs.</para>
                    <para>We also introduce two new deserialization functions.
                            boost::asynchronous::tcp::<emphasis role="bold"
                            >deserialize_and_call_task</emphasis> was used for normal tasks, we now
                        have boost::asynchronous::tcp::<emphasis role="bold"
                            >deserialize_and_call_top_level_callback_continuation_task</emphasis>
                        for our top-level continuation task, and boost::asynchronous::tcp::<emphasis
                            role="bold">deserialize_and_call_callback_continuation_task</emphasis>
                        for the continuation-sub-task.</para>
                    <para>We now need to build our TCP server, which we decide will get only one
                        thread for task serialization. This ought to be enough, Fibonacci tasks have
                        little data (2 long).</para>
                    <programlisting>// we need a server
// we use a tcp pool using 1 worker
auto server_pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;> >(<emphasis role="bold">1</emphasis>));

auto tcp_server= boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">tcp_server_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>,
                            boost::asynchronous::any_callable,true>
                                (server_pool,own_server_address,(unsigned int)own_server_port));</programlisting>
                <para>We have a TCP server pool, as before, even a client to steal work ourselves,
                        but how do we get ourselves this combined pool, which executes some work or
                        gives some away? </para>
                    <para>Wait a minute, combined pool? Yes, a
                            <code>composite_threadpool_scheduler</code> will do the trick. As we're
                        at it, we create a servant to coordinate the work, as we now always
                        do:</para>
                    <programlisting>// we need a composite for stealing
auto composite = boost::asynchronous::create_shared_scheduler_proxy
                (new boost::asynchronous::<emphasis role="bold">composite_threadpool_scheduler</emphasis>&lt;boost::asynchronous::any_serializable>
                          (<emphasis role="bold">pool</emphasis>,<emphasis role="bold">tcp_server</emphasis>));

// a single-threaded world, where Servant will live.
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::lockfree_queue&lt;> >);
{
      ServantProxy proxy(scheduler,<emphasis role="bold">pool</emphasis>);
      // result of BOOST_ASYNC_FUTURE_MEMBER is a shared_future,
      // so we have a shared_future of a shared_future(result of start_async_work)
      std::future&lt;std::future&lt;long> > fu = proxy.calc_fibonacci(fibo_val,cutoff);
      std::future&lt;long> resfu = fu.get();
      long res = resfu.get();
}</programlisting>                    
                <para>Notice how we give only the worker "pool" to the servant. This means, the
                        servant will post the top-level task to it, it will immediately be called
                        and create 2 Fibonacci tasks, which will create each one 2 more, etc. until
                        at some point a client connects and steals one, which will create 2 more,
                        etc.</para>
                    <para>The client will not steal directly from this pool, it will steal from the
                            <code>tcp_server</code> pool, which, as long as a client request comes,
                        will steal from the worker pool, as they belong to the same composite. This
                        will continue until the composite is destroyed, or the work is done. For the
                        sake of the example, we do not give the composite as the Servant's worker
                        pool but keep it alive until the end of calculation. Please have a look at
                        the <link xlink:href="examples/example_tcp_server_fib2.cpp">complete example</link>.</para>
                    <para>In this example, we start taking care of homogenous work distribution by
                        packing a client and a server in the same application. But we need a bit
                        more: our last client would steal work so fast, every 10ms that it would
                        starve the server or other potential client applications, so we're going to
                        tell it to only steal if the size of its work queues are under a certain
                        amount, which we will empirically determine, according to our hardware,
                        network speed, etc.</para>
                    <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12346";
    int threads = (argc>3) ? strtol(argv[3],0,0) : 4;
    // 1..n => check at regular time intervals if the queue is under the given size
    int job_getting_policy = (argc>4) ? strtol(argv[4],0,0):0;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                new boost::asynchronous::asio_scheduler&lt;>);
    {
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> 
        executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_top_level_callback_continuation_task(fib,resp,when_done);
            }
            else if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_callback_continuation_task(fib,resp,when_done);
            }
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };

        // guarded_deque supports queue size
        auto pool = boost::asynchronous::create_shared_scheduler_proxy(
                        new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::<emphasis role="bold">guarded_deque</emphasis>&lt;boost::asynchronous::any_serializable> >(threads));
        // more advanced policy
        // or <emphasis role="bold">simple_tcp_client_proxy&lt;boost::asynchronous::tcp::queue_size_check_policy&lt;>></emphasis> if your compiler can (clang)
        typename boost::asynchronous::tcp::<emphasis role="bold">get_correct_simple_tcp_client_proxy</emphasis>&lt;boost::asynchronous::tcp::queue_size_check_policy&lt;>>::type proxy(
                        scheduler,pool,server_address,server_port,executor,
                        0/*ms between calls to server*/,
                        <emphasis role="bold">job_getting_policy /* number of jobs we try to keep in queue */</emphasis>);
        // run forever
        std::future&lt;std::future&lt;void> > fu = proxy.run();
        std::future&lt;void> fu_end = fu.get();
        fu_end.get();
    }
    return 0;
}</programlisting>                    
                <para>The important new part is highlighted. <code>simple_tcp_client_proxy</code>
                        gets an extra template argument, <code>queue_size_check_policy</code>, and a
                        new constructor argument, the number of jobs in the queue, under which the
                        client will try, every 10ms, to steal a job. Normally, that would be all,
                        but g++ (up to 4.7 at least) is uncooperative and requires an extra level of
                        indirection to get the desired client proxy. Otherwise, there is no
                        change.</para>
                    <para>Notice that our standard lockfree queue offers no size() so we use a less
                        efficient guarded_deque.</para>
                    <para>You will find in the <link xlink:href="examples/simple_tcp_client.cpp"
                            >complete example</link> a few other tasks which we will explain
                        shortly.</para>
                    <para>Let's stop a minute to think about what we just did. We built, with little
                        code, a complete framework for distributing tasks homogenously among
                        machines, by reusing standard component offered by the library: threadpools,
                        composite pools, clients, servers. If we really have client connecting or
                        not is secondary, all what can happen is that calculating our Fibonacci
                        number will last a little longer.</para>
                    <para>We also separate the task (Fibonacci) from the threadpool configuration,
                        from the network configuration, and from the control of the task (Servant),
                        leading us to highly reusable, extendable code.</para>
                    <para>In the next chapter, we will add a way to further distribute work among
                        not only machines, but whole networks. </para>
                </sect2>                
                <sect2>
                    <title>Example: a hierarchical network</title>
                    <para>We already distribute and parallelize work, so we can scale a great deal,
                        but our current model is one server, many clients, which means a potentially
                        high network load and a lesser scalability as more and more clients connect
                        to a server. What we want is a client/server combo application  where the
                        client steals and executes jobs and a server component of the same
                        application which steals jobs from the client on behalf of other clients.
                        What we want is to achieve something like this:</para>
                    <para><inlinemediaobject>
                            <imageobject>
                                <imagedata fileref="pics/TCPHierarchical.jpg"/>
                            </imageobject>
                    </inlinemediaobject></para>
                 <para>We have our server application, as seen until now, called interestingly
                        ServerApplication on a machine called MainJobServer. This machine executes
                        work and offers at the same time a steal-from capability. We also have a
                        simple client called ClientApplication running on ClientMachine1, which
                        steals jobs and executes them itself without further delegating. We have
                        another client machine called ClientMachine2 on which
                        ClientServerApplication runs. This applications has two parts, a client
                        stealing jobs like ClientApplication and a server part stealing jobs from
                        the client part upon request. For example, another simple ClientApplication
                        running on ClientMachine2.1 connects to it and steals further jobs in case
                        ClientMachine2 is not executing them fast enough, or if ClientMachine2 is
                        only seen as a pass-through to move jobs execution to another network.
                        Sounds scalable. How hard is it to build? Not so hard, because in fact, we
                        already saw all we need to build this, so it's kind of a Lego game.</para>
                    <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12345";
    std::string own_server_address = (argc>3) ? argv[3]:"localhost";
    long own_server_port = (argc>4) ? strtol(argv[4],0,0):12346;
    int threads = (argc>5) ? strtol(argv[5],0,0) : 4;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port
         &lt;&lt; " listening on " &lt;&lt; own_server_address &lt;&lt; " port " &lt;&lt; own_server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

// to be continued</programlisting>  
                <para>We take as arguments the address and port of the server we are going to steal
                        from, then our own address and port. We now need a client with its
                        communication asio scheduler and its threadpool for job execution.</para>  
                    <programlisting>auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">asio_scheduler</emphasis>&lt;>);
    { //block start
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,
                           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_top_level_callback_continuation_task(fib,resp,when_done);
            }
            else if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_callback_continuation_task(fib,resp,when_done);
            }
            // else whatever functor we support
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };
        // create pools
        // we need a pool where the tasks execute
        auto pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable> >(<emphasis role="bold">threads</emphasis>));
        boost::asynchronous::tcp::<emphasis role="bold">simple_tcp_client_proxy client_proxy</emphasis>(scheduler,<emphasis role="bold">pool</emphasis>,server_address,server_port,executor,
                                                                       10/*ms between calls to server*/);
// to be continued</programlisting>   
                <para>We now need a server to which more clients will connect, and a composite
                binding it to our worker pool:</para>   
                    <programlisting>   // we need a server
   // we use a tcp pool using 1 worker
   auto server_pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;> >(1));
   auto tcp_server= boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">tcp_server_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>,
                            boost::asynchronous::any_callable,true>
                                (server_pool,own_server_address,(unsigned int)own_server_port));
   // we need a composite for stealing
   auto composite = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">composite_threadpool_scheduler</emphasis>&lt;boost::asynchronous::any_serializable>
                                                                   (<emphasis role="bold">pool</emphasis>,<emphasis role="bold">tcp_server</emphasis>));

   std::future&lt;std::future&lt;void> > fu = client_proxy.run();
   std::future&lt;void> fu_end = fu.get();
   fu_end.get();
} //end block

 return 0;
 } //end main</programlisting>   
                <para>And we're done! The client part will steal jobs and execute them, while the
                        server part, bound to the client pool, will steal on sub-client-demand.
                        Please have a look at the <link xlink:href="examples/tcp_client_server.cpp">
                            complete code</link>.</para>                    
                </sect2>
            </sect1>
            <sect1>
                <title>Picking your archive</title>
                <para>By default, Asynchronous uses a Boost Text archive (text_oarchive,
                    text_iarchive), which is simple and efficient enough for our Fibonacci example,
                    but inefficient for tasks holding more data.</para>
                <para>Asynchronous supports any archive task, requires however a different job type
                    for this. At the moment, we can use a
                        <code>portable_binary_oarchive</code>/<code>portable_binary_iarchive</code>
                    by selecting <code>any_bin_serializable</code> as job. If Boost supports more
                    archive types, support is easy to add.</para>
                <para>The previous Fibonacci server example has been <link
                        xlink:href="examples/example_tcp_server_fib2_bin.cpp">rewritten</link> to use this
                    capability. The <link xlink:href="examples/simple_tcp_client_bin_archive.cpp">client</link> has also been rewritten using this new job type.</para>
            </sect1>
            <sect1>
                <title><command xml:id="parallel_algos"/>Parallel Algorithms (Christophe Henry / Tobias Holl)</title>
                <para>Asynchronous supports out of the box quite some asynchronous parallel
                    algorithms, as well as interesting combination usages. These algorithms are
                    callback-continuation-based. Some of these algorithms also support distributed
                    calculations as long as the user-provided functors are (meaning they must be
                    serializable).</para>
                <para>What is the point of adding yet another set of parallel algorithms which can
                    be found elsewhere? Because truly asynchronous algorithms are hard to find. By
                    this we mean non-blocking. If one needs parallel algorithms, it's because they
                    could need long to complete. And if they take long, we really do not want to
                    block until it happens.</para>
                <para>All of the algorithms are made for use in a worker threadpool. They represent
                    the work part of a <code>post_callback</code>;</para>
                <para>In the philosophy of Asynchronous, the programmer knows better the task size
                    where he wants to start parallelizing, so all these algorithms take a cutoff.
                    Work is cut into packets of this size.</para>
                <para>All range algorithms also have a version taking a continuation as range
                    argument. This allows to combine algorithms functional way, for example this
                    (more to come):</para>
                <programlisting>return <emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(...)));</programlisting>
                <para>Asynchronous implements the following algorithms. It is also indicated whether
                    the algorithm supports arguments passed as iterators, moved range, or
                    continuation. Indicated is also whether the algorithm is distributable.<table
                        frame="all">
                        <title>Non-modifying Algorithms, in boost/asynchronous/algorithm</title>
                        <tgroup cols="5">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                            <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Description</entry>
                                    <entry>Header</entry>
                                    <entry>Input Parameters</entry>
                                    <entry>Distributable</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry><command xlink:href="#parallel_all_of">parallel_all_of</command></entry>
                                    <entry>checks if a predicate is true for all of the elements in
                                        a range </entry>
                                    <entry>parallel_all_of.hpp</entry>
                                    <entry>Iterators, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_any_of">parallel_any_of</command></entry>
                                    <entry>checks if a predicate is true for any of the elements in
                                        a range </entry>
                                    <entry>parallel_any_of.hpp</entry>
                                    <entry>Iterators, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_none_of">parallel_none_of</command></entry>
                                    <entry>checks if a predicate is true for none of the elements in
                                        a range </entry>
                                    <entry>parallel_none_of.hpp</entry>
                                    <entry>Iterators, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_for_each">parallel_for_each</command></entry>
                                    <entry>aplpies a functor to a range of elements and accumulates
                                        the result into the functor</entry>
                                    <entry>parallel_for_each.hpp</entry>
                                    <entry>Iterators</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_for">parallel_for</command></entry>
                                    <entry>applies a function to a range of elements</entry>
                                    <entry>parallel_for.hpp</entry>
                                    <entry>Iterators, moved range, continuation</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_count">parallel_count</command></entry>
                                    <entry>returns the number of elements satisfying specific
                                        criteria </entry>
                                    <entry>parallel_count.hpp</entry>
                                    <entry>Iterators, moved range, continuation</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_count">parallel_count_if</command></entry>
                                    <entry>returns the number of elements satisfying specific
                                        criteria </entry>
                                    <entry>parallel_count_if.hpp</entry>
                                    <entry>Iterators, moved range, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_equal">parallel_equal</command></entry>
                                    <entry>determines if two sets of elements are the same </entry>
                                    <entry>parallel_equal.hpp</entry>
                                    <entry>Iterators</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_mismatch">parallel_mismatch</command></entry>
                                    <entry>finds the first position where two ranges differ</entry>
                                    <entry>parallel_mismatch.hpp</entry>
                                    <entry>Iterators</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_find_all">parallel_find_all</command></entry>
                                    <entry>finds all the elements satisfying specific criteria </entry>
                                    <entry>parallel_find_all.hpp</entry>
                                    <entry>Iterators, moved range, continuation</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_find_end">parallel_find_end</command></entry>
                                    <entry>finds the last sequence of elements in a certain
                                        range</entry>
                                    <entry>parallel_find_end.hpp</entry>
                                    <entry>Iterators, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_find_first_of">parallel_find_first_of</command></entry>
                                    <entry>searches for any one of a set of elements</entry>
                                    <entry>parallel_find_first_of.hpp</entry>
                                    <entry>Iterators, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_adjacent_find">parallel_adjacent_find</command></entry>
                                    <entry>finds the first two adjacent items that are equal (or
                                        satisfy a given predicate)</entry>
                                    <entry>parallel_adjacent_find.hpp</entry>
                                    <entry>Iterators</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_lexicographical_compare">parallel_lexicographical_compare</command></entry>
                                    <entry>returns true if one range is lexicographically less than
                                        another</entry>
                                    <entry>parallel_lexicographical_compare.hpp</entry>
                                    <entry>Iterators</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_search">parallel_search</command></entry>
                                    <entry>searches for a range of elements</entry>
                                    <entry>parallel_search.hpp</entry>
                                    <entry>Iterators, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_search_n">parallel_search_n</command></entry>
                                    <entry>searches for a number consecutive copies of an element in
                                        a range</entry>
                                    <entry>parallel_search_n.hpp</entry>
                                    <entry>Iterators, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_scan">parallel_scan</command></entry>
                                    <entry>does a custom scan over a range of elements</entry>
                                    <entry>parallel_scan.hpp</entry>
                                    <entry>Iterators, moved range, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_inclusive_scan">parallel_inclusive_scan</command></entry>
                                    <entry>does an inclusive scan over a range of elements</entry>
                                    <entry>parallel_inclusive_scan.hpp</entry>
                                    <entry>Iterators, moved range, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_exclusive_scan">parallel_exclusive_scan</command></entry>
                                    <entry>does an exclusive scan over a range of elements</entry>
                                    <entry>parallel_exclusive_scan.hpp</entry>
                                    <entry>Iterators, moved range, continuation</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry><command xlink:href="#parallel_kmp">parallel_kmp</command></entr>
                                    <entry>searches a range of elements for a subsequence using the Knuth-Morris-Pratt algorithm</entry>
                                    <entry>parallel_kmp.hpp</entry>
                                    <entry>Iterators, moved ranges, continuations</entry>
                                    <entry>No</entry>
                            </tbody>
                        </tgroup>
                    </table></para><para></para>
                <table frame="all">
                    <title>Modifying Algorithms, in boost/asynchronous/algorithm</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_copy">parallel_copy</command></entry>
                            <entry>copies a range of elements to a new location</entry>
                            <entry>parallel_copy.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_copy_if">parallel_copy_if</command></entry>
                            <entry>copies a the elements to a new location for which the given
                                predicate is true. </entry>
                            <entry>parallel_copy_if.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_move">parallel_move</command></entry>
                            <entry>moves a range of elements to a new location</entry>
                            <entry>parallel_move.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_fill">parallel_fill</command></entry>
                            <entry>assigns a range of elements a certain value </entry>
                            <entry>parallel_fill.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_transform">parallel_transform</command></entry>
                            <entry>applies a function to a range of elements</entry>
                            <entry>parallel_transform.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_generate">parallel_generate</command></entry>
                            <entry>saves the result of a function in a range </entry>
                            <entry>parallel_generate.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_remove_copy">parallel_remove_copy</command></entry>
                            <entry>copies a range of elements that are not equal to a specific
                                value</entry>
                            <entry>parallel_remove_copy.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_remove_copy">parallel_remove_copy_if</command></entry>
                            <entry>copies a range of elements omitting those that satisfy specific
                                criteria</entry>
                            <entry>parallel_remove_copy.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_replace">parallel_replace</command></entry>
                            <entry>replaces all values with a specific value with another value </entry>
                            <entry>parallel_replace.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_replace">parallel_replace_if</command></entry>
                            <entry>replaces all values satisfying specific criteria with another value </entry>
                            <entry>parallel_replace.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_reverse">parallel_reverse</command></entry>
                            <entry>reverses the order of elements in a range </entry>
                            <entry>parallel_reverse.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>                        
                        <row>
                            <entry><command xlink:href="#parallel_swap_ranges">parallel_swap_ranges</command></entry>
                            <entry>swaps two ranges of elements</entry>
                            <entry>parallel_swap_ranges.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_transform_inclusive_scan">parallel_transform_inclusive_scan</command></entry>
                            <entry>does an inclusive scan over a range of elements after applying a
                                function to each element </entry>
                            <entry>parallel_transform_inclusive_scan.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_transform_exclusive_scan">parallel_transform_exclusive_scan</command></entry>
                            <entry>does an exclusive scan over a range of elements after applying a
                                function to each element</entry>
                            <entry>parallel_transform_exclusive_scan.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                    </tbody>
                   </tgroup>
                </table><para></para>
                <table frame="all">
                    <title>Partitioning Operations, in boost/asynchronous/algorithm</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_is_partitioned">parallel_is_partitioned</command></entry>
                            <entry>determines if the range is partitioned by the given predicate</entry>
                            <entry>parallel_is_partitioned.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_partition">parallel_partition</command></entry>
                            <entry>divides a range of elements into two groups</entry>
                            <entry>parallel_partition.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_stable_partition">parallel_stable_partition</command></entry>
                            <entry>divides elements into two groups while preserving their relative
                                order</entry>
                            <entry>parallel_stable_partition.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_partition_copy">parallel_partition_copy</command></entry>
                            <entry>copies a range dividing the elements into two groups</entry>
                            <entry>parallel_partition_copy.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                    </tbody>
                  </tgroup>
                </table>
                <para></para>
                <table frame="all">
                    <title>Sorting Operations, in boost/asynchronous/algorithm</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_is_sorted">parallel_is_sorted</command></entry>
                            <entry>checks whether a range is sorted according to the given predicate </entry>
                            <entry>parallel_is_sorted.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_is_reverse_sorted">parallel_is_reverse_sorted</command></entry>
                            <entry>checks whether a range is reverse sorted according to the given
                                predicate</entry>
                            <entry>parallel_is_sorted.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_sort">parallel_sort</command></entry>
                            <entry>sorts a range according to the given predicate </entry>
                            <entry>parallel_sort.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>Yes</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_sort">parallel_sort_inplace</command></entry>
                            <entry>sorts a range according to the given predicate using inplace
                                merge</entry>
                            <entry>parallel_sort_inplace.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_sort">parallel_spreadsort_inplace</command></entry>
                            <entry>sorts a range according to the given predicate using a
                                Boost.Spreadsort algorithm and inplace merge</entry>
                            <entry>parallel_sort_inplace.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_sort">parallel_stable_sort_inplace</command></entry>
                            <entry>sorts a range of elements while preserving order between equal
                                elements using inplace merge</entry>
                            <entry>parallel_sort_inplace.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_sort">parallel_spreadsort</command></entry>
                            <entry>sorts a range according to the given predicate using a
                                Boost.Spreadsort algorithm</entry>
                            <entry>parallel_sort.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_sort">parallel_stable_sort</command></entry>
                            <entry>sorts a range of elements while preserving order between equal
                                elements</entry>
                            <entry>parallel_stable_sort.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_partial_sort">parallel_partial_sort</command></entry>
                            <entry>sorts the first N elements of a range </entry>
                            <entry>parallel_partial_sort.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_quicksort">parallel_quicksort</command></entry>
                            <entry>sorts a range according to the given predicate using a
                                quicksort</entry>
                            <entry>parallel_quicksort.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_quicksort">parallel_quick_spreadsort</command></entry>
                            <entry>sorts a range according to the given predicate using a quicksort and
                                a Boost.Spreadsort algorithm</entry>
                            <entry>parallel_quicksort.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_nth_element">parallel_nth_element</command></entry>
                            <entry>partially sorts the given range making sure that it is partitioned
                                by the given element </entry>
                            <entry>parallel_nth_element.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                    </tbody>
                  </tgroup>
                </table>
                <para></para>
                <table frame="all">
                    <title>Numeric Algorithms in boost/asynchronous/algorithm</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_iota">parallel_iota</command></entry>
                            <entry>fills a range with successive increments of the starting value</entry>
                            <entry>parallel_iota.hpp</entry>
                            <entry>Iterators, moved range</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_reduce">parallel_reduce</command></entry>
                            <entry>sums up a range of elements </entry>
                            <entry>parallel_reduce.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>Yes</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_inner_product">parallel_inner_product</command></entry>
                            <entry>computes the inner product of two ranges of elements </entry>
                            <entry>parallel_inner_product.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>Yes</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_partial_sum">parallel_partial_sum</command></entry>
                            <entry>computes the partial sum of a range of elements </entry>
                            <entry>parallel_partial_sum.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                    </tbody>
                  </tgroup>
                </table>
                <para></para>
                <table frame="all">
                    <title>Algorithms Operating on Sorted Sequences in
                        boost/asynchronous/algorithm</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_merge">parallel_merge</command></entry>
                            <entry>merges two sorted ranges </entry>
                            <entry>parallel_merge.hpp</entry>
                            <entry>Iterators</entry>
                            <entry>No</entry>
                        </row>
                    </tbody>
                  </tgroup>
                </table>
                <para></para>
                <table frame="all">
                    <title>Minimum/maximum operations in boost/asynchronous/algorithm</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_extremum">parallel_extremum</command></entry>
                            <entry>returns an extremum(smaller/ greater) of the given values according
                                to a given predicate </entry>
                            <entry>parallel_extremum.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>Yes</entry>
                        </row>
                    </tbody>
                  </tgroup>
                </table>
                <para></para>
                <table frame="all">
                    <title>Miscellaneous Algorithms in boost/asynchronous/algorithm</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_invoke">parallel_invoke</command></entry>
                            <entry>invokes a variable number of operations in parallel</entry>
                            <entry>parallel_invoke.hpp</entry>
                            <entry>variadic sequence of functions</entry>
                            <entry>Yes</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#if_then_else">if_then_else</command></entry>
                            <entry>invokes algorithms based on the result of a function</entry>
                            <entry>if_then_else.hpp</entry>
                            <entry>if/then/else clauses</entry>
                            <entry>No</entry>
                        </row>
                    </tbody>   
                  </tgroup>                        
                </table>
                <para></para>
                <table frame="all">
                    <title>(Boost) Geometry Algorithms in boost/asynchronous/algorithm/geometry
                        (compatible with boost geometry 1.58). Experimental and tested only with
                        polygons.</title>
                    <tgroup cols="5">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="c4" colnum="4" colwidth="1.0*"/>
                        <colspec colname="c5" colnum="5" colwidth="1.0*"/>
                        <thead>
                            <row>
                                <entry>Name</entry>
                                <entry>Description</entry>
                                <entry>Header</entry>
                                <entry>Parameters taken</entry>
                                <entry>Distributable</entry>
                            </row>
                        </thead>
                    <tbody>
                        <row>
                            <entry><command xlink:href="#parallel_geometry_intersection_of_x">parallel_geometry_intersection_of_x</command></entry>
                            <entry>calculates the intersection of many geometries </entry>
                            <entry>parallel_geometry_intersection_of_x.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_geometry_union_of_x">parallel_geometry_union_of_x</command></entry>
                            <entry>combines many geometries which each other </entry>
                            <entry>parallel_geometry_union_of_x.hpp</entry>
                            <entry>Iterators, moved range, continuation</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_intersection">parallel_intersection</command></entry>
                            <entry>calculates the intersection of two geometries</entry>
                            <entry>parallel_intersection.hpp</entry>
                            <entry>two geometries</entry>
                            <entry>No</entry>
                        </row>
                        <row>
                            <entry><command xlink:href="#parallel_union">parallel_union</command></entry>
                            <entry>combines two geometries which each other</entry>
                            <entry>parallel_union.hpp</entry>
                            <entry>two geometries</entry>
                            <entry>No</entry>
                        </row>
                    </tbody>
                  </tgroup>
                </table>
                <para></para>
                <sect2>
                    <title>Finding the best cutoff</title>
                    <para>The algorithms described above all make use of a cutoff, which is the
                        number of elements where the algorithm should stop going parallel and
                        execute sequentially. It is sometimes also named grain size in the
                        literrature. Finding the best value can often make quite a big difference in
                        execution time. Unfortunately, the best cutoff differs greatly between
                        different processors or even machines. To make this task easier,
                        Asynchronous provides a helper function, <emphasis role="bold"
                            >find_best_cutoff</emphasis>, which helps finding the best cutoff. For
                        best results, it makes sense to use it at deployment time. <emphasis
                            role="bold">find_best_cutoff</emphasis> can be found in
                        boost/asynchronous/helpers.hpp.</para>
                    <programlisting>template &lt;class Func, class Scheduler>
std::tuple&lt;std::size_t,std::vector&lt;std::size_t>> find_best_cutoff(Scheduler s, Func f,
                                                     std::size_t cutoff_begin,
                                                     std::size_t cutoff_end,
                                                     std::size_t steps,
                                                     std::size_t retries,
                                                     const std::string&amp; task_name="",
                                                     std::size_t prio=0 )</programlisting>
                    <para><emphasis role="underline">Return value</emphasis>: A tuple containing the
                        best cutoff and a std::vector containing the elapsed times of this best
                        cutoff.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>Scheduler s: the scheduler which will execute the algorithm we
                                    want to optimize</para>
                            </listitem>
                            <listitem>
                                <para>Func f: a unary predicate which will be called for every
                                    cutoff value. It must have a signature of the form
                                    Unspecified-Continuation f (std::size_t cutoff); which will
                                    return a continuation, which is what algorithms described in the
                                    next sections will return.</para>
                            </listitem>
                            <listitem>
                                <para>cutoff_begin, cutoff_end: the range of cutoffs to test</para>
                            </listitem>
                            <listitem>
                                <para>steps: step between two possible cutoff values. This is needed
                                    because testing every possible cutoff would take very
                                    long.</para>
                            </listitem>
                            <listitem>
                                <para>retries: how many times the same cutoff value will be used.
                                    Using retries will give us a better mean vakue for a given
                                    cutoff.</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                    </itemizedlist></para>
                    <para>Please have a look at <link xlink:href="examples/example_cutoff_sort.cpp"
                        >this example finding the best cutoff for a parallel_sort</link>.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_for"/>parallel_for</title>
                    <para>Applies a functor to every element of the range [beg,end) .</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
parallel_for(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The parallel_for version taking iterators requires that the iterators stay
                        valid until completion. It is the programmer's job to ensure this.</para>
                    <para>The third argument is the predicate applied on each element of the
                        algorithm.</para>
                    <para>The fourth argument is the cutoff, meaning in this case the max. number of
                        elements of the input range in a task.</para>
                    <para>The optional fifth argument is the name of the tasks used for
                        logging.</para>
                    <para>The optional sixth argument is the priority of the tasks in the
                        pool.</para>
                    <para>The return value is a void continuation containing either nothing or an
                        exception if one was thrown from one of the tasks.</para>
                    <para>Example:</para>
                    <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    void start_async_work()
    {
        // start long tasks in threadpool (first lambda) and callback in our thread
        post_callback(
               [this](){
                        <emphasis role="bold">return</emphasis> boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">this->m_data.begin(),this->m_data.end()</emphasis>,
                                                                 [](int&amp; i)
                                                                 {
                                                                    i += 2;
                                                                 },1500);
                      },// work
               // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
               [](boost::asynchronous::expected&lt;<emphasis role="bold">void</emphasis>> /*res*/){
                            ...
               }// callback functor.
        );
    }
    std::vector&lt;int> m_data;
};

// same using post_future
std::future&lt;void> fu = post_future(
                           [this](){
                            <emphasis role="bold">return</emphasis> boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">this->m_data.begin(),this->m_data.end()</emphasis>,
                                                                 [](int&amp; i)
                                                                 {
                                                                    i += 2;
                                                                 },1500);});</programlisting>
                    <para>The most important parts are highlighted. Do not forget the return statement
                        as we are returning a continuation and we do not want the lambda to be
                        interpreted as a void lambda. The caller has responsibility of the input
                        data, given in the form of iterators. </para>
                    <para>The code will do following:<itemizedlist>
                        <listitem>
                            <para>start tasks in the current worker pool of max 1500 elements of
                                the input data</para>
                        </listitem>
                        <listitem>
                            <para>add 2 to each element in parallel</para>
                        </listitem>
                        <listitem>
                            <para>The parallel_for will return a continuation</para>
                        </listitem>
                        <listitem>
                            <para>The callback lambda will be called when all tasks complete.
                                The expected will be either set or contain an exception</para>
                        </listitem>
                        <listitem>
                            <para>If post_future is used, a future&lt;void> will be
                                returned.</para>
                        </listitem>
                    </itemizedlist></para>
                    <para>Please have a look at <link xlink:href="examples/example_parallel_for.cpp"
                        >the complete example</link>.</para>
                    <para>The functor can either be called for every single element, or for a range
                        of elements:
                        <programlisting>std::future&lt;void> fu = post_future(
                           [this](){
                            <emphasis role="bold">return</emphasis> boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">this->m_data.begin(),this->m_data.end()</emphasis>,
                                                                 [](std::vector&lt;int>::iterator beg, std::vector&lt;int>::iterator end)
                                                                 {
                                                                    for(;beg != end; ++beg)
                                                                    {
                                                                        *beg += 2;
                                                                    }
                                                                 },1500);});</programlisting></para>
                    <para>The second version takes a range per rvalue reference. This is signal
                        given to Asynchronous that it must take ownership of the range. The return
                        value is then a continuation of the given range type:</para>
                    <programlisting>template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
parallel_for(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>A <code>post_callback / post_future</code> will therefore get a
                        expected&lt;new range>, for example:</para>
                    <programlisting>post_callback(
    []()
    {
       std::vector&lt;int> data;
       return boost::asynchronous::parallel_for(std::move(data),
                                                      [](int&amp; i)
                                                      {
                                                        i += 2;
                                                      },1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;std::vector&lt;int>></emphasis> ){}
);</programlisting>
                    <para>In this case, the programmer does not need to ensure the container stays
                        valid, Asynchronous takes care of it.</para>
                    <para>The third version of this algorithm takes a range continuation instead of
                        a range as argument and will be invoked after the continuation is
                        ready.</para>
                    <programlisting>// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">typename Range::return_type</emphasis>,Job>
parallel_for(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>This version allows chaining parallel calls. For example, it is now possible
                        to write:</para>
                    <programlisting>post_callback(
    []()
    {
       std::vector&lt;int> data;
       return <emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(
                                                            // executed first
                                                            std::move(data),
                                                            [](int&amp; i)
                                                            {
                                                               i += 2;
                                                            },1500),
                                              // executed second
                                              [](int&amp; i)
                                              {
                                                  i += 2;
                                              },1500),
                                 // executed third
                                 [](int&amp; i)
                                 {
                                      i += 2;
                                 },1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;std::vector&lt;int>></emphasis> ){} // callback
);</programlisting>
                    <para>This code will be executed as follows:<itemizedlist>
                        <listitem>
                            <para>the most inner parallel_for (parallel execution)</para>
                        </listitem>
                        <listitem>
                            <para>A kind of synchronization point will be done at this point
                                until the parallel_for completes</para>
                        </listitem>
                        <listitem>
                            <para>the middle parallel_for will be executed (parallel
                                execution)</para>
                        </listitem>
                        <listitem>
                            <para>A kind of synchronization point will be done at this point
                                until the parallel_for completes</para>
                        </listitem>
                        <listitem>
                            <para>the outer parallel_for will be executed (parallel
                                execution)</para>
                        </listitem>
                        <listitem>
                            <para>A kind of synchronization point will be done at this point
                                until the parallel_for completes</para>
                        </listitem>
                        <listitem>
                            <para>The callback will be called</para>
                        </listitem>
                    </itemizedlist></para>
                    <para>With "kind of synchronization point", we mean there will be no blocking
                        synchronization, it will just be waited until completion.</para>
                    <para>Finally, this algorithm has a distributed version. We need, as with our
                        Fibonacci example, a serializable sub-task which will be created as often as
                        required by our cutoff and which will handle a part of our range:</para>
                    <programlisting>struct dummy_parallel_for_subtask : public boost::asynchronous::serializable_task
{
    dummy_parallel_for_subtask(int d=0):boost::asynchronous::serializable_task(<emphasis role="bold">"dummy_parallel_for_subtask"</emphasis>),m_data(d){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    void operator()(int&amp; i)const
    {
        i += m_data;
    }
    // some data, so we have something to serialize
    int m_data;
};</programlisting>
                    <para>We also need a serializable top-level task, creating sub-tasks:</para>
                    <programlisting>struct dummy_parallel_for_task : public boost::asynchronous::serializable_task
{
    dummy_parallel_for_task():boost::asynchronous::serializable_task(<emphasis role="bold">"dummy_parallel_for_task"</emphasis>),m_data(1000000,1){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    auto operator()() -> decltype(boost::asynchronous::parallel_for&lt;std::vector&lt;int>,dummy_parallel_for_subtask,boost::asynchronous::any_serializable>(
                                      std::move(std::vector&lt;int>()),
                                      dummy_parallel_for_subtask(2),
                                      10))
    {
        <emphasis role="bold">return boost::asynchronous::parallel_for</emphasis>
                &lt;std::vector&lt;int>,<emphasis role="bold">dummy_parallel_for_subtask</emphasis>,boost::asynchronous::any_serializable>(
            std::move(m_data),
            dummy_parallel_for_subtask(2),
            10);
    }
    std::vector&lt;int> m_data;
};</programlisting>
                    <para>We now post our top-level task inside a servant or use post_future:</para>
                    <programlisting>post_callback(
               dummy_parallel_for_task(),
               // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
               [this](boost::asynchronous::expected&lt;std::vector&lt;int>> res){
                 try
                 {
                    // do something
                 }
                 catch(std::exception&amp; e)
                 {
                    std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                 }
              }// end of callback functor.
);</programlisting>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_for_tcp.cpp">complete server example</link>.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_for_each"/>parallel_for_each</title>
                    <para>Applies a functor to every element of the range [beg,end). This functor
                        can save data. It is merged at different steps with other instances of this
                        functor. The algorithm returns the last merged instance.</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Func</emphasis>,Job>
<emphasis role="bold">parallel_all_of</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: A merged instance of a
                        functor of type Func.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the range of elements to search</para>
                        </listitem>
                        <listitem>
                            <para>func: a class / struct object with a:</para>
                                <para>
                                    <itemizedlist>
                                        <listitem>
                                            <para>void operator()(const Type&amp; a)</para>
                                        </listitem>
                                        <listitem>
                                            <para>void merge (Func const&amp; f)</para>
                                        </listitem>
                                    </itemizedlist>
                                </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_all_of"/>parallel_all_of</title>
                    <para>Checks if unary predicate p returns true for all elements in the range
                        [begin, end). </para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_all_of</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for ranges returned as continuations
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_all_of</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if unary
                        predicate returns true for all elements in the range, false otherwise.
                        Returns true if the range is empty.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements to search</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_any_of"/>parallel_any_of</title>
                    <para>Checks if unary predicate p returns true for at least one element in the
                        range [begin, end).</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_any_of</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for ranges returned as continuations
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_any_of</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if unary
                        predicate returns true for at least one element in the range, false
                        otherwise. Returns false if the range is empty.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements to search</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_none_of"/>parallel_none_of</title>
                    <para>Checks if unary predicate p returns true for no elements in the range
                        [begin, end).</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_none_of</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for ranges returned as continuations
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_none_of</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if unary
                        predicate returns true for no elements in the range, false otherwise.
                        Returns true if the range is empty.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements to search</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_equal"/>parallel_equal</title>
                    <para>Checks if two ranges are equal.</para>
                    <programlisting>template &lt;class Iterator1,class Iterator2, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_equal</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Iterator2, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_equal</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>:  If the length of the
                        range [first1, end1) does not equal the length of the range beginning at
                        begin2, returns false If the elements in the two ranges are equal, returns
                        true. Otherwise returns false. </para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1: the first range</para>
                            </listitem>
                            <listitem>
                                <para>begin2: the beginning of the second range</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_mismatch"/>parallel_mismatch</title>
                    <para>Returns the first mismatching pair of elements from two ranges: one
                        defined by [begin1, end1) and another starting at begin2.</para>
                    <programlisting>template &lt;class Iterator1,class Iterator2, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Iterator1,Iterator2></emphasis>,Job>
<emphasis role="bold">parallel_mismatch</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Iterator2, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Iterator1,Iterator2></emphasis>,Job>
<emphasis role="bold">parallel_mismatch</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: std::pair with
                        iterators to the first two non-equivalent elements. If no mismatches are
                        found when the comparison reaches end1 or end2, whichever happens first, the
                        pair holds the end iterator and the corresponding iterator from the other
                        range. </para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1: the first range</para>
                            </listitem>
                            <listitem>
                                <para>begin2: the beginning of the second range</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_find_end"/>parallel_find_end</title>
                    <para>Searches for the last subsequence of elements [begin2, end2) in the range
                        [begin1, end1). [begin2, end2) can be replaced (3rd form) by a
                        continuation.</para>
                    <programlisting>template &lt;class Iterator1,class Iterator2, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_find_end</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Iterator2, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_find_end</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Range,class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_find_end</emphasis>(Iterator1 begin1, Iterator1 end1,Range range, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: Iterator to the
                        beginning of last subsequence [begin2, end2) in range [begin1, end1). If
                        [begin2, end2) is empty or if no such subsequence is found, end1 is
                        returned.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1: the first range</para>
                            </listitem>
                            <listitem>
                                <para>begin2, end2 / Range: the subsequence to look for.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_find_first_of"/>parallel_find_first_of</title>
                    <para>Searches the range [begin1, end1) for any of the elements in the range
                        [begin2, end2). [begin2, end2) can be replaced (3rd form) by a
                        continuation.</para>
                    <programlisting>template &lt;class Iterator1,class Iterator2, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_find_first_of</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Iterator2, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_find_first_of</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Range,class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_find_first_of</emphasis>(Iterator1 begin1, Iterator1 end1,Range range, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: Iterator to the first
                        element in the range [begin1, end1) that is equal to an element from the
                        range [begin2; end2). If no such element is found, end1 is returned.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1: the first range</para>
                            </listitem>
                            <listitem>
                                <para>begin2, end2 / Range: the subsequence to look for.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>  
                <sect2>
                    <title><command xml:id="parallel_adjacent_find"/>parallel_adjacent_find</title>
                    <para>Searches the range  [begin, end) for two consecutive identical
                        elements.</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator</emphasis>,Job>
<emphasis role="bold">parallel_adjacent_find</emphasis>(Iterator begin, Iterator end,Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator</emphasis>,Job>
<emphasis role="bold">parallel_adjacent_find</emphasis>(Iterator begin, Iterator end, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: an iterator to the
                        first of the first pair of identical elements, that is, the first iterator
                        it such that *it == *(it+1) for the second version or func(*it, *(it + 1))
                        != false for the first version. If no such elements are found, last is
                        returned.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements to examine</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>  
                <sect2>
                    <title><command xml:id="parallel_lexicographical_compare"/>parallel_lexicographical_compare</title>
                    <para>Checks if the first range [begin1, end1) is lexicographically less than
                        the second range [begin2, end2).</para>
                    <programlisting>template &lt;class Iterator1,class Iterator2, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_lexicographical_compare</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Iterator2, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_lexicographical_compare</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if the first
                        range is lexicographically less than the second.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1: the first range of elements to examine </para>
                            </listitem>
                            <listitem>
                                <para>begin2, end2: the second range of elements to examine.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>                  
                <sect2>
                    <title><command xml:id="parallel_search"/>parallel_search</title>
                    <para>Searches for the first occurrence of the subsequence of elements [begin2,
                        end2)in the range [begin1, end1 - (end2 - end1)).</para>
                    <programlisting>template &lt;class Iterator1,class Iterator2, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_search</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Iterator2, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_search</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2,Iterator2 end2, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1,class Range,class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_search</emphasis>(Iterator1 begin1, Iterator1 end1,Range range, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: Iterator to the
                        beginning of first subsequence [begin2, end2) in the range [begin1, end1 -
                        (end2 - begin2)). If no such subsequence is found, end1 is returned. If
                        [begin2, end2) is empty, begin1 is returned. </para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1: the first range</para>
                            </listitem>
                            <listitem>
                                <para>begin2, end2 / Range: the subsequence to look for.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_search_n"/>parallel_search_n</title>
                    <para>Searches the range [begin, end) for the first sequence of count identical
                        elements, each equal to the given value value.</para>
                    <programlisting>template &lt;class Iterator1, class Size, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_search_n</emphasis>(Iterator1 begin1, Iterator1 end1, Size count, const T&amp; value, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator1, class Size, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator1</emphasis>,Job>
<emphasis role="bold">parallel_search_n</emphasis>(Iterator1 begin1, Iterator1 end1, Size count, const T&amp; value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: Iterator to the
                        beginning of the found sequence in the range [begin1, end1). If no such
                        sequence is found, end1 is returned. </para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1: the first range</para>
                            </listitem>
                            <listitem>
                                <para>count: the length of the sequence to search for</para>
                            </listitem>
                            <listitem>
                                <para>value: the value of the elements to search for </para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate which returns â€‹true if the elements
                                    should be treated as equal. The signature of the function should
                                    be equivalent to the following: bool pred(const Type
                                    &amp;a,const Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_scan"/>parallel_scan</title>
                    <para>Computes all partial reductions of a collection. For every output
                        position, a reduction of the input up to that point is computed. This is the
                        basic, most flexible underlying implementation of parallel_exclusive_scan,
                        parallel_inclusive_scan, parallel_transform_exclusive_scan,
                        parallel_transform_inclusive_scan.</para>
                    <para>The operator, represented by the Combine function, must be
                        associative.</para>
                    <para>The algorithm works by doing two passes on the sequence: the first pass
                        uses the Reduce function, the second pass uses the result of Reduce in
                        Combine. Scan will output the result.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class OutIterator, class T, class Reduce, class Combine, class Scan, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;OutIterator,Job>
<emphasis role="bold">parallel_scan</emphasis>(Iterator beg, Iterator end, OutIterator out, T init, Reduce r, Combine c, Scan s, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved ranges
template &lt;class Range, class OutRange, class T, class Reduce, class Combine, class Scan, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Range,OutRange></emphasis>,Job>
<emphasis role="bold">parallel_scan</emphasis>(Range&amp;&amp; range,OutRange&amp;&amp; out_range,T init,Reduce r, Combine c, Scan s, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for a single moved range (in/out)
template &lt;class Range, class T, class Reduce, class Combine, class Scan, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_scan</emphasis>(Range&amp;&amp; range,T init,Reduce r, Combine c, Scan s, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range, class T, class Reduce, class Combine, class Scan, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_scan</emphasis>(Range range,T init,Reduce r, Combine c, Scan s, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                                <para>begin, end / range: the range of elements to scan.</para>
                            </listitem>
                            <listitem>
                                <para>out: the beginning of the output range.</para>
                            </listitem>
                            <listitem>
                                <para>out_range: the output range.</para>
                            </listitem>
                            <listitem>
                                <para>init: initial value, combined with the first scanned
                                    element.</para>
                            </listitem>
                        <listitem>
                                <para>Reduce: binary predicate which returns an accumulated value.
                                    The signature of the function should be equivalent to the
                                    following: Ret reduce(Iterator,Iterator); The type Ret must be
                                    such that an object of type Iterator can be dereferenced and
                                    assigned a value of type Ret.</para>
                            </listitem>
                            <listitem>
                                <para>Combine: binary predicate which combines two elements, like
                                    std::plus would do. The signature of the function should be
                                    equivalent to the following: Ret combine(const Type&amp;,const
                                    Type&amp;); The type Ret must be such that an object of type
                                    Iterator can be dereferenced and assigned a value of type
                                    Ret.</para>
                            </listitem>
                            <listitem>
                                <para>Scan: assigns the result to the out range / iterator. The
                                    signature of the function should be equivalent to the following:
                                    void scan(Iterator beg, Iterator end, OutIterator out, T
                                    init).</para>
                            </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>Last version returns a Range of the type returned by
                                    continuation</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_inclusive_scan"/>parallel_inclusive_scan</title>
                    <para>Computes all partial reductions of a collection. For every output
                        position, a reduction of the input up to that point is computed. The nth
                        output value is a reduction over the first n input values.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class OutIterator, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;OutIterator,Job>
<emphasis role="bold">parallel_inclusive_scan</emphasis>(Iterator beg, Iterator end, OutIterator out, T init, Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved ranges
template &lt;class Range, class OutRange, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Range,OutRange></emphasis>,Job>
<emphasis role="bold">parallel_inclusive_scan</emphasis>(Range&amp;&amp; range,OutRange&amp;&amp; out_range, T init, Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for a single moved range (in/out)
template &lt;class Range, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_inclusive_scan</emphasis>(Range&amp;&amp; range,T init,Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_inclusive_scan</emphasis>(Range range,T init,Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to scan.</para>
                        </listitem>
                        <listitem>
                            <para>out: the beginning of the output range.</para>
                        </listitem>
                        <listitem>
                            <para>out_range: the output range.</para>
                        </listitem>
                        <listitem>
                            <para>init: initial value, combined with the first scanned
                                element.</para>
                        </listitem>
                            <listitem>
                                <para>Func: binary operation function object that will be applied.
                                    The signature of the function should be equivalent to the
                                    following: Ret fun(const Type &amp;a, const Type &amp;b); The
                                    type Ret must be such that an object of type  OutIterator can be
                                    dereferenced and assigned a value of type Ret</para>
                            </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>Last version returns a Range of the type returned by
                                continuation</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>               
                <sect2>
                    <title><command xml:id="parallel_exclusive_scan"/>parallel_exclusive_scan</title>
                    <para>Computes all partial reductions of a collection. For every output
                        position, a reduction of the input up to that point is computed. The nth
                        output value is a reduction over the first n- input values.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class OutIterator, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;OutIterator,Job>
<emphasis role="bold">parallel_exclusive_scan</emphasis>(Iterator beg, Iterator end, OutIterator out, T init, Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved ranges
template &lt;class Range, class OutRange, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Range,OutRange></emphasis>,Job>
<emphasis role="bold">parallel_exclusive_scan</emphasis>(Range&amp;&amp; range,OutRange&amp;&amp; out_range, T init, Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for a single moved range (in/out)
template &lt;class Range, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_exclusive_scan</emphasis>(Range&amp;&amp; range,T init,Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_exclusive_scan</emphasis>(Range range,T init,Func f, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to scan.</para>
                        </listitem>
                        <listitem>
                            <para>out: the beginning of the output range.</para>
                        </listitem>
                        <listitem>
                            <para>out_range: the output range.</para>
                        </listitem>
                        <listitem>
                            <para>init: initial value, combined with the first scanned
                                element.</para>
                        </listitem>
                        <listitem>
                            <para>Func: binary operation function object that will be applied.
                                The signature of the function should be equivalent to the
                                following: Ret fun(const Type &amp;a, const Type &amp;b); The
                                type Ret must be such that an object of type  OutIterator can be
                                dereferenced and assigned a value of type Ret</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>Last version returns a Range of the type returned by
                                continuation</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_kmp"/>parallel_kmp</title>
                    <para>Searches a range of elements (such as a string) for a subsequence. On each match, a functor is called with the index of the matched subsequence. This functor can save data and is merged with other instances of this functor (like parallel_for_each). This allows the caller to either obtain a list of all matches (by keeping track of the matched indices when merging) or just performing an operation directly when the match is found. The algorithm returns the last merged instance of the functor type.</para>
                    <programlisting>template &lt;/* see below */, class Functor, class Job=/* see below */&gt;
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Functor</emphasis>,Job>
<emphasis role="bold">parallel_kmp</emphasis>(/* see below */, Functor functor, long cutoff, const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>Both the range of elements that will be searched ("haystack") and the subsequence that is searched for ("needle") can be passed to parallel_kmp as either a pair of iterators (begin and end), as a range (taken by reference or moved into parallel_kmp), or as a continuation that returns the range in question</para>
                    <para><emphasis role="underline">Return value</emphasis>: A merged instance of a
                        functor of type Func.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>The "haystack": the range of elements to search in (as iterators, as a range, or as a continuation)</para>
                        </listitem>
                        <listitem>
                            <para>The "needle": the range of elements to search for (as iterators, as a range, or as a continuation)</para>
                        </listitem>
                        <listitem>
                            <para>functor: an object with:</para>
                                <para>
                                    <itemizedlist>
                                        <listitem>
                                            <para>void operator()(const index_type&amp;), where index_type is the type used to index into the haystack range</para>
                                        </listitem>
                                        <listitem>
                                            <para>void merge(const Functor&amp;) or void merge(Functor&amp;&amp;) (the latter is preferred if it is available)</para>
                                        </listitem>
                                    </itemizedlist>
                                </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                    <para>When passing iterators or a reference to a range to this algorithm, the programmer must ensure that the reference or iterators stay valid until the algorithm completes.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_copy"/>parallel_copy</title>
                    <para>Copies the elements in the range, defined by [begin, end), to another
                        range beginning at result. The order of the elements that are not removed is
                        preserved.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator,class ResultIterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_copy</emphasis>(Iterator begin, Iterator end,ResultIterator result, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved range
template &lt;class Range,class ResultIterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_copy</emphasis>(Range&amp;&amp; range, ResultIterator result, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range,class ResultIterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_copy</emphasis>(Range range,ResultIterator out, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to copy </para>
                        </listitem>
                        <listitem>
                                <para>result: the beginning of the destination range.</para>
                            </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_copy_if"/>parallel_copy_if</title>
                    <para>Copies copies the elements for which a predicate returns true. The order
                        of the elements that are not removed is preserved.</para>
                    <programlisting>template &lt;class Iterator,class ResultIterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">ResultIterator</emphasis>,Job>
<emphasis role="bold">parallel_copy_if</emphasis>(Iterator begin, Iterator end,ResultIterator result,Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the range of elements to copy </para>
                        </listitem>
                        <listitem>
                                <para>result: the beginning of the destination range.</para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate which returns â€‹true for the required
                                    elements. The signature of the function should be equivalent to
                                    the following: bool pred(const Type &amp;a);</para>
                            </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_move"/>parallel_move</title>
                    <para>Moves the elements in the range [begin, end), to another range beginning
                        at d_first. After this operation the elements in the moved-from range will
                        still contain valid values of the appropriate type, but not necessarily the
                        same values as before the move.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator,class ResultIterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_move</emphasis>(Iterator begin, Iterator end,ResultIterator result, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved range
template &lt;class Range,class ResultIterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_move</emphasis>(Range&amp;&amp; range, ResultIterator result, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range,class ResultIterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_move</emphasis>(Range range,ResultIterator out, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to move </para>
                        </listitem>
                        <listitem>
                            <para>result: the beginning of the destination range.</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                            <listitem>
                                <para>3rd version returns a Range of the type returned by
                                    continuation</para>
                            </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_fill"/>parallel_fill</title>
                    <para>Assigns the given value to the elements in the range [begin, end). </para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class Value, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_fill</emphasis>(Iterator begin, Iterator end,const Value&amp; value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved range
template &lt;class Range, class Value, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_fill</emphasis>(Range&amp;&amp; range, const Value&amp; value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range, class Value, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_fill</emphasis>(Range range,const Value&amp; value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to fill </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                            <listitem>
                                <para>3rd version returns a Range of the type returned by
                                    continuation</para>
                            </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_transform"/>parallel_transform</title>
                    <para>Applies a given function to one or a variadic number of ranges and stores
                        the result in another range. </para>
                    <programlisting>// version for iterators, one range tranformed to another
template &lt;class Iterator1, class ResultIterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">ResultIterator</emphasis>,Job>
<emphasis role="bold">parallel_transform</emphasis>(Iterator1 begin1, Iterator1 end1, ResultIterator result, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for iterators, two ranges tranformed to another
template &lt;class Iterator1, class Iterator2, class ResultIterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">ResultIterator</emphasis>,Job>
<emphasis role="bold">parallel_transform</emphasis>(Iterator1 begin1, Iterator1 end1, Iterator2 begin2, ResultIterator result, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for any number of iterators (not with ICC)
template &lt;class ResultIterator, class Func, class Job, class Iterator, class... Iterators>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">ResultIterator</emphasis>,Job>
<emphasis role="bold">parallel_transform</emphasis>(ResultIterator result, Func func, Iterator begin, Iterator end, Iterators... iterators, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: Result iterator to the
                        element past the last element transformed.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin1, end1, begin2, end2, iterators: the range of input
                                    elements </para>
                            </listitem>
                            <listitem>
                                <para>result: the beginning of the destination range.</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                            <listitem>
                                <para>3rd version takes a variadic number of input ranges</para>
                            </listitem>
                            <listitem>
                                <para>func in first version : unary predicate which returns a new
                                    value. The signature of the function should be equivalent to the
                                    following: Ret pred(const Type &amp;a); The type Ret must be
                                    such that an object of type ResultIterator can be dereferenced
                                    and assigned a value of type Ret</para>
                            </listitem>
                            <listitem>
                                <para>func in second version : binary predicate which returns a new
                                    value. The signature of the function should be equivalent to the
                                    following: Ret pred(const Type1 &amp;a,const Type2 &amp;b); The
                                    type Ret must be such that an object of type ResultIterator can
                                    be dereferenced and assigned a value of type Ret</para>
                            </listitem>
                            <listitem>
                                <para>func in third version : n-ary predicate which returns a new
                                    value. The signature of the function should be equivalent to the
                                    following: Ret pred(const Type1 &amp;a,const Type2
                                    &amp;b...,const TypeN &amp;n); The type Ret must be such that an
                                    object of type ResultIterator can be dereferenced and assigned a
                                    value of type Ret</para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_generate"/>parallel_generate</title>
                    <para>Assigns each element in range [begin, end) a value generated by the given
                        function object func.  </para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_generate</emphasis>(Iterator begin, Iterator end,Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved range
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_generate</emphasis>(Range&amp;&amp; range, Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_generate</emphasis>(Range range,Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                                <para>begin, end / range: the range of elements to assign </para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate which returns â€‹a new value. The
                                    signature of the function should be equivalent to the following:
                                    Ret pred(); The type Ret must be such that an object of type
                                    Iterator can be dereferenced and assigned a value of type
                                    Ret.</para>
                            </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>3rd version returns a Range of the type returned by
                                continuation</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_remove_copy"/>parallel_remove_copy / parallel_remove_copy_if</title>
                    <para>Copies elements from the range [begin, end), to another range beginning at
                        out, omitting the elements which satisfy specific criteria. The first
                        version ignores the elements that are equal to value, the second version
                        ignores the elements for which predicate func returns true. Source and
                        destination ranges cannot overlap.</para>
                    <programlisting>template &lt;class Iterator,class Iterator2, class Value, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator2</emphasis>,Job>
<emphasis role="bold">parallel_remove_copy</emphasis>(Iterator begin, Iterator end,Iterator2 out,Value const&amp; value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator,class Iterator2, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator2</emphasis>,Job>
<emphasis role="bold">parallel_remove_copy_if</emphasis>(Iterator begin, Iterator end,Iterator2 out,Func func, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: Iterator to the
                        element past the last element copied.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements to copy </para>
                            </listitem>
                            <listitem>
                                <para>out: the beginning of the destination range.</para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate which returns â€‹true for the required
                                    elements. The signature of the function should be equivalent to
                                    the following: bool pred(const Type &amp;a);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_replace"/>parallel_replace /
                        parallel_replace_if</title>
                    <para>Replaces all elements satisfying specific criteria with new_value in the
                        range [begin, end). The replace version replaces the elements that are equal
                        to old_value, the replace_if version replaces elements for which predicate
                        func returns true. </para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_replace</emphasis>(Iterator begin, Iterator end, const T&amp; new_value, const T&amp; new_value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
template &lt;class Iterator, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_replace_if</emphasis>(Iterator begin, Iterator end,Func func, const T&amp; new_value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved range
template &lt;class Range, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_replace</emphasis>(Range&amp;&amp; range, const T&amp; old_value, const T&amp; new_value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
template &lt;class Range, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_replace_if</emphasis>(Range&amp;&amp; range, Func func, const T&amp; new_value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_replace</emphasis>(Range range, const T&amp; old_value, const T&amp; new_value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
template &lt;class Range, class T, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_replace_if</emphasis>(Range range, Func func, const T&amp; new_value, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to modify </para>
                        </listitem>
                        <listitem>
                            <para>func: unary predicate which returns â€‹true if the element value
                                    should be replaced. The signature of the predicate function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a);</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>continuation version returns a Range of the type returned by
                                    continuation</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_reverse"/>parallel_reverse</title>
                    <para>Reverses the order of the elements in the range [begin, end).</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_reverse</emphasis>(Iterator begin, Iterator end, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for moved range
template &lt;class Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_reverse</emphasis>(Range&amp;&amp; range, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for continuation
template &lt;class Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_reverse</emphasis>(Range range, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the range of elements to reverse </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                            <listitem>
                                <para>continuation version returns a Range of the type returned by
                                    continuation</para>
                            </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_swap_ranges"/>parallel_swap_ranges</title>
                    <para>Exchanges elements between range [begin1, end1) and another range starting
                        at begin2..</para>
                    <programlisting>template &lt;class Iterator1,class Iterator2, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator2</emphasis>,Job>
<emphasis role="bold">parallel_swap_ranges</emphasis>(Iterator1 begin1, Iterator1 end1,Iterator2 begin2, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if the range
                        [begin, end) is empty or is partitioned by p. false otherwise.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the first range of elements to swap </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_transform_inclusive_scan"/>parallel_transform_inclusive_scan</title>
                    <para>Computes all partial reductions of a collection. For every output
                        position, a reduction of the input up to that point is computed. The nth
                        output value is a reduction over the first n input values. This algorithm
                        applies a transformation to the input element before scan. Fusing transform
                        and scan avoids having to process two passes on the element and saves
                        time.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class OutIterator, class T, class Func, class Transform, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;OutIterator,Job>
<emphasis role="bold">parallel_transform_inclusive_scan</emphasis>(Iterator beg, Iterator end, OutIterator out, T init, Func f, Transform t, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to scan.</para>
                        </listitem>
                        <listitem>
                            <para>out: the beginning of the output range.</para>
                        </listitem>
                        <listitem>
                            <para>out_range: the output range.</para>
                        </listitem>
                        <listitem>
                            <para>init: initial value, combined with the first scanned
                                element.</para>
                        </listitem>
                        <listitem>
                            <para>Func: binary operation function object that will be applied.
                                The signature of the function should be equivalent to the
                                following: Ret fun(const Type &amp;a, const Type &amp;b); The
                                type Ret must be such that an object of type OutIterator can be
                                dereferenced and assigned a value of type Ret</para>
                        </listitem>
                        <listitem>
                            <para>Transform: unary operation function object that will be
                                applied. The signature of the function should be equivalent to
                                the following: Ret fun(const Type &amp;a); The type Ret must be
                                such that an object of type OutIterator can be dereferenced and
                                assigned a value of type Ret</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>Last version returns a Range of the type returned by
                                continuation</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_transform_exclusive_scan"/>parallel_transform_exclusive_scan</title>
                    <para>Computes all partial reductions of a collection. For every output
                        position, a reduction of the input up to that point is computed. The nth
                        output value is a reduction over the first n-1 input values. This algorithm
                        applies a transformation to the input element before scan. Fusing transform
                        and scan avoids having to process two passes on the element and saves
                        time.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class OutIterator, class T, class Func, class Transform, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;OutIterator,Job>
<emphasis role="bold">parallel_transform_exclusive_scan</emphasis>(Iterator beg, Iterator end, OutIterator out, T init, Func f, Transform t, long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm requires that the iterators stay valid until completion. It
                        is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end / range: the range of elements to scan.</para>
                        </listitem>
                        <listitem>
                            <para>out: the beginning of the output range.</para>
                        </listitem>
                        <listitem>
                            <para>out_range: the output range.</para>
                        </listitem>
                        <listitem>
                            <para>init: initial value, combined with the first scanned
                                element.</para>
                        </listitem>
                        <listitem>
                            <para>Func: binary operation function object that will be applied.
                                The signature of the function should be equivalent to the
                                following: Ret fun(const Type &amp;a, const Type &amp;b); The
                                type Ret must be such that an object of type OutIterator can be
                                dereferenced and assigned a value of type Ret</para>
                        </listitem>
                        <listitem>
                            <para>Transform: unary operation function object that will be
                                applied. The signature of the function should be equivalent to
                                the following: Ret fun(const Type &amp;a); The type Ret must be
                                such that an object of type OutIterator can be dereferenced and
                                assigned a value of type Ret</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>Last version returns a Range of the type returned by
                                continuation</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_is_partitioned"/>parallel_is_partitioned</title>
                    <para>Returns true if all elements in the range [begin, end) that satisfy the
                        predicate func appear before all elements that don't. Also returns true if
                        [begin, end) is empty.  </para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_is_partitioned</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if unary
                        predicate returns true for all elements in the range, false otherwise.
                        Returns true if the range is empty.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the range of elements to search</para>
                            <para>Or a continuation, coming from another algorithm.</para>
                        </listitem>
                        <listitem>
                            <para>func: unary predicate function. The signature of the function
                                should be equivalent to the following: bool pred(const Type
                                &amp;a);</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_partition"/>parallel_partition</title>
                    <para>Reorders the elements in the range [begin, end) in such a way that all
                        elements for which the predicate func returns true precede the elements for
                        which predicate func returns false </para>
                    <programlisting>// version with iterators
template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator</emphasis>,Job>
<emphasis role="bold">parallel_partition</emphasis>(Iterator begin, Iterator end,Func func,const uint32_t thread_num = boost::thread::hardware_concurrency(),const std::string&amp; task_name="", std::size_t prio=0);

// version with moved range
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Range,decltype(range.begin())></emphasis>,Job>
<emphasis role="bold">parallel_partition</emphasis>(Range&amp;&amp; range,Func func,const uint32_t thread_num = boost::thread::hardware_concurrency(),const std::string&amp; task_name="", std::size_t prio=0);

// version with continuation
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Unspecified-Range,Unspecified-Range-Iterator></emphasis>,Job>
<emphasis role="bold">parallel_partition</emphasis>(Range&amp;&amp; range,Func func,const uint32_t thread_num = boost::thread::hardware_concurrency(),const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: an iterator to the
                        first element of the second group for the first version. A pair of a range
                        and an iterator to the first element of the second group of this range for
                        the second and third.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the range of elements to partition</para>
                            <para>Or a continuation, coming from another algorithm.</para>
                        </listitem>
                        <listitem>
                                <para>func: unary predicate function. The signature of the function
                                    should be equivalent to the following: bool pred(const Type
                                    &amp;a);</para>
                            </listitem>
                            <listitem>
                                <para>thread_num: this algorithm being not a divide and conquer, it
                                    requires the number of threads which will be used. By default
                                    boost::thread::hardware_concurrency() is used.</para>
                            </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_stable_partition"/>parallel_stable_partition</title>
                    <para>Reorders the elements in the range [begin, end) in such a way that all
                        elements for which the predicate func returns true precede the elements for
                        which predicate func returns false. Relative order of the elements is
                        preserved. </para>
                    <programlisting>template &lt;class Iterator, class Iterator2, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator2</emphasis>,Job>
<emphasis role="bold">parallel_stable_partition</emphasis>(Iterator begin, Iterator end, Iterator2 out, Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking ownership of the container to be sorted
template &lt;class Range, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">&lt;std::pair&lt;Range,decltype(range.begin())></emphasis>,Job>
<emphasis role="bold">parallel_stable_partition</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Unspecified Range, Unspecified-Iterator></emphasis>,Job>
<emphasis role="bold">parallel_stable_partition</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Returns</emphasis>:</para>
                    <para>
                        <itemizedlist>
                            <listitem>
                                <para>an iterator to the first element of the second group (first
                                    version)</para>
                            </listitem>
                            <listitem>
                                <para>The partitioned range and an iterator to the first element of
                                    the second group  (second version)</para>
                            </listitem>
                            <listitem>
                                <para>The partitioned range returned from the continuation and an
                                    iterator to the first element of the second group  (second
                                    version)</para>
                            </listitem>
                        </itemizedlist>
                    </para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements to reorder.</para>
                                <para>Or range: a moved range. Returns the sorted moved
                                    range.</para>
                                <para>Or a continuation, coming from another algorithm. Returns the
                                    sorted range.</para>
                            </listitem>
                            <listitem>
                                <para>func:  unary predicate which returns â€‹true if the element
                                    should be ordered before other elements. The signature of the
                                    predicate function should be equivalent to the following: bool
                                    func(const Type &amp;a); </para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2> 
                <sect2>
                    <title><command xml:id="parallel_partition_copy"/>parallel_partition_copy</title>
                    <para>Copies the elements from the range [begin, end) to two output ranges in
                        such a way that all elements for which the predicate func returns true are
                        in the first range and all the others are in the second range. Relative
                        order of the elements is preserved. </para>
                    <programlisting>template &lt;class Iterator, class OutputIt1, class OutputIt2, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;OutputIt1, OutputIt2></emphasis>,Job>
<emphasis role="bold">parallel_partition_copy</emphasis>(Iterator begin, Iterator end, OutputIt1 out_true, OutputIt2 out_false, Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Returns</emphasis>: a pair of iterators to the
                        first element of the first and second group</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                                <para>begin, end: the range of elements to reorder.</para>
                            </listitem>
                            <listitem>
                                <para>out_true: the beginning of the range for which func returns
                                    true.</para>
                            </listitem>
                            <listitem>
                                <para>out_false: the beginning of the range for which func returns
                                    false.</para>
                            </listitem>
                        <listitem>
                            <para>func: unary predicate which returns â€‹true if the element should be
                                    in the first range. The signature of the predicate function
                                    should be equivalent to the following: bool func(const Type
                                    &amp;a); </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2> 
                <sect2>
                    <title><command xml:id="parallel_is_sorted"/>parallel_is_sorted</title>
                    <para>Checks if the elements in range [first, last) are sorted in ascending
                        order. It uses the given comparison function func to compare the elements. </para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_is_sorted</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if the elements
                        in the range are sorted in ascending order.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the range of elements to search</para>
                            <para>Or a continuation, coming from another algorithm.</para>
                        </listitem>
                        <listitem>
                            <para>func: binary predicate function. The signature of the function
                                    should be equivalent to the following: bool func(const Type
                                    &amp;a, const Type &amp;b);</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_is_reverse_sorted"/>parallel_is_reverse_sorted</title>
                    <para>Checks if the elements in range [first, last) are sorted in descending
                        order. It uses the given comparison function func to compare the elements. </para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">bool</emphasis>,Job>
<emphasis role="bold">parallel_is_reverse_sorted</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Return value</emphasis>: true if the elements
                        in the range are sorted in descending order.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: the range of elements to search</para>
                            <para>Or a continuation, coming from another algorithm.</para>
                        </listitem>
                        <listitem>
                            <para>func: binary predicate function. The signature of the function
                                should be equivalent to the following: bool func(const Type
                                &amp;a, const Type &amp;b);</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>                
                <sect2>
                    <title><command xml:id="parallel_iota"/>parallel_iota</title>
                    <para>Fills the range [begin, end) with sequentially increasing values, starting
                        with value and repetitively evaluating ++value</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_iota</emphasis>(Iterator beg, Iterator end,T const&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a moved range
template &lt;class Range, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_iota</emphasis>(Range&amp;&amp; range,T const&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Returns</emphasis>: a void continuation for the
                        first version, a continuation containing the new range for the second
                        one.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>beg, end: the range of elements</para>
                            <para>Or range: a moved range.</para>
                        </listitem>
                        <listitem>
                            <para>T value:  initial value to store, the expression ++value must
                                be well-formed </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>    
                <sect2>
                    <title><command xml:id="parallel_reduce"/>parallel_reduce</title>
                    <para><emphasis role="underline">Description</emphasis>: Sums up elements of a
                        range using func.</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">decltype(func(...))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">decltype(func(...))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">decltype(func(...))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>beg, end: the range of elements to sum</para>
                                <para>Or range: a moved range.</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary operation function object that will be applied.
                                    The signature of the function should be equivalent to the
                                    following: Ret fun(const Type &amp;a, const Type &amp;b);</para>
                                <para>Alternatively, func might have a signature with a range: Ret
                                    fun(Iterator a, Iterator b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                    <para><emphasis role="underline">Returns</emphasis>:  the return value type of
                        calling func.</para>
                    <para><emphasis role="underline">Example</emphasis>:</para>
                    <para>
                        <programlisting>std::vector&lt;int> data;
post_callback(
    [this]()
    {
       return boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(this->data.begin(),this->data.end(),
                                                     [](int const&amp; a, int const&amp; b)
                                                     {
                                                         return a + b; // returns an int
                                                     },
                                                     1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;int></emphasis> ){} // callback gets an int
);</programlisting>
                    </para>
                    <para>We also have a <link xlink:href="examples/example_parallel_reduce_tcp.cpp">distributed version</link> as an example, which strictly looks like the parallel_for version.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_inner_product"/>parallel_inner_product</title>
                    <para><emphasis role="underline">Description</emphasis>: Computes inner product
                        (i.e. sum of products) of the range [begin1, end1) and another range
                        beginning at begin2.It uses op and red for these tasks respectively. </para>
                    <programlisting>template &lt;class Iterator1, class Iterator2, class BinaryOperation, class Reduce, class Value, class Enable, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">T</emphasis>,Job>
<emphasis role="bold">parallel_inner_product</emphasis>(Iterator1 begin1, Iterator1 end1, Iterator2 begin2, BinaryOperation op, Reduce red, const Value&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range1, class Range2, class BinaryOperation, class Reduce, class Value, class Enable, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">T</emphasis>,Job>
<emphasis role="bold">parallel_inner_product</emphasis>(Range1 &amp;&amp; range1, Range2 &amp;&amp; range2, BinaryOperation op, Reduce red, const Value&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking continuations of ranges as first and second argument
template &lt;class Continuation1, class Continuation2, class BinaryOperation, class Reduce, class Value, class Enable, class T, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">T</emphasis>,Job>
<emphasis role="bold">parallel_inner_product</emphasis>(Continuation1 &amp;&amp; cont1, Continuation2 &amp;&amp; cont2, BinaryOperation op, Reduce red, const Value&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                                <para>begin1, end1: the first range of elements </para>
                                <para>Or range1: a moved range.</para>
                                <para>Or a cont1, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>begin2, end2: the second range of elements </para>
                                <para>Or range2: a moved range.</para>
                                <para>Or a cont2, coming from another algorithm.</para>
                            </listitem>
                        <listitem>
                                <para>op:  binary operation function object that will be applied.
                                    This "sum" function takes a value returned by op2 and the
                                    current value of the accumulator and produces a new value to be
                                    stored in the accumulator. </para>
                            </listitem>
                            <listitem>
                                <para>red: binary operation function object that will be applied.
                                    This "product" function takes one value from each range and
                                    produces a new value. The signature of the function should be
                                    equivalent to the following: Ret fun(const Type1 &amp;a, const
                                    Type2 &amp;b); </para>
                            </listitem>
                            <listitem>
                                <para>value: initial value of the sum of the products </para>
                            </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                    <para><emphasis role="underline">Returns</emphasis>:  the return value type of
                        calling func.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_partial_sum"/>parallel_partial_sum</title>
                    <para><emphasis role="underline">Description</emphasis>: Computes the partial
                        sums of the elements in the subranges of the range [begin, end) and writes
                        them to the range beginning at out. It uses the given binary function func
                        to sum up the elements.</para>
                    <programlisting>// version for iterators
template &lt;class Iterator, class OutIterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">OutIterator</emphasis>,Job>
<emphasis role="bold">parallel_partial_sum</emphasis>(Iterator beg, Iterator end, OutIterator out, Func func, long cutoff, const std::string&amp; task_name="", std::size_t prio=0);

// version for moved ranges
template &lt;class Range, class OutRange, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">std::pair&lt;Range,OutRange></emphasis>,Job>
<emphasis role="bold">parallel_partial_sum</emphasis>(Range&amp;&amp; range,OutRange&amp;&amp; out_range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version for a single moved range (in/out) => will return the range as continuation
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_partial_sum</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class OutIterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_partial_sum</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                                <para>begin, end: the range of elements to sum</para>
                                <para>Or range: a moved range.</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>out_range: a moved range which will be returned upon
                                    completion</para>
                            </listitem>
                        <listitem>
                            <para>func: binary operation function object that will be applied.
                                The signature of the function should be equivalent to the
                                following: Ret fun(const Type &amp;a, const Type &amp;b);</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                    <para><emphasis role="underline">Returns</emphasis>: Iterator to the element
                        past the last element written in the first version, an iterator and the
                        passed moved range in the second, the passed range in the third, a range
                        returned by the continuation in the fourth.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_merge"/>parallel_merge</title>
                    <para><emphasis role="underline">Description</emphasis>: Merges two sorted
                        ranges [begin1, end1) and [begin2, end2) into one sorted range beginning at
                        out. It uses the given comparison function func to compare the elements. For
                        equivalent elements in the original two ranges, the elements from the first
                        range (preserving their original order) precede the elements from the second
                        range (preserving their original order). The behavior is undefined if the
                        destination range overlaps either of the input ranges (the input ranges may
                        overlap each other). </para>
                    <programlisting>template &lt;class Iterator1, class Iterator2, class OutIterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_merge</emphasis>(Iterator1 begin1, Iterator1 end1, Iterator2 beg2, Iterator2 end2, OutIterator out, Func func, long cutoff, const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                                <para>begin1, end1:  the first range of elements to merge.</para>
                            </listitem>
                            <listitem>
                                <para>begin2, end2:  the second range of elements to merge.</para>
                            </listitem>
                        <listitem>
                            <para>out:  the beginning of the destination range </para>
                        </listitem>
                        <listitem>
                            <para>func: comparison function object (i.e. an object that satisfies
                                    the requirements of Compare) which returns â€‹true if the first
                                    argument is less than (i.e. is ordered before) the second. The
                                    signature of the comparison function should be equivalent to the
                                    following: bool func(const Type1 &amp;a, const Type2 &amp;b); </para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                    <para><emphasis role="underline">Returns</emphasis>: A void continuation.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_invoke"/>parallel_invoke</title>
                    <para>parallel_invoke invokes a variadic list of predicates in parallel and
                        returns a (continuation of) tuple of expected containing the result of all
                        of them. Functors have to be wrapped within a <emphasis role="bold"
                            >boost::asynchronous::to_continuation_task</emphasis>.</para>
                    <programlisting>template &lt;class Job, typename... Args>
boost::asynchronous::detail::<emphasis role="bold">callback_continuation</emphasis>&lt;std::tuple&lt;expected&lt;return type of args>...>,Job>
<emphasis role="bold">parallel_invoke</emphasis>(Args&amp;&amp;... args);</programlisting>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>args: functors to call.</para>
                            </listitem>
                        </itemizedlist></para>
                    <para><emphasis role="underline">Returns</emphasis>:  an expected of tuple of
                        expected containing the result of every called functor.</para>
                    <para><emphasis role="underline">Example</emphasis>:</para>
                    <para>Of course, the futures can have exceptions if exceptions are thrown, as in
                        the following example:</para>
                    <programlisting>post_callback(
               []()
               {
                   return boost::asynchronous::parallel_invoke&lt;boost::asynchronous::any_callable>(
                                     boost::asynchronous::<emphasis role="bold">to_continuation_task</emphasis>([](){throw my_exception();}), // void lambda
                                     boost::asynchronous::<emphasis role="bold">to_continuation_task</emphasis>([](){return 42.0;}));         // double lambda
                },// work
                // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
                [this](boost::<emphasis role="bold">asynchronous::expected</emphasis>&lt;std::<emphasis role="bold">tuple</emphasis>&lt;<emphasis role="bold">asynchronous::expected&lt;void>,asynchronous::expected&lt;double></emphasis>>> res)
                {
                   try
                   {
                        auto t = res.get();
                        std::cout &lt;&lt; "got result: " &lt;&lt; (<emphasis role="bold">std::get&lt;1></emphasis>(t)).get() &lt;&lt; std::endl;                // 42.0
                        std::cout &lt;&lt; "got exception?: " &lt;&lt; (<emphasis role="bold">std::get&lt;0></emphasis>(t)).has_exception() &lt;&lt; std::endl;  // true, has exception
                    }
                    catch(std::exception&amp; e)
                    {
                        std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                     }
                }// callback functor.
);</programlisting>
                    <para>Notice the use of <emphasis role="bold">to_continuation_task</emphasis> to
                        convert the lambdas in continuations.</para>
                    <para>As always, the callback lambda will be called when all tasks complete and
                        the futures are non-blocking.</para>
                    <para>Please have a look at the <link
                            xlink:href="examples/example_parallel_invoke.cpp">complete
                            example</link>.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="if_then_else"/>if_then_else</title>
                    <para><emphasis role="underline">Description</emphasis>: Executes a then or an
                        else clause passed as continuations depending on an if clause. If clause is
                        a functor returning a bool. Then and Else clauses are functors returning a
                        continuation. Typically, if_then_else is called after an algorithm returning
                        a continuation for further processing.</para>
                    <programlisting>template &lt;class IfClause, class ThenClause, class ElseClause, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
Unspecified-Continuation-Type
<emphasis role="bold">if_then_else</emphasis>((IfClause if_clause, ThenClause then_clause, ElseClause else_clause, const std::string&amp; task_name="");</programlisting>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>if_clause: unary predicate function which returns a bool. The
                                    signature of the function should be equivalent to the following:
                                    bool pred(const Type &amp;a); where Type is returned by a
                                    previous algorithm</para>
                        </listitem>
                        <listitem>
                            <para>then_clause: unary predicate function which returns a
                                    continuation. The signature of the function should be equivalent
                                    to the following: Unspecified-Continuation pred(const Type
                                    &amp;a); where Type is returned by a previous algorithm.</para>
                        </listitem>
                        <listitem>
                            <para>else_clause: unary predicate function which returns a
                                    continuation. The signature of the function should be equivalent
                                    to the following: Unspecified-Continuation pred(const Type
                                    &amp;a); where Type is returned by a previous algorithm. </para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                    </itemizedlist></para>
                    <para><emphasis role="underline">Returns</emphasis>: A continuation of the type
                        returned by then_clause or else_clause. Both must return the same
                        continuation type.</para>
                    <para><emphasis role="underline">Example:</emphasis> the following code calls a
                        parallel_for (1) based on the result of another algorithm, a parallel_for
                        (4). Both then_clause (2) and else_clause (3) will return a continuation
                        representing a different parallel_for. The if-clause (5) makes the
                        decision.</para>
                    <programlisting>post_callback(
           // task
           [this](){
                    // the outer algorithm (1). Called with a continuation returned by if_then_else
                    return boost::asynchronous::parallel_for(
                                    boost::asynchronous::if_then_else(
                                        // if clause (5). Here, always true.
                                        [](std::vector&lt;int> const&amp;){return true;},
                                        // then clause. Will be called, as if-clause (5) returns true. Returns a continuation
                                        [](std::vector&lt;int> res)
                                        {
                                            std::vector&lt;unsigned int> new_result(res.begin(),res.end());
                                            // This algorithm (2) will be called after (4)
                                            return boost::asynchronous::parallel_for(
                                                                std::move(new_result),
                                                                [](unsigned int const&amp; i)
                                                                {
                                                                   const_cast&lt;unsigned int&amp;>(i) += 3;
                                                                },1500);
                                        },
                                        // else clause. Will NOT be called, as if-clause returns true. Returns a continuation
                                        [](std::vector&lt;int> res)
                                        {
                                            std::vector&lt;unsigned int> new_result(res.begin(),res.end());
                                            // This algorithm (3) would be called after (4) if (5) returned false.
                                            return boost::asynchronous::parallel_for(
                                                                std::move(new_result),
                                                                [](unsigned int const&amp; i)
                                                                {
                                                                   const_cast&lt;unsigned int&amp;>(i) += 4;
                                                                },1500);
                                        }
                                    )
                                    // argument of if_then_else, a continuation (4)
                                    (
                                        boost::asynchronous::parallel_for(
                                                                         std::move(this->m_data),
                                                                         [](int const&amp; i)
                                                                         {
                                                                            const_cast&lt;int&amp;>(i) += 2;
                                                                         },1500)
                                    ),
                        [](unsigned int const&amp; i)
                        {
                           const_cast&lt;unsigned int&amp;>(i) += 1;
                        },1500
                    );
                    },
           // callback functor
           [](boost::asynchronous::expected&lt;std::vector&lt;unsigned int>> res){
                        auto modified_vec = res.get();
                        auto it = modified_vec.begin();
                        BOOST_CHECK_MESSAGE(*it == 7,"data[0] is wrong: "+ std::to_string(*it));
                        std::advance(it,100);
                        BOOST_CHECK_MESSAGE(*it == 7,"data[100] is wrong: "+ std::to_string(*it));
                        std::advance(it,900);
                        BOOST_CHECK_MESSAGE(*it == 7,"data[1000] is wrong: "+ std::to_string(*it));
                        std::advance(it,8999);
                        BOOST_CHECK_MESSAGE(*it == 7,"data[9999] is wrong: "+ std::to_string(*it));
                        auto r = std::accumulate(modified_vec.begin(),modified_vec.end(),0,[](int a, int b){return a+b;});
                        BOOST_CHECK_MESSAGE((r == 70000),
                                            ("result of parallel_for after if/else was " + std::to_string(r) +
                                             ", should have been 70000"));
           }
        );</programlisting>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_geometry_intersection_of_x"/>parallel_geometry_intersection_of_x</title>
                    <para>Calculate the intersection of any number of (Boost.Geometry) geometries.
                        The free function intersection calculates the spatial set theoretic
                        intersection of geometries. </para>
                    <para>
                        <programlisting>template &lt;class Iterator, class Range,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Iterator-Reference</emphasis>,Job>
<emphasis role="bold">parallel_geometry_intersection_of_x</emphasis>(Iterator beg, Iterator end,const std::string&amp; task_name="", std::size_t prio=0, long cutoff=300, long overlay_cutoff=1500, long partition_cutoff=80000);

template &lt;class Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range-Begin-Reference</emphasis>,Job>
<emphasis role="bold">parallel_geometry_intersection_of_x</emphasis>(Range&amp;&amp; range,const std::string&amp; task_name="", std::size_t prio=0, long cutoff=300, long overlay_cutoff=1500, long partition_cutoff=80000);

// version taking a continuation of a range as first argument
template &lt;class Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_geometry_intersection_of_x</emphasis>(Range range,const std::string&amp; task_name="", std::size_t prio=0, long cutoff=300, long overlay_cutoff=1500, long partition_cutoff=80000);</programlisting>
                    </para>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Returns</emphasis>: a geometry of the type
                        referenced by the iterator / contained in the range</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>beg, end: the range of input geometries</para>
                            <para>Or range: a moved range.</para>
                            <para>Or a continuation, coming from another algorithm.</para>
                        </listitem>
                        <listitem>
                            <para>cutoff: the maximum size of a sequential chunk of
                                geometries</para>
                        </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                        <listitem>
                            <para>overlay_cutoff: performance tuning. Cutoff used for the
                                parallelization of the internal overlay algorithm</para>
                        </listitem>
                        <listitem>
                            <para>partition_cutoff: performance tuning. Cutoff used for the
                                parallelization of the internal partition algorithm</para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_geometry_union_of_x"/>parallel_geometry_union_of_x</title>
                    <para>Combines any number of (Boost.Geometry) geometries which each other. he
                        free function union calculates the spatial set theoretic union of
                        geometries.</para>
                    <para>
                        <programlisting>template &lt;class Iterator, class Range,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_geometry_union_of_x</emphasis>(Iterator beg, Iterator end,const std::string&amp; task_name="", std::size_t prio=0, long cutoff=300, long overlay_cutoff=1500, long partition_cutoff=80000);

template &lt;class Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_geometry_union_of_x</emphasis>(Range&amp;&amp; range,const std::string&amp; task_name="", std::size_t prio=0, long cutoff=300, long overlay_cutoff=1500, long partition_cutoff=80000);

// version taking a continuation of a range as first argument
template &lt;class Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Unspecified-Range</emphasis>,Job>
<emphasis role="bold">parallel_geometry_union_of_x</emphasis>(Range range,const std::string&amp; task_name="", std::size_t prio=0, long cutoff=300, long overlay_cutoff=1500, long partition_cutoff=80000);</programlisting>
                    </para>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Returns</emphasis>: a sequence containing the
                        resulting geometries</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>beg, end: the range of geometries to combine</para>
                                <para>Or range: a moved range.</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk of
                                    geometries</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                            <listitem>
                                <para>overlay_cutoff: performance tuning. Cutoff used for the
                                    parallelization of the internal overlay algorithm</para>
                            </listitem>
                            <listitem>
                                <para>partition_cutoff: performance tuning. Cutoff used for the
                                    parallelization of the internal partition algorithm</para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_union"/>parallel_union</title>
                    <para> Combines two geometries which each other. </para>
                    <para>
                        <programlisting>template &lt;class Geometry1, class Geometry2, class Collection,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Collection</emphasis>,Job>
<emphasis role="bold">parallel_union</emphasis>(Geometry1 geometry1,Geometry2 geometry2,long overlay_cutoff,long partition_cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    </para>
                    <para><emphasis role="underline">Returns</emphasis>: a geometry. Currently, Type
                        of Geometry1= Type of Geometry2 = Type of Collection</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>geometry1, geometry2: the geometries to combine</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                            <listitem>
                                <para>overlay_cutoff: performance tuning. Cutoff used for the
                                    parallelization of the internal overlay algorithm</para>
                            </listitem>
                            <listitem>
                                <para>partition_cutoff: performance tuning. Cutoff used for the
                                    parallelization of the internal partition algorithm</para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_intersection"/>parallel_intersection</title>
                    <para>Calculate the intersection of two geometries.</para>
                    <para>
                        <programlisting>template &lt;class Geometry1, class Geometry2, class Collection,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Collection</emphasis>,Job>
<emphasis role="bold">parallel_intersection</emphasis>(Geometry1 geometry1,Geometry2 geometry2,long overlay_cutoff,long partition_cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    </para>
                    <para><emphasis role="underline">Returns</emphasis>: a geometry. Currently, Type
                        of Geometry1= Type of Geometry2 = Type of Collection</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>geometry1, geometry2: the geometries to combine</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                            <listitem>
                                <para>overlay_cutoff: performance tuning. Cutoff used for the
                                    parallelization of the internal overlay algorithm</para>
                            </listitem>
                            <listitem>
                                <para>partition_cutoff: performance tuning. Cutoff used for the
                                    parallelization of the internal partition algorithm</para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_find_all"/>parallel_find_all</title>
                    <para>Finds and copies into a returned container all elements of a range for
                        which a predicate returns true. </para>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>beg, end: the range of elements to search</para>
                                <para>Or range: a moved range.</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate function which returns true for searched
                                    elements. The signature of the function should be equivalent to
                                    the following: bool pred(const Type &amp;a);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                    <para><emphasis role="underline">Returns</emphasis>: a container with the
                        searched elements. Default is a std::vector for the iterator version, a
                        range of the same type as the input range for the others.</para>
                    <programlisting>template &lt;class Iterator, class Func,
          <emphasis role="bold">class ReturnRange=std::vector&lt;...></emphasis>,
          class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, <emphasis role="bold">class ReturnRange=Range</emphasis>, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class Func, <emphasis role="bold">class ReturnRange=typename Range::return_type</emphasis>, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para><emphasis role="underline">Example</emphasis>:</para>
                    <para>
                        <programlisting><emphasis role="bold">std::vector&lt;int></emphasis> data;
post_callback(
    [this]()
    {
       return boost::asynchronous::<emphasis role="bold">parallel_find_all</emphasis>(this->data.begin(),this->data.end(),
                                                     [](int i)
                                                     {
                                                         return (400 &lt;= i) &amp;&amp; (i &lt; 600);
                                                     },
                                                     1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;std::vector&lt;int>></emphasis> ){}
);</programlisting>
                    </para>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_find_all.cpp">complete example</link>.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_extremum"/>parallel_extremum</title>
                    <para>parallel_extremum finds an extremum (min/max) of a range given by a
                        predicate.</para>
                    <para>
                        <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;typename std::iterator_traits&lt;Iterator>::value_type,Job>
<emphasis role="bold">parallel_extremum</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    </para>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>beg, end: the range of elements to search</para>
                                <para>Or range: a moved range.</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function which returns true for
                                    searched elements. The signature of the function should be
                                    equivalent to the following: bool func(const Type &amp;a, const
                                    Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist>Please have a look at the <link
                            xlink:href="examples/example_parallel_extremum.cpp">complete
                            example</link>.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_count"/>parallel_count /
                        parallel_count_if</title>
                    <para>parallel_count counts the elements of a range satisfying a
                        predicate.</para>
                    <programlisting>template &lt;class Iterator, class T,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">long</emphasis>,Job>
<emphasis role="bold">parallel_count</emphasis>(Iterator beg, Iterator end,T const&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">long</emphasis>,Job>
<emphasis role="bold">parallel_count_if</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class T,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">long</emphasis>,Job>
<emphasis role="bold">parallel_count</emphasis>(Range&amp;&amp; range,T const&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
template &lt;class Range, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
<emphasis role="bold">parallel_count_if</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class T,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">long</emphasis>,Job>
<emphasis role="bold">parallel_count</emphasis>(Range range,T const&amp; value,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
template &lt;class Range, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">long</emphasis>,Job>
<emphasis role="bold">parallel_count_if</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>beg, end: the range of elements</para>
                                <para>Or range: a moved range.</para>
                                <para>Or a continuation, coming from another algorithm.</para>
                            </listitem>
                            <listitem>
                                <para>T value: the value to search for</para>
                            </listitem>
                            <listitem>
                                <para>func: unary predicate function which returns true for searched
                                    elements. The signature of the function should be equivalent to
                                    the following: bool func(const Type &amp;a);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist>Please have a look at the <link
                            xlink:href="examples/example_parallel_count.cpp">complete
                        example</link>.</para>
                </sect2>
                <sect2>
                    <title><command xml:id="parallel_sort"/>parallel_sort / parallel_stable_sort /
                        parallel_spreadsort / parallel_sort_inplace / parallel_stable_sort_inplace /
                        parallel_spreadsort_inplace</title>
                    <para>parallel_sort / parallel_stable_sort implement a parallel mergesort.
                        parallel_spreadsort is a parallel version of Boost.Spreadsort if
                        BOOST_ASYNCHRONOUS_USE_BOOST_SPREADSORT is defined. They all use a parallel
                        mergesort. For the sequential part, parallel_sort uses std::sort,
                        parallel_stable_sort uses std::stable_sort, parallel_spreadsort uses
                        Boost.Spreadsort.</para>
                    <para>parallel_sort_inplace / parallel_stable_sort_inplace /
                        parallel_spreadsort_inplace use an inplace merge to save memory, at the cost
                        of a performance penalty. For the sequential part, parallel_sort_inplace
                        uses std::sort, parallel_stable_sort_inplace uses std::stable_sort,
                        parallel_spreadsort_inplace uses Boost.Spreadsort.</para>
                    <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_sort</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_spreadsort</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_sort_inplace</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort_inplace</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_spreadsort_inplace</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking ownership of the container to be sorted
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_sort</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_spreadsort</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_sort_inplace</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort_inplace</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_spreadsort_inplace</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
<emphasis role="bold">parallel_sort</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_spreadsort</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_sort_inplace</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort_inplace</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_spreadsort_inplace</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>beg, end: the range of elements. Returns nothing.</para>
                                <para>Or range: a moved range. Returns the sorted moved
                                    range.</para>
                                <para>Or a continuation, coming from another algorithm. Returns the
                                    sorted range.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function which returns true for
                                    searched elements. The signature of the function should be
                                    equivalent to the following: bool func(const Type &amp;a, const
                                    Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist>Please have a look at the <link
                            xlink:href="examples/example_parallel_count.cpp">complete
                        example</link>.</para>
                </sect2> 
                <sect2>
                    <title><command xml:id="parallel_partial_sort"/>parallel_partial_sort</title>
                    <para>Rearranges elements such that the range [begin, middle) contains the
                        sorted middle - begin smallest elements in the range [begin, end). The order
                        of equal elements is not guaranteed to be preserved. The order of the
                        remaining elements in the range [begin, end) is unspecified. It uses the
                        given comparison function func. </para>
                    <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_partial_sort</emphasis>(Iterator begin, Iterator middle, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements.</para>
                            </listitem>
                            <listitem>
                                <para>middle: until where the range will be sorted</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function which returns true for
                                    searched elements. The signature of the function should be
                                    equivalent to the following: bool func(const Type &amp;a, const
                                    Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2> 
                <sect2>
                    <title><command xml:id="parallel_quicksort"/>parallel_quicksort /
                        parallel_quick_spreadsort</title>
                    <para>Sorts the range [begin,end) using quicksort. The order of the remaining
                        elements in the range [begin, end) is unspecified. It uses the given
                        comparison function func. parallel_quicksort will use std::sort to sort when
                        the algorithm finishes partitioning. parallel_quick_spreadsort will use
                        Boost.Spreadsort for this.</para>
                    <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_quicksort</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_quick_spreadsort</emphasis>(Iterator begin, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                            <listitem>
                                <para>begin, end: the range of elements.</para>
                            </listitem>
                            <listitem>
                                <para>func: binary predicate function which returns true for
                                    searched elements. The signature of the function should be
                                    equivalent to the following: bool func(const Type &amp;a, const
                                    Type &amp;b);</para>
                            </listitem>
                            <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>task_name: the name displayed in the scheduler
                                    diagnostics</para>
                            </listitem>
                            <listitem>
                                <para>prio: task priority </para>
                            </listitem>
                        </itemizedlist></para>
                </sect2>  
                <sect2>
                    <title><command xml:id="parallel_nth_element"/>parallel_nth_element</title>
                    <para>nth_element is a partial sorting algorithm that rearranges elements in
                        [begin, end) such that: The element pointed at by nth is changed to whatever
                        element would occur in that position if [begin, end) was sorted. All of the
                        elements before this new nth element are less than or equal to the elements
                        after the new nth element.  It uses the given comparison function func. </para>
                    <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
<emphasis role="bold">parallel_nth_element</emphasis>(Iterator begin, Iterator nth, Iterator end,Func func,long cutoff,const uint32_t thread_num = 1,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The version taking iterators requires that the iterators stay valid until
                        completion. It is the programmer's job to ensure this.</para>
                    <para><emphasis role="underline">Parameters</emphasis>: <itemizedlist>
                        <listitem>
                            <para>begin, end: andom access iterators defining the range sort.</para>
                        </listitem>
                        <listitem>
                            <para>nth: random access iterator defining the sort partition
                                    point</para>
                        </listitem>
                        <listitem>
                            <para>func: binary predicate function which returns true for
                                searched elements. The signature of the function should be
                                equivalent to the following: bool func(const Type &amp;a, const
                                Type &amp;b);</para>
                        </listitem>
                        <listitem>
                                <para>cutoff: the maximum size of a sequential chunk</para>
                            </listitem>
                            <listitem>
                                <para>thread_num: the number of threads in the pool executing the
                                    algorithm</para>
                            </listitem>
                        <listitem>
                            <para>task_name: the name displayed in the scheduler diagnostics</para>
                        </listitem>
                        <listitem>
                            <para>prio: task priority </para>
                        </listitem>
                    </itemizedlist></para>
                </sect2>                 
            </sect1>
            <sect1>
                <title>Parallel containers</title>
                <para>Any gain made by using a parallel algorithm can be reduced to nothing if the
                    calling codes spends most of its time creating a std::vector. Interestingly,
                    most parallel libraries provide parallel algorithms, but very few offer parallel
                    data structures. This is unfortunate because a container can be parallelized
                    with a great gain as long as the contained type either has a non-simple
                    constructor / destructor or simply is big enough, as our tests show (see
                    test/perf/perf_vector.cpp). Though memory allocating is not parallel,
                    constructors can be made so. Reallocating and resizing, adding elements can also
                    greatly benefit.</para>
                <para>Asynchronous fills this gap by providing boost::asynchronous::vector. It can
                    be used like std::vector by default.</para>
                <para>However, it can also be used as a parallel, synchronous type if provided a
                    threadpool. Apart from the construction, it looks and feels very much like a
                    std::vector. In this case, <emphasis role="underline">it cannot be posted to its
                        own threadpool without releasing it</emphasis> (see release_scheduler /
                    set_scheduler) as it would create a cycle, and therefore a possible deadlock. It
                    is defined in:</para>
                <para>#include &lt;boost/asynchronous/container/vector.hpp> </para>
                <para>The vector supports the same constructors that std::vector, with as extra
                    parameters, the threadpool for parallel work, and a cutoff. Optionally, a name
                    used for logging and a threadpool priority can be given, for example:</para>
                <programlisting>struct LongOne;
boost::asynchronous::any_shared_scheduler_proxy&lt;> pool = 
               boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;>
                        >>(tpsize,tasks);

<emphasis role="bold">boost::asynchronous::vector</emphasis>&lt;LongOne> vec (pool,1024 /* cutoff */, /* std::vector ctor arguments */ 10000,LongOne()
                                            // optional, name for logging, priority in the threadpool
                                            , "vector", 1);                </programlisting>
                <para>At this point, asynchronous::vector can be used like std::vector, with the
                    difference that constructor, destructor, operator=, assign, clear, push_back,
                    emplace_back, reserve, resize, erase, insert are executed in parallel in the
                    given threadpool.</para>
                <para>The vector adds a few members compared to std::vector:<itemizedlist>
                        <listitem>
                            <para>release_scheduler(): removes the threadpool from vector. At this
                                point, the vector is no more parallel, but can live from within the
                                pool.</para>
                        </listitem>
                        <listitem>
                            <para>set_scheduler(): (re)sets scheduler, so that vector is again
                                parallel. At this point, the vector cannot live from within the
                                pool.</para>
                        </listitem>
                        <listitem>
                            <para>long get_cutoff() const: returns the cutoff as given in
                                constructor.</para>
                        </listitem>
                        <listitem>
                            <para>std::string get_name() const: the logged name, as given in the
                                constructor.</para>
                        </listitem>
                        <listitem>
                            <para>std::size_t get_prio()const: the priority, as given in the
                                constructor.</para>
                        </listitem>
                </itemizedlist></para>
                <para><link xlink:href="examples/example_vector.cpp">This example</link> displays
                    some basic usage of vector.</para>
                <table frame="all">
                    <title>#include
                        &lt;boost/asynchronous/container/vector.hpp></title>
                    <tgroup cols="3">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.27*"/>
                        <colspec colname="newCol3" colnum="3" colwidth="2.53*"/>
                        <thead>
                            <row>
                                <entry>Public Member functions as in std::vector</entry>
                                <entry>Description</entry>
                                <entry>Parallel if threadpool?</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>(constructor)</entry>
                                <entry>constructs the vector</entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>(destructor)</entry>
                                <entry>destructs the vector</entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>operator=</entry>
                                <entry>assigns values to the container</entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>assign</entry>
                                <entry>assigns values to the container </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>get_allocator</entry>
                                <entry>returns the associated allocator </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>at</entry>
                                <entry>access specified element with bounds checking </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>operator[]</entry>
                                <entry>access specified element </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>front</entry>
                                <entry>access the first element </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>back</entry>
                                <entry>access the last element </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>data</entry>
                                <entry>direct access to the underlying array </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>begin / cbegin </entry>
                                <entry>returns an iterator to the beginning</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>end / cend </entry>
                                <entry>returns an iterator to the end </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>rbegin / crbegin </entry>
                                <entry>returns a reverse iterator to the beginning </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>rend / crend </entry>
                                <entry>returns a reverse iterator to the end </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>empty</entry>
                                <entry>checks whether the container is empty </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>size</entry>
                                <entry>returns the number of elements </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>max_size</entry>
                                <entry>returns the maximum possible number of elements</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>reserve</entry>
                                <entry>reserves storage </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>capacity</entry>
                                <entry>returns the number of elements that can be held in currently
                                    allocated storage </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>shrink_to_fit </entry>
                                <entry>reduces memory usage by freeing unused memory</entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>clear</entry>
                                <entry>clears the contents </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>insert</entry>
                                <entry>inserts elements </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>emplace</entry>
                                <entry>constructs element in-place </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>erase</entry>
                                <entry>erases elements</entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>push_back</entry>
                                <entry>adds an element to the end </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>emplace_back</entry>
                                <entry>constructs an element in-place at the end </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>pop_back</entry>
                                <entry>removes the last element</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>resize</entry>
                                <entry>changes the number of elements stored</entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>swap</entry>
                                <entry>swaps the contents </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>operator==</entry>
                                <entry>lexicographically compares the values in the vector </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>operator!=</entry>
                                <entry>lexicographically compares the values in the vector </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>operator&lt; </entry>
                                <entry>lexicographically compares the values in the vector </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>operator&lt;= </entry>
                                <entry>lexicographically compares the values in the vector </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>operator> </entry>
                                <entry>lexicographically compares the values in the vector </entry>
                                <entry>Yes</entry>
                            </row>
                            <row>
                                <entry>operator>=</entry>
                                <entry>lexicographically compares the values in the vector </entry>
                                <entry>Yes</entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <para>All these members have the same signature as std::vector. Only some
                    constructors are new. First the standard ones:</para>
                <programlisting>         vector();

explicit vector( const Allocator&amp; alloc );	

         vector( size_type count,
                 const T&amp; value = T(),
                 const Allocator&amp; alloc = Allocator());

template&lt; class InputIt >
vector( InputIt first, InputIt last,
        const Allocator&amp; alloc = Allocator() );

vector( const vector&amp; other );

vector( vector&amp;&amp; other )

vector( std::initializer_list&lt;T> init,
        const Allocator&amp; alloc = Allocator() );</programlisting>
                <para>There are variants taking a scheduler making them a servant with parallel capabilities:</para>
                <programlisting>explicit vector(boost::asynchronous::any_shared_scheduler_proxy&lt;Job> scheduler,
                long cutoff,
                const std::string&amp; task_name="", 
                std::size_t prio=0,
                const Alloc&amp; alloc = Alloc());

explicit vector( boost::asynchronous::any_shared_scheduler_proxy&lt;Job> scheduler,
                 long cutoff,
                 size_type count,
                 const std::string&amp; task_name="", 
                 std::size_t prio=0,
                 const Allocator&amp; alloc = Allocator());
	
explicit vector( boost::asynchronous::any_shared_scheduler_proxy&lt;Job> scheduler,
                 long cutoff,
                 size_type count,
                 const T&amp; value = T(),
                 const std::string&amp; task_name="", 
                 std::size_t prio=0,
                 const Allocator&amp; alloc = Allocator());

         template&lt; class InputIt >
         vector( boost::asynchronous::any_shared_scheduler_proxy&lt;Job> scheduler,
                 long cutoff,
                 InputIt first, InputIt last,
                 const std::string&amp; task_name="", 
                 std::size_t prio=0,
                 const Allocator&amp; alloc = Allocator() );

         vector( boost::asynchronous::any_shared_scheduler_proxy&lt;Job> scheduler,
                 long cutoff,
                 std::initializer_list&lt;T> init,
                 const std::string&amp; task_name="", 
                 std::size_t prio=0,
                 const Allocator&amp; alloc = Allocator() );</programlisting>
                <para>Some new members have also been added to handle the new functionality:</para>
                <table frame="all">
                    <title>#include
                        &lt;boost/asynchronous/container/vector.hpp></title>
                    <tgroup cols="3">
                        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.25*"/>
                        <colspec colname="newCol3" colnum="3" colwidth="2.5*"/>
                        <thead>
                            <row>
                                <entry>Public Member functions as in std::vector</entry>
                                <entry>Description</entry>
                                <entry>Parallel if threadpool?</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>void
                                    set_scheduler(any_shared_scheduler_proxy&lt;Job>)</entry>
                                <entry>adds / replaces the scheduler pool</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>void release_scheduler()</entry>
                                <entry>Removes the scheduler pool. vector is now "standard"</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>long get_cutoff()const</entry>
                                <entry>returns cutoff</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>void set_cutoff(long)</entry>
                                <entry>sets cutoff </entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>std::string get_name() const</entry>
                                <entry>returns vector name, used in task names</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>void set_name(std::string const&amp;)</entry>
                                <entry>sets vector name, used in task names</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>std::size_t get_prio()const</entry>
                                <entry>returns vector task priority in threadpool</entry>
                                <entry>No</entry>
                            </row>
                            <row>
                                <entry>void set_prio(std::size_t)</entry>
                                <entry>set vector task priority in threadpool</entry>
                                <entry>No</entry>
                            </row>                            
                        </tbody>
                    </tgroup>
                </table>
            </sect1>
        </chapter>
        <chapter>
            <title>Tips.</title>
            <sect1>
                <title>Which protections you get, which ones you don't.</title>
                <para>Asynchronous is doing much to protect developers from some ugly beasts around:<itemizedlist>
                        <listitem>
                            <para>(visible) threads</para>
                        </listitem>
                        <listitem>
                            <para>races</para>
                        </listitem>
                        <listitem>
                            <para>deadlocks</para>
                        </listitem>
                        <listitem>
                            <para>crashes at the end of an object lifetime</para>
                        </listitem>
                </itemizedlist></para>
                <para>It also helps parallelizing and improve performance by not blocking. It also
                    helps find out where bottlenecks and hidden possible performance gains
                    are.</para>
                <para>There are, however, things for which it cannot help:<itemizedlist>
                        <listitem>
                            <para>cycles in design</para>
                        </listitem>
                        <listitem>
                            <para>C++ legal ways to work around the protections if one really
                                wants.</para>
                        </listitem>
                        <listitem>
                            <para>blocking on a future if one really wants.</para>
                        </listitem>
                        <listitem>
                            <para>using "this" captured in a task lambda.</para>
                        </listitem>
                        <listitem>
                            <para>writing a not clean task with pointers or references to data used
                                in a servant.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>No cycle, ever</title>
                 <para>This is one of the first things one learns in a design class. Cycles are
                    evil. Everybody knows it. And yet, designs are often made without care in a too
                    agile process, dependency within an application is not thought out carefully
                    enough and cycles happen. What we do learn in these classes is that cycles make
                    our code monolithic and not reusable. What we however do not learn is how bad,
                    bad, bad this is in face of threads. It becomes impossible to follow the flow of
                    information, resource usage, degradation of performance. But the worst of all,
                    it becomes almost impossible to prevent deadlocks and resource leakage.</para>
                <para>Using Asynchronous will help write clean layered architectures. But it will
                    not replace carefully crafted designs, thinking before writing code and the
                    experience which make a good designer. Asynchronous will not be able to prevent
                    code having cycles in a design. </para>
                <para>Fortunately, there is an easy solution: back to the basics, well-thought
                    designs before coding, writing diagrams, using a real development process (hint:
                    an agile "process" is not all this in the author's mind).</para>
            </sect1>
            <sect1>
                <title>No "this" within a task.</title>
                <para>A very easy way to see if you are paving the way to a race even using
                    Asynchronous is to have a look at the captured variables of a lambda posted to a
                    threadpool. If you find "this", it's probably bad, unless you really know that
                    the single-thread code will do nothing. Apart from a simple application, this
                    will not be true. By extension, pointers, references, or even shared smart
                    pointers pointing to data living in a single-thread world is usually bad.</para>
                <para>Experience shows that there are only two safe way to pass data to a posted
                    task: copy for basic types or types having a trivial destructor and move for
                    everything else. Keep to this rule and you will be safe.</para>
                <para>On the other hand, "this" is okay in the capture list of a callback task as
                    Asynchronous will only call it if the servant is still alive.</para>
            </sect1>
        </chapter>
        <chapter>
            <title>Design examples</title>
            <para>This section shows some examples of strongly simplified designs using
                Asynchronous.</para>
            <sect1>
                <title>A state machine managing a succession of tasks</title>
                <para>This example will show how the library allows a solution more powerful than
                    the proposed future.then() extension.</para>
                <para>Futures returned by an asynchronous function like std::async have to be
                    get()-ed by the caller before a next task can be started. To overcome this
                    limitation, a solution is to add a then(some_functor) member to futures. The
                    idea is to chain several tasks without having to get() the first future. While
                    this provides some improvement, some serious limitations stay:<itemizedlist>
                        <listitem>
                            <para>A future still has to be waited for</para>
                        </listitem>
                        <listitem>
                            <para>The then-functor is executed in one of the worker threads</para>
                        </listitem>
                        <listitem>
                            <para>The then-functor has to be complete at the first call. By complete
                                we mean that it should not use any data from the caller thread to
                                avoid races.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>All this makes from the then functor a fire-and-forget one and prevents
                    reacting on changes happening between the first functor and the then
                    functor.</para>
                <para>A superior solution exists using Asynchronous schedulers. <link
                    xlink:href="examples/example_callback.cpp">In this example</link>, we
                    define a Manager object, which lives in his single thread scheduler. This
                    Manager, a simplified state machine, starts a task when asked (calling start()
                    on its proxy). Upon completing the first task, the Manager chooses to start or
                    not the second part of the calculation (what would be done in future.then()). In
                    our example, an event cancels the second part (calling cancel()) so that it
                    never starts. </para>
                <para>Notice in this example some important points:<itemizedlist>
                        <listitem>
                            <para>The time we choose to start or not the second part is done after
                                the first part completes, not before, which is a noticeable
                                improvement compared to future.then().</para>
                        </listitem>
                        <listitem>
                            <para>We have no race although no mutex is used and at least three
                                threads are implied (main / 2 std::thread / Manager's world /
                                threadpool). All events (start / cancel /completion of first task /
                                completion of second task) are done within the Manager thread
                                world.</para>
                        </listitem>
                        <listitem>
                            <para>A proxy object can be copied and the copies used safely in any
                                number of threads. The proxy is copied, the thread world no. We use
                                in our example 2 std::threads which both share the proxy (and share
                                control of the thread world's lifecycle) with the main thread and
                                access members of the servant, all safely. The last thread going
                                will cause the thread world to shutdown. </para>
                        </listitem>
                        <listitem>
                            <para>Thinking in general design, this is very powerful. Software is
                                usually designed in components, which one tries to make reusable.
                                This is usually difficult because of thread issues. This problem is
                                now gone. The component delimited by one (or several) proxy is safe,
                                completely reusable and its thread limits are well defined.</para>
                        </listitem>
                        <listitem>
                            <para>We have in this example only one servant and its proxy. It would
                                be easily possible to define any number of pair of those. In this
                                case, the last proxy destroyed would shut down the thread
                                world.</para>
                        </listitem>
                </itemizedlist></para>
                <para><link
                    xlink:href="examples/example_callback_msm.cpp">We can also write the same example using a real Boost.MSM state machine</link></para>
            </sect1>
             <sect1>
                <title>A layered application</title>
                 <para>A common design pattern for an application is organizing it into layers. We
                    suppose we are having three layers:<itemizedlist>
                        <listitem>
                            <para>TopLevel: what the user of the application is seeing</para>
                        </listitem>
                        <listitem>
                            <para>MiddleLevel: some internal business logic</para>
                        </listitem>
                        <listitem>
                            <para>LowLevel: communication layer</para>
                        </listitem>
                    </itemizedlist></para>
                 <para>This is a common design in lots of books. A top level layer receives user
                    commands , checks for their syntax, then delegates to a middle layer, composed
                    of business logic, checking whether the application is in a state to execute the
                    order. If it is, the low-level communication task is delegated to a low level
                    layer. </para>
                <para>Of course this example is strongly simplified. A layer can be made of hundreds
                    of objects and thousands of lines of code.</para>
                <para>What the books often ignore, however, are threads and lifecycle issues.
                    Non-trivial applications are likely to be running many threads. This is where
                    the problems start:<itemizedlist>
                        <listitem>
                            <para>Which layer is running into which thread?</para>
                        </listitem>
                        <listitem>
                            <para>How do to avoid races if each layer is running his own thread(s).
                                Usually, mutexes are used.</para>
                        </listitem>
                        <listitem>
                            <para>How to handle callbacks from lower layers, as these are likely to
                                be executed in the lower layer thread(s)</para>
                        </listitem>
                        <listitem>
                            <para>Lifecycles. Usually, each layer has responsibility of keeping his
                                lower layers alive. But how to handle destruction of higher-levels?
                                Callbacks might already be under way and they will soon meet a
                                destroyed mutex?</para>
                        </listitem>
                    </itemizedlist></para>
                 <para><inlinemediaobject>
                     <imageobject>
                         <imagedata fileref="pics/layers.jpg"/>
                     </imageobject>
                 </inlinemediaobject></para>
                 <para>Asychronous provides a solution to these problems:<itemizedlist>
                        <listitem>
                            <para>Each layer is living in its own thread world (or sharing
                                one).</para>
                        </listitem>
                        <listitem>
                            <para>Asynchronous guarantees that each layer will be destroyed in turn:
                                LowLevel - MiddleLevel - TopLevel.</para>
                        </listitem>
                        <listitem>
                            <para>Asynchronous provides proxies to serialize outside calls into a
                                servant thread world.</para>
                        </listitem>
                        <listitem>
                            <para>Asynchronous provides safe callbacks: each servant can use
                                make_safe_calback, which guarantees execution in the servant thread
                                if and only if the servant is still alive.</para>
                        </listitem>
                 </itemizedlist></para>
                 <para><link xlink:href="examples/example_layers.cpp">In this simplified
                        example</link>, each layer has its own thread world. Using the proxies
                    provided by the library, each servant is protected from races through calls from
                    their upper layer or the outside world (main). Servants are also protected from
                    callbacks from their lower layer by make_safe_callback.</para>
             </sect1>
            <sect1>
                <title>Boost.Meta State Machine and Asynchronous behind a Qt User Interface </title>
                 <para>We will now implement an application closer to a real-life case. The code is
                    to be found under asynchronous/libs/asynchronous/doc/examples/msmplayer/
                    .</para>
                <para>The Boost.MSM documentation introduces a CD player implementation. This CD
                    player reacts to events coming from a hardware: opening of the player, adding a
                    disc, playing a song, increasing volume, etc. This logic is implemented in the
                    logic layer, which mostly delegates it to a state machine based on Boost.MSM.
                    The file playerlogic.cpp implements this state machine, which is an extension of
                    the ones found in the Boost.MSM documentation.</para>
                <para>This logic is to be thought of as a reusable component, which must be
                    thread-safe, living in a clearly defined thread context. Asychronous handles
                    this: the PlayerLogic class is a servant, protected by a proxy,
                    PlayerLogicProxy, which in its constructor creates the necessary single-threaded
                    scheduler. At this point, we have a self-sufficient component.</para>
                <para>Supposing that, like often in real-life, that the hardware is not ready while
                    our software would be ready for testing, we decide to build a Qt application,
                    acting as a hardware simulator. This also allows fast prototyping, early
                    testing, writing of training material and concept-checking by the different
                    stakeholders.</para>
                <para>To achieve this, we write a QWidget-derived class named PlayerGui, a simple
                    interface simulating the controls which will be offered by the real CD player.
                    It implements IDisplay, the interface which the real CD player will provide. </para>
                <para>The real hardware will also implement IHardware, an interface allowing control
                    of the buttons and motors of the player. Our simple PlayerGui will also
                    implement it for simplicity.</para>
                <para>A Qt application is by definition asynchronous. Boost.Asynchronous provides
                    qt_servant, allowing a Qt object to make us of the library features (safe
                    callbacks, threadpools, etc.).</para>
                <para>The application is straightforward: PlayerGui creates the logic of the
                    application, which means constructing a PlayerLogicProxy object. After the
                    construction, we have a usable, movable, perfectly asynchronous component, which
                    means being based on an almost 0 time run-to-completion implemented by the state
                    machine. The PlayerGui itself is also fully asynchronous: all actions it
                    triggers are posted into the logic component, and therefore non-blocking. After
                    the logic updates its internal states, it calls a provided safe callback, which
                    will update the status of all buttons. So we now have an asynchronous, non
                    blocking user interface delegating handling the hardware to an asynchronous, non
                    blocking logic layer:</para>
                <para>
                    <itemizedlist>
                        <listitem>
                            <para>PlayerGui(<link xlink:href="examples/msmplayer/playergui.h">.h</link><link xlink:href="examples/msmplayer/playergui.cpp">.cpp</link>): a QWidget providing a simple user interface with
                                buttons like the real hardware would. It inherits qt_servant to get
                                access to safe callbacks. It provides SafeDisplay, an implementation
                                of IDisplay, interface which the real CD player will also implement,
                                and SafeHardware, an implementation of IHardware, an interface which
                                the real hardware will implement. The "Safe" part is that the
                                callbacks being passed are resulting from make_safe_callback: they
                                will be executed within the Qt thread, only if the QWidget is still
                                alive.</para>
                        </listitem>
                        <listitem>
                            <para>PlayerLogic(<link xlink:href="examples/msmplayer/playerlogic.h">.h</link><link xlink:href="examples/msmplayer/playerlogic.cpp">.cpp</link>): the entry point to our logic layer: a
                                trackable_servant, hiden behind a servant_proxy. In our example, it
                                will delegate all logic work to the state machine.</para>
                        </listitem>
                        <listitem>
                            <para><link xlink:href="examples/msmplayer/playerlogic.cpp">StateMachine</link>: a Boost.MSM state machine, implementing the whole CD
                                player logic.</para>
                        </listitem>
                        <listitem>
                            <para><link xlink:href="examples/msmplayer/idisplay.h">IDisplay</link>: the user interface provided by the real player.</para>
                        </listitem>
                        <listitem>
                            <para><link xlink:href="examples/msmplayer/ihardware.h">IHardware</link>: the interface provided by the real hardware (buttons,
                                motors, sensor, etc).</para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>This example shows very important concepts of the Boost.MSM and Asynchronous
                    libraries in actions:</para>
                <para>
                    <itemizedlist>
                        <listitem>
                            <para>A state machine is based on run-to-completion: it is unstable
                                while processing an event and should move as fast as possible back
                                to a stable state.</para>
                        </listitem>
                        <listitem>
                            <para>To achieve this, the actions should be short. Whatever takes long
                                is posted to a thread pool. Our actions are costing only the cost of
                                a transition and posting.</para>
                        </listitem>
                        <listitem>
                            <para>Asynchronous provides the infrastructure needed by the state
                                machine: pools, safe callbacks, protection from external
                                threads.</para>
                        </listitem>
                        <listitem>
                            <para>A logic component is behaving like a simple, safe, moveable
                                object. All the application sees is a proxy object.</para>
                        </listitem>
                        <listitem>
                            <para>A Qt Object can also make use of thread pools, safe
                                callbacks.</para>
                        </listitem>
                        <listitem>
                            <para>Our application is completely asynchronous: it never ever blocks.
                                This is no small feat when we consider that we are controlling a
                                "real" hardware with huge response times compared to the speed of a
                                CPU.</para>
                        </listitem>
                    </itemizedlist>                   
                </para>
                <para>Please have a closer look at the following implementation details:<itemizedlist>
                        <listitem>
                            <para><link xlink:href="examples/msmplayer/playerlogic.h">PlayerLogicProxy</link> has a future-free interface. Its members take a
                                callback. This is a sign there will be no blocking.</para>
                        </listitem>
                        <listitem>
                            <para>The state machine in PlayerLogic's state machine uses
                                post_callback for long tasks. In the callback, the next event
                                processing will start.</para>
                        </listitem>
                        <listitem>
                            <para>The UI (PlayerGui) is also non-blocking. We make use of callbacks
                                (look at the make_safe_callback calls). We therefore have a very
                                responsive UI.</para>
                        </listitem>
                        <listitem>
                            <para>PlayerGui::actionsHandler will set the new state of all buttons
                                each time the state machine updates its status.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
       </chapter>
    </part>
    <part>
        <title>Reference</title>
        <chapter>
            <title>Queues</title>
            <para> Asynchronous provides a range of queues with different trade-offs. Use
                    <code>lockfree_queue</code> as default for a quickstart with
                Asynchronous.</para>
            <sect1>
                <title>threadsafe_list</title>
                <para>This queue is mostly the one presented in Anthony Williams' book, "C++
                    Concurrency In Action". It is made of a single linked list of nodes, with a
                    mutex at each end of the queue to minimize contention. It is reasonably fast and
                    of simple usage. It can be used in all configurations of pools.</para>
                <para>Its constructor does not require any parameter forwarded from the
                    scheduler.</para>
                <para>Stealing: from the same queue end as pop. Will be implemented better (from the
                    other end to reduce contention) in a future version.</para>
                <para><emphasis role="underline">Caution</emphasis>: crashes were noticed with gcc
                    4.8 while 4.7 and clang 3.3 seemed ok though the compiler might be the reason.
                    For this reason, lockfree_queue is now the default queue.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class threadsafe_list;</programlisting>
            </sect1>
            <sect1>
                <title>lockfree_queue</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::queue</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation.
                    Please use this container as default when starting with Asynchronous.</para>
                <para>The container is faster than a <code>threadsafe_list</code>, provided one
                    manages to set the queue size to an optimum value. A too small size will cause
                    expensive memory allocations, a too big size will significantly degrade
                    performance.</para>
                <para>Its constructor takes optionally a default size forwarded from the
                    scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::queue</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_queue;</programlisting>
            </sect1>
            <sect1>
                <title>lockfree_spsc_queue</title>
                <para>This queue is a light wrapper around a
                        <code>boost::lockfree::spsc_queue</code>, which gives lockfree behavior at
                    the cost of an extra dynamic memory allocation. </para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: None. Stealing is not supported by
                        <code>boost::lockfree::spsc_queue</code>. It can only be used
                    Single-Producer / Single-Consumer, which reduces its typical usage to a queue of
                    a <code>multiqueue_threadpool_scheduler</code> as consumer, with a
                        <code>single_thread_scheduler</code> as producer.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_spsc_queue;                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_stack</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::stack</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation.
                    This container creates a task inversion as the last posted tasks will be
                    executed first.</para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::stack</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_stack;</programlisting>
            </sect1>
        </chapter>
        <chapter>
            <title>Schedulers</title>
            <para> There is no perfect scheduler. In any case it's a question of trade-off. Here are
                the schedulers offered by Asynchronous.</para>
            <sect1>
                <title>single_thread_scheduler</title>
                <para>The scheduler of choice for all servants which are not thread-safe. Serializes
                    all calls to a single queue and executes them in order. Using
                        <code>any_queue_container</code> as queue will however allow it to support
                    task priority.</para>
                <para>This scheduler does not steal from other queues or pools, and does not get
                    stolen from to avoid races.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue, class CPULoad>
class single_thread_scheduler;               </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
         boost::asynchronous::<emphasis role="bold">single_thread_scheduler</emphasis>&lt;
            boost::asynchronous::lockfree_queue&lt;>>>();  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
       boost::asynchronous::<emphasis role="bold">single_thread_scheduler</emphasis>&lt;
          boost::asynchronous::lockfree_queue&lt;>>>(10); // size of queue
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;std::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>();                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(10); // size of queue</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/single_thread_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>1</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 thread)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>multiple_thread_scheduler</title>
                <para>The scheduler is an extended version of single_thread_scheduler, where all
                    servants are operated by only one thread at a time, though not always the same
                    one. It creates a n (servants) to m (threads) dependency. The advantages of this
                    scheduler is that one long task will not block other servants, more flexibility
                    in distributing threads among servants, and better cache behaviour (a thread
                    tries to serve servants in order).</para>
                <para>This scheduler does not steal from other queues or pools, and does not get
                    stolen from to avoid races.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue, class CPULoad>
class multiple_thread_scheduler;               </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
           boost::asynchronous::<emphasis role="bold">multiple_thread_scheduler</emphasis>&lt;
              boost::asynchronous::lockfree_queue&lt;>>>(n,m); // n: max number of servants, m: number of worker threads

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
           boost::asynchronous::<emphasis role="bold">multiple_thread_scheduler</emphasis>&lt;
              boost::asynchronous::lockfree_queue&lt;>>>(n,m,10); // n: max number of servants, m: number of worker threads, 10: size of queue
 
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;std::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>(n,m); // n: max number of servants, m: number of worker threads
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(n,m,10); // n: max number of servants, m: number of worker threads, 10: size of queue</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/single_thread_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>1..n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>threadpool_scheduler</title>
                <para>The simplest and easiest threadpool using a single queue, though multiqueue
                    behavior could be done using <code>any_queue_container</code>. The advantage is
                    that it allows the pool to be given 0 thread and only be stolen from. The cost
                    is a slight performance loss due to higher contention on the single
                    queue.</para>
                <para>This pool does not steal from other pool's queues.</para>
                <para>Use this pool as default for a quickstart with Asynchronous.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class CPULoad>
class threadpool_scheduler;</programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;>>>(4,10); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;std::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>(4); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(4,10); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>multiqueue_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with multiple queues to reduce
                    contention. On the other hand, this pool requires at least one thread.</para>
                <para>This pool does not steal from other pool's queues though pool threads do steal
                    from each other's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; >, class CPULoad >
class multiqueue_threadpool_scheduler;</programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;>>>(4,10); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;std::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>(4); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(4,10); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with the added capability to steal
                    from other pool's queues within a <code>composite_threadpool_scheduler</code>.
                    Not used within a <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class CPULoad, bool /* InternalOnly */ = true >
class stealing_threadpool_scheduler;</programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool</programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;><emphasis role="bold">,true</emphasis> >>(4); // 4 threads in pool</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/stealing_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_multiqueue_threadpool_scheduler</title>
                <para>This is a <code>multiqueue_threadpool_scheduler</code> with the added
                    capability to steal from other pool's queues within a
                        <code>composite_threadpool_scheduler</code> (of course, threads within this
                    pool do steal from each other queues, with higher priority). Not used within a
                        <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>multiqueue_threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; >,class CPULoad, bool /* InternalOnly */= true  >
class stealing_multiqueue_threadpool_scheduler;</programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool  
                </programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>,boost::asynchronous::default_find_position&lt;>,<emphasis role="bold">true</emphasis>  >>(4); // 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/stealing_multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>composite_threadpool_scheduler</title>
                <para>This pool owns no thread by itself. Its job is to contain other pools,
                    accessible by the priority given by posting, and share all queues of its
                    subpools among them. Only the stealing_* pools and <code>asio_scheduler</code>
                    will make use of this and steal from other pools though.</para>
                <para>For creation we need to create other pool of stealing or not stealing, stolen
                    from or not, schedulers. stealing_xxx pools will try to steal jobs from other
                    pool of the same composite, but only if these schedulers support this. Other
                    threadpools will not steal but get stolen from.
                        <code>single_thread_scheduler</code> will not steal or get stolen
                    from.</para>
                <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 0 thread
// This scheduler does not steal from other schedulers, but will lend its queues for stealing
auto tp = boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;>>> (0,100);

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
auto tp2 = boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;>>> (3);

// composite pool made of the previous 2
auto tp_worker = boost::asynchronous::make_shared_scheduler_proxy&lt;<emphasis role="bold">boost::asynchronous::composite_threadpool_scheduler&lt;>>(tp,tp2)</emphasis>; 
                </programlisting>
                <para>Declaration:</para>
                <programlisting>template&lt;class Job = boost::asynchronous::any_callable,
         class FindPosition=boost::asynchronous::default_find_position&lt; >,
         class Clock = std::chrono::high_resolution_clock  >
class composite_threadpool_scheduler;                 
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/composite_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>0</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>asio_scheduler</title>
                <para>This pool brings the infrastructure and access to io_service for an integrated
                    usage of Boost.Asio. Furthermore, if used withing a
                        <code>composite_threadpool_scheduler</code>, it will steal jobs from other
                    pool's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class FindPosition=boost::asynchronous::default_find_position&lt; boost::asynchronous::sequential_push_policy >, class CPULoad >
class asio_scheduler;</programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::asio_scheduler&lt;>>(4); // 4 threads in pool</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/extensions/asio/asio_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No*</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
        </chapter>
        <chapter>
            <title>Performance tests</title>
            <sect1>
                <title>asynchronous::vector</title>
                <para><emphasis role="underline">Test</emphasis>:
                    libs/asynchonous/test/perf_vector.cpp. </para>
                <para><emphasis role="underline">Test</emphasis> processor Core i7-5960X / Xeon Phi
                    3120A. </para>
                <para><emphasis role="underline">Compiler</emphasis>: g++ 6.1, -O3, -std=c++14, link
                    with libtbbmalloc_proxy.</para>
                <para>The test uses a vector of 10000000 elements, each element containing a
                    std::vector of 10 integers.</para>
                <para>
                    <table frame="all">
                        <title>Performance of asynchronous::vector members using 4 threads</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="1.42*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.15*"/>
                            <colspec colname="newCol4" colnum="4" colwidth="1.15*"/>
                            <thead>
                                <row>
                                    <entry>Member</entry>
                                    <entry>std::vector</entry>
                                    <entry>asynchronous::vector</entry>
                                    <entry>speedup asynchronous / std</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Constructor</entry>
                                    <entry>385ms</entry>
                                    <entry>147ms</entry>
                                    <entry>2.62</entry>
                                </row>
                                <row>
                                    <entry>Copy</entry>
                                    <entry>337ms</entry>
                                    <entry>190ms</entry>
                                    <entry>1.77</entry>
                                </row>
                                <row>
                                    <entry>compare</entry>
                                    <entry>81ms</entry>
                                    <entry>43ms</entry>
                                    <entry>1.88</entry>
                                </row>
                                <row>
                                    <entry>clear</entry>
                                    <entry>221ms</entry>
                                    <entry>98ms</entry>
                                    <entry>2.25</entry>
                                </row>
                                <row>
                                    <entry>resize</entry>
                                    <entry>394ms</entry>
                                    <entry>276ms</entry>
                                    <entry>1.43</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                    <table frame="all">
                        <title>Performance of asynchronous::vector members using 8 threads</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="1.42*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.15*"/>
                            <colspec colname="newCol4" colnum="4" colwidth="1.15*"/>
                            <thead>
                                <row>
                                    <entry>Member</entry>
                                    <entry>std::vector</entry>
                                    <entry>asynchronous::vector</entry>
                                    <entry>speedup asynchronous / std</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Constructor</entry>
                                    <entry>380ms</entry>
                                    <entry>90ms</entry>
                                    <entry>4.22</entry>
                                </row>
                                <row>
                                    <entry>Copy</entry>
                                    <entry>306ms</entry>
                                    <entry>120ms</entry>
                                    <entry>2.55</entry>
                                </row>
                                <row>
                                    <entry>compare</entry>
                                    <entry>74ms</entry>
                                    <entry>30ms</entry>
                                    <entry>2.47</entry>
                                </row>
                                <row>
                                    <entry>clear</entry>
                                    <entry>176ms</entry>
                                    <entry>54ms</entry>
                                    <entry>3.26</entry>
                                </row>
                                <row>
                                    <entry>resize</entry>
                                    <entry>341ms</entry>
                                    <entry>178ms</entry>
                                    <entry>1.92</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                    <table frame="all">
                        <title>Performance of asynchronous::vector members Xeon Phi 3120A 57 Cores /
                            228 Threads</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="1.42*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.15*"/>
                            <colspec colname="newCol4" colnum="4" colwidth="1.15*"/>
                            <thead>
                                <row>
                                    <entry>Member</entry>
                                    <entry>std::vector</entry>
                                    <entry>asynchronous::vector</entry>
                                    <entry>speedup asynchronous / std</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Constructor</entry>
                                    <entry>4175 ms</entry>
                                    <entry>240 ms</entry>
                                    <entry>17.4</entry>
                                </row>
                                <row>
                                    <entry>Copy</entry>
                                    <entry>5439 ms</entry>
                                    <entry>389 ms</entry>
                                    <entry>14</entry>
                                </row>
                                <row>
                                    <entry>compare</entry>
                                    <entry>4139 ms</entry>
                                    <entry>43 ms</entry>
                                    <entry>96</entry>
                                </row>
                                <row>
                                    <entry>clear</entry>
                                    <entry>2390 ms</entry>
                                    <entry>39 ms</entry>
                                    <entry>61.3</entry>
                                </row>
                                <row>
                                    <entry>resize</entry>
                                    <entry>5223 ms</entry>
                                    <entry>222 ms</entry>
                                    <entry>23.5</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>Sort</title>
                <para>This test will compare asynchronous:parallel_sort with TBB 4.3 parallel_sort.
                    16 threads used.</para>
                <para><emphasis role="underline">Test</emphasis>:
                    libs/asynchonous/test/perf/parallel_sort_future_v1.cpp. TBB test:
                    libs/asynchronous/test/perf/tbb/tbb_parallel_sort.cpp</para>
                <para><emphasis role="underline">Test</emphasis> processor Core i7-5960X. </para>
                <para><emphasis role="underline">Compiler</emphasis>: g++ 6.1, -O3, -std=c++14, link
                    with libtbbmalloc_proxy.</para>
                <para>The same test will be done using TBB then asynchronous + std::sort, then
                    asynchronous + boost::spreadsort. 
                    <table frame="all">
                        <title>Sorting 200000000 uint32_t</title>
                        <tgroup cols="6">
                            <colspec colname="c1" colnum="1" colwidth="3.1*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.22*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                            <colspec colname="newCol4" colnum="4" colwidth="1.43*"/>
                            <colspec colname="newCol5" colnum="5" colwidth="1.37*"/>
                            <colspec colname="newCol6" colnum="6" colwidth="2.48*"/>
                            <thead>
                                <row>
                                    <entry>Test</entry>
                                    <entry>TBB</entry>
                                    <entry>asynchronous</entry>
                                    <entry>asynchronous + boost::spreadsort</entry>
                                    <entry>asynchronous::parallel_sort2</entry>
                                    <entry>asynchronous::parallel_sort2 + boost::spreadsort</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>test_random_elements_many_repeated</entry>
                                    <entry>2416 ms</entry>
                                    <entry>1133 ms</entry>
                                    <entry>561 ms</entry>
                                    <entry>1425 ms</entry>
                                    <entry>928 ms</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_few_repeated</entry>
                                    <entry>2408 ms</entry>
                                    <entry>1301 ms</entry>
                                    <entry>1002 ms</entry>
                                    <entry>1694 ms</entry>
                                    <entry>1385 ms</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_quite_repeated</entry>
                                    <entry>2341 ms</entry>
                                    <entry>1298 ms</entry>
                                    <entry>1027 ms</entry>
                                    <entry>1687 ms</entry>
                                    <entry>1344 ms</entry>
                                </row>
                                <row>
                                    <entry>test_sorted_elements</entry>
                                    <entry>22.5 ms</entry>
                                    <entry>16 ms</entry>
                                    <entry>16 ms</entry>
                                    <entry>16 ms</entry>
                                    <entry>16 ms</entry>
                                </row>
                                <row>
                                    <entry>test_reversed_sorted_elements</entry>
                                    <entry>554 ms</entry>
                                    <entry>47 ms</entry>
                                    <entry>47 ms</entry>
                                    <entry>48 ms</entry>
                                    <entry>48 ms</entry>
                                </row>
                                <row>
                                    <entry>test_equal_elements</entry>
                                    <entry>26 ms</entry>
                                    <entry>16 ms</entry>
                                    <entry>16 ms</entry>
                                    <entry>17 ms</entry>
                                    <entry>17 ms</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></para>
                <table frame="all">
                    <title>Sorting 200000000 double</title>
                    <tgroup cols="6">
                        <colspec colname="c1" colnum="1" colwidth="3.06*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.26*"/>
                        <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="newCol4" colnum="4" colwidth="1.41*"/>
                        <colspec colname="newCol5" colnum="5" colwidth="1.2*"/>
                        <colspec colname="newCol6" colnum="6" colwidth="2.66*"/>
                        <thead>
                            <row>
                                <entry>Test</entry>
                                <entry>TBB</entry>
                                <entry>asynchronous</entry>
                                <entry>asynchronous + boost::spreadsort</entry>
                                <entry>asynchronous::parallel_sort2</entry>
                                <entry>asynchronous::parallel_sort2 + boost::spreadsort</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>test_random_elements_many_repeated</entry>
                                <entry>2504 ms</entry>
                                <entry>1446 ms</entry>
                                <entry>1133 ms</entry>
                                <entry>2173 ms</entry>
                                <entry>1919 ms</entry>
                            </row>
                            <row>
                                <entry>test_random_elements_few_repeated</entry>
                                <entry>2690 ms</entry>
                                <entry>1714 ms</entry>
                                <entry>1266 ms</entry>
                                <entry>2406 ms</entry>
                                <entry>2044 ms</entry>
                            </row>
                            <row>
                                <entry>test_random_elements_quite_repeated</entry>
                                <entry>2602 ms</entry>
                                <entry>1715 ms</entry>
                                <entry>1309 ms</entry>
                                <entry>2448 ms</entry>
                                <entry>2037 ms</entry>
                            </row>
                            <row>
                                <entry>test_sorted_elements</entry>
                                <entry>34 ms</entry>
                                <entry>32 ms</entry>
                                <entry>32 ms</entry>
                                <entry>32 ms</entry>
                                <entry>34 ms</entry>
                            </row>
                            <row>
                                <entry>test_reversed_sorted_elements</entry>
                                <entry>644 ms</entry>
                                <entry>95 ms</entry>
                                <entry>94 ms</entry>
                                <entry>95 ms</entry>
                                <entry>95 ms</entry>
                            </row>
                            <row>
                                <entry>test_equal_elements</entry>
                                <entry>34 ms</entry>
                                <entry>33 ms</entry>
                                <entry>32 ms</entry>
                                <entry>33 ms</entry>
                                <entry>32 ms</entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <table frame="all">
                    <title>Sorting 200000000 std::string</title>
                    <tgroup cols="6">
                        <colspec colname="c1" colnum="1" colwidth="3.36*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.39*"/>
                        <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                        <colspec colname="newCol4" colnum="4" colwidth="1.52*"/>
                        <colspec colname="newCol5" colnum="5" colwidth="1.28*"/>
                        <colspec colname="newCol6" colnum="6" colwidth="2.91*"/>
                        <thead>
                            <row>
                                <entry>Test</entry>
                                <entry>TBB</entry>
                                <entry>asynchronous</entry>
                                <entry>asynchronous + boost::spreadsort</entry>
                                <entry>asynchronous::parallel_sort2</entry>
                                <entry>asynchronous::parallel_sort2 + boost::spreadsort</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>test_random_elements_many_repeated</entry>
                                <entry>891 ms</entry>
                                <entry>924 ms</entry>
                                <entry>791 ms</entry>
                                <entry>889 ms</entry>
                                <entry>777 ms</entry>
                            </row>
                            <row>
                                <entry>test_random_elements_few_repeated</entry>
                                <entry>1031 ms</entry>
                                <entry>1069 ms</entry>
                                <entry>906 ms</entry>
                                <entry>1053 ms</entry>
                                <entry>967 ms</entry>
                            </row>
                            <row>
                                <entry>test_random_elements_quite_repeated</entry>
                                <entry>929 ms</entry>
                                <entry>1000 ms</entry>
                                <entry>838 ms</entry>
                                <entry>998 ms</entry>
                                <entry>1003 ms</entry>
                            </row>
                            <row>
                                <entry>test_sorted_elements</entry>
                                <entry>11 ms</entry>
                                <entry>16 ms</entry>
                                <entry>16 ms</entry>
                                <entry>16 ms</entry>
                                <entry>32 ms</entry>
                            </row>
                            <row>
                                <entry>test_reversed_sorted_elements</entry>
                                <entry>265 ms</entry>
                                <entry>28 ms</entry>
                                <entry>28 ms</entry>
                                <entry>29 ms</entry>
                                <entry>38 ms</entry>
                            </row>
                            <row>
                                <entry>test_equal_elements</entry>
                                <entry>12 ms</entry>
                                <entry>4 ms</entry>
                                <entry>3 ms</entry>
                                <entry>3 ms</entry>
                                <entry>4 ms</entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <table frame="all">
                    <title>Sorting 10000000 objects containing 10 longs</title>
                    <tgroup cols="4">
                        <colspec colname="c1" colnum="1" colwidth="2.39*"/>
                        <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                        <colspec colname="newCol3" colnum="3" colwidth="1.24*"/>
                        <colspec colname="newCol5" colnum="4" colwidth="1.7*"/>
                        <thead>
                            <row>
                                <entry>Test</entry>
                                <entry>TBB</entry>
                                <entry>asynchronous</entry>
                                <entry>asynchronous::parallel_sort2</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>test_random_elements_many_repeated</entry>
                                <entry>869 ms</entry>
                                <entry>1007 ms</entry>
                                <entry>204 ms</entry>
                            </row>
                            <row>
                                <entry>test_random_elements_few_repeated</entry>
                                <entry>803 ms</entry>
                                <entry>887 ms</entry>
                                <entry>226 ms</entry>
                            </row>
                            <row>
                                <entry>test_random_elements_quite_repeated</entry>
                                <entry>810 ms</entry>
                                <entry>960 ms</entry>
                                <entry>175 ms</entry>
                            </row>
                            <row>
                                <entry>test_sorted_elements</entry>
                                <entry>22 ms</entry>
                                <entry>27 ms</entry>
                                <entry>2 ms</entry>
                            </row>
                            <row>
                                <entry>test_reversed_sorted_elements</entry>
                                <entry>338 ms</entry>
                                <entry>34 ms</entry>
                                <entry>3 ms</entry>
                            </row>
                            <row>
                                <entry>test_equal_elements</entry>
                                <entry>25 ms</entry>
                                <entry>23 ms</entry>
                                <entry>2 ms</entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
            </sect1>
            <sect1>
                <title>parallel_scan</title>
                <para><emphasis role="underline">Test</emphasis>:
                    libs/asynchonous/test/perf_scan.cpp. </para>
                <para><emphasis role="underline">Test</emphasis> processor Core i7-5960X / Xeon Phi
                    3120A. </para>
                <para><emphasis role="underline">Compiler</emphasis>: g++ 6.1, -O3, -std=c++14, link
                    with libtbbmalloc_proxy.</para>
                <para>The test will exercise 10 times a parallel_scan on vector of 1000000
                    elements.</para>
                <para>
                    <table frame="all">
                        <title>Performance of parallel_scan vs serial scan on a i7 / Xeon Phi
                            Knight's Corner</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="1.42*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.15*"/>
                            <colspec colname="newCol4" colnum="4" colwidth="1.15*"/>
                            <thead>
                                <row>
                                    <entry>Member</entry>
                                    <entry>parallel_scan</entry>
                                    <entry>sequential scan</entry>
                                    <entry>speedup parallel / serial</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>i7-5960X (8 Cores / 16 Threads)</entry>
                                    <entry>204 ms</entry>
                                    <entry>812 ms</entry>
                                    <entry>4</entry>
                                </row>
                                <row>
                                    <entry>i7-5960X (8 Cores / 8 Threads)</entry>
                                    <entry>305 ms</entry>
                                    <entry>887 ms</entry>
                                    <entry>2.9</entry>
                                </row>
                                <row>
                                    <entry>Xeon Phi (57 Cores /228 Threads)</entry>
                                    <entry>74 ms</entry>
                                    <entry>423 ms</entry>
                                    <entry>57</entry>
                                </row>
                                <row>
                                    <entry>Xeon Phi (57 Cores /114 Threads)</entry>
                                    <entry>143 ms</entry>
                                    <entry>354 ms</entry>
                                    <entry>25</entry>
                                </row>
                                <row>
                                    <entry>Xeon Phi (57 Cores /10 Threads)</entry>
                                    <entry>373 ms</entry>
                                    <entry>350 ms</entry>
                                    <entry>9.4</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>                    
                </para>
            </sect1>
            <sect1>
                <title>parallel_stable_partition</title>
                <para><emphasis role="underline">Test</emphasis>:
                    libs/asynchonous/test/perf_scan.cpp. </para>
                <para><emphasis role="underline">Test</emphasis> processor Core i7-5960X / Xeon Phi
                    3120A. </para>
                <para><emphasis role="underline">Compiler</emphasis>: g++ 6.1, -O3, -std=c++11, link
                    with libtbbmalloc_proxy.</para>
                <para>The test will exercise a parallel_stable_partition on vector of 100000000
                    floats. As comparison, std::partition will be used.</para>
                <para>
                    <table frame="all">
                        <title>Partitioning 100000000 floats on Core i7-5960X 8 Cores / 8 Threads
                            (16 Threads bring no added value)</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="3.06*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.26*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                            <colspec colname="newCol6" colnum="4" colwidth="2.66*"/>
                            <thead>
                                <row>
                                    <entry>Test</entry>
                                    <entry>parallel</entry>
                                    <entry>std::partition</entry>
                                    <entry>speedup parallel / serial</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>test_random_elements_many_repeated</entry>
                                    <entry>187 ms</entry>
                                    <entry>720 ms</entry>
                                    <entry>3.87</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_few_repeated</entry>
                                    <entry>171 ms</entry>
                                    <entry>1113 ms</entry>
                                    <entry>6.5</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_quite_repeated</entry>
                                    <entry>172 ms</entry>
                                    <entry>555 ms</entry>
                                    <entry>3.22</entry>
                                </row>
                                <row>
                                    <entry>test_sorted_elements</entry>
                                    <entry>176 ms</entry>
                                    <entry>1139 ms</entry>
                                    <entry>6.5</entry>
                                </row>
                                <row>
                                    <entry>test_reversed_sorted_elements</entry>
                                    <entry>180 ms</entry>
                                    <entry>1125 ms</entry>
                                    <entry>6.25</entry>
                                </row>
                                <row>
                                    <entry>test_equal_elements</entry>
                                    <entry>168 ms</entry>
                                    <entry>1121 ms</entry>
                                    <entry>6.7</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                    <table frame="all">
                        <title>Partitioning 100000000 floats on Core i7-5960X 4 Cores / 4
                            Threads</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="3.06*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.26*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                            <colspec colname="newCol6" colnum="4" colwidth="2.66*"/>
                            <thead>
                                <row>
                                    <entry>Test</entry>
                                    <entry>parallel</entry>
                                    <entry>std::partition</entry>
                                    <entry>speedup parallel / serial</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>test_random_elements_many_repeated</entry>
                                    <entry>296 ms</entry>
                                    <entry>720 ms</entry>
                                    <entry>2.43</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_few_repeated</entry>
                                    <entry>301 ms</entry>
                                    <entry>1113 ms</entry>
                                    <entry>3.7</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_quite_repeated</entry>
                                    <entry>294 ms</entry>
                                    <entry>555 ms</entry>
                                    <entry>1.9</entry>
                                </row>
                                <row>
                                    <entry>test_sorted_elements</entry>
                                    <entry>287 ms</entry>
                                    <entry>1139 ms</entry>
                                    <entry>4</entry>
                                </row>
                                <row>
                                    <entry>test_reversed_sorted_elements</entry>
                                    <entry>288 ms</entry>
                                    <entry>1125 ms</entry>
                                    <entry>3.9</entry>
                                </row>
                                <row>
                                    <entry>test_equal_elements</entry>
                                    <entry>286 ms</entry>
                                    <entry>1121 ms</entry>
                                    <entry>3.9</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                    <table frame="all">
                        <title>Partitioning 100000000 floats on Xeon Phi 3120A 57 Cores / 228
                            Threads</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="3.06*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.26*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                            <colspec colname="newCol6" colnum="4" colwidth="2.66*"/>
                            <thead>
                                <row>
                                    <entry>Test</entry>
                                    <entry>parallel</entry>
                                    <entry>std::partition</entry>
                                    <entry>speedup parallel / serial</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>test_random_elements_many_repeated</entry>
                                    <entry>88 ms</entry>
                                    <entry>15944 ms</entry>
                                    <entry>181</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_few_repeated</entry>
                                    <entry>80 ms</entry>
                                    <entry>27186 ms</entry>
                                    <entry>339</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_quite_repeated</entry>
                                    <entry>89 ms</entry>
                                    <entry>16067 ms</entry>
                                    <entry>180</entry>
                                </row>
                                <row>
                                    <entry>test_sorted_elements</entry>
                                    <entry>77 ms</entry>
                                    <entry>26830 ms</entry>
                                    <entry>348</entry>
                                </row>
                                <row>
                                    <entry>test_reversed_sorted_elements</entry>
                                    <entry>73 ms</entry>
                                    <entry>27367 ms</entry>
                                    <entry>374</entry>
                                </row>
                                <row>
                                    <entry>test_equal_elements</entry>
                                    <entry>82 ms</entry>
                                    <entry>27464 ms</entry>
                                    <entry>334</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>   
                    <table frame="all">
                        <title>Partitioning 100000000 floats on Xeon Phi 3120A 57 Cores / 114
                            Threads</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="3.06*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.26*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                            <colspec colname="newCol6" colnum="4" colwidth="2.66*"/>
                            <thead>
                                <row>
                                    <entry>Test</entry>
                                    <entry>parallel</entry>
                                    <entry>std::partition</entry>
                                    <entry>speedup parallel / serial</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>test_random_elements_many_repeated</entry>
                                    <entry>152 ms</entry>
                                    <entry>15944 ms</entry>
                                    <entry>104</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_few_repeated</entry>
                                    <entry>129 ms</entry>
                                    <entry>27186 ms</entry>
                                    <entry>210</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_quite_repeated</entry>
                                    <entry>153 ms</entry>
                                    <entry>16067 ms</entry>
                                    <entry>105</entry>
                                </row>
                                <row>
                                    <entry>test_sorted_elements</entry>
                                    <entry>104 ms</entry>
                                    <entry>26830 ms</entry>
                                    <entry>258</entry>
                                </row>
                                <row>
                                    <entry>test_reversed_sorted_elements</entry>
                                    <entry>110 ms</entry>
                                    <entry>27367 ms</entry>
                                    <entry>249</entry>
                                </row>
                                <row>
                                    <entry>test_equal_elements</entry>
                                    <entry>114 ms</entry>
                                    <entry>27464 ms</entry>
                                    <entry>241</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>    
                    <table frame="all">
                        <title>Partitioning 100000000 floats on Xeon Phi 3120A 57 Cores / 10
                            Threads</title>
                        <tgroup cols="4">
                            <colspec colname="c1" colnum="1" colwidth="3.06*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.26*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="1.0*"/>
                            <colspec colname="newCol6" colnum="4" colwidth="2.66*"/>
                            <thead>
                                <row>
                                    <entry>Test</entry>
                                    <entry>parallel</entry>
                                    <entry>std::partition</entry>
                                    <entry>speedup parallel / serial</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>test_random_elements_many_repeated</entry>
                                    <entry>1131 ms</entry>
                                    <entry>15944 ms</entry>
                                    <entry>14</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_few_repeated</entry>
                                    <entry>816 ms</entry>
                                    <entry>27186 ms</entry>
                                    <entry>33</entry>
                                </row>
                                <row>
                                    <entry>test_random_elements_quite_repeated</entry>
                                    <entry>1212 ms</entry>
                                    <entry>16067 ms</entry>
                                    <entry>13</entry>
                                </row>
                                <row>
                                    <entry>test_sorted_elements</entry>
                                    <entry>755 ms</entry>
                                    <entry>26830 ms</entry>
                                    <entry>35</entry>
                                </row>
                                <row>
                                    <entry>test_reversed_sorted_elements</entry>
                                    <entry>739 ms</entry>
                                    <entry>27367 ms</entry>
                                    <entry>37</entry>
                                </row>
                                <row>
                                    <entry>test_equal_elements</entry>
                                    <entry>798 ms</entry>
                                    <entry>27464 ms</entry>
                                    <entry>34</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>                        
                </para>
                <para>The Xeon Phi speedups are quie surprising. The implementation of
                    std::partition seems inefficient on this platform.</para>
            </sect1>
            <sect1>
                <title>parallel_for</title>
                <para><emphasis role="underline">Test</emphasis>:
                    libs/asynchonous/test/parallel_for.cpp. </para>
                <para><emphasis role="underline">Test</emphasis> processor Core i7-5960X / Xeon Phi
                    3120A. </para>
                <para><emphasis role="underline">Compiler</emphasis>: g++ 6.1, -O3, -std=c++14, link
                    with libtbbmalloc_proxy.</para>
                <para>The test will exercise 10 times a parallel_for with a dummy operation on a
                    vector of 100000000 elements.</para>
                <para>
                    <table frame="all">
                        <title>Performance of parallel_for on a i7 / Xeon Phi Knight's
                            Corner</title>
                        <tgroup cols="3">
                            <colspec colname="c1" colnum="1" colwidth="4.18*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <colspec colname="newCol3" colnum="3" colwidth="2.56*"/>
                            <thead>
                                <row>
                                    <entry>Member</entry>
                                    <entry>parallel_for</entry>
                                    <entry>speedup parallel / serial</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>i7-5960X (8 Cores / 16 Threads)</entry>
                                    <entry>83 ms</entry>
                                    <entry>5.9</entry>
                                </row>
                                <row>
                                    <entry>i7-5960X (8 Cores / 8 Threads)</entry>
                                    <entry>76 ms</entry>
                                    <entry>6.4</entry>
                                </row>
                                <row>
                                    <entry>i7-5960X (8 Cores / 4 Threads)</entry>
                                    <entry>136 ms</entry>
                                    <entry>3.6</entry>
                                </row>
                                <row>
                                    <entry>i7-5960X (8 Cores / 1 Threads)</entry>
                                    <entry>491 ms</entry>
                                    <entry> -</entry>
                                </row>
                                <row>
                                    <entry>Xeon Phi (57 Cores /228 Threads)</entry>
                                    <entry>51 ms</entry>
                                    <entry>42</entry>
                                </row>
                                <row>
                                    <entry>Xeon Phi (57 Cores /114 Threads)</entry>
                                    <entry>84 ms</entry>
                                    <entry>25</entry>
                                </row>
                                <row>
                                    <entry>Xeon Phi (57 Cores /10 Threads)</entry>
                                    <entry>2124 ms</entry>
                                    <entry> -</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>                    
                </para>
            </sect1>
        </chapter>
        <chapter>
            <title>Compiler, linker, settings</title>
            <sect1>
                <title>C++ 11</title>
                <para>Asynchronous is C++11/14-only. Please check that your compiler has C++11
                    enabled (-std=c++0x or -std=c++11 in different versions of gcc). Usually, C++14
                    is recommended.</para>
            </sect1>
            <sect1>
                <title>Supported compilers</title>
                <para>Asynchronous is tested and ok with: <itemizedlist>
                        <listitem>
                            <para>gcc: >= 4.9</para>
                        </listitem>
                        <listitem>
                            <para>clang: >= 3.5</para>
                        </listitem>
                        <listitem>
                            <para>VS2015 with a limitation: BOOST_ASYNC_FUTURE/POST_MEMBER_1(or _2
                                or _3) as variadic macros are not supported</para>
                        </listitem>
                        <listitem>
                            <para>Intel ICC >= 13.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Supported targets</title>
                <para>Asynchronous has been tested on Linux and Windows PCs, Intel and AMD, with the
                    above compilers, and with mingw.</para>
                <para>Asynchronous being based on Boost.Thread, can also work on Intel Xeon Phi with
                    a minor change: within Boost, all usage of boost::shared_ptr must be replaced by
                    std::shared_ptr. Strongly recommended is linking with tbbmalloc_proxy for better
                    performance.</para>
            </sect1>
            <sect1>
                <title>Linking</title>
                <para>Asynchronous is header-only, but requires Boost libraries which are not. One
                    should link with: boost_system, boost_thread, boost_chrono and boost_date_time
                    if logging is required</para>
            </sect1>
            <sect1>
                <title>Compile-time switches</title>
                <para>The following symbols will, when defined, influence the behaviour of the library:<itemizedlist>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_DEFAULT_JOB replaces
                                boost::asynchronous::any_callable by the required job type.</para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_REQUIRE_ALL_ARGUMENTS: forces Asynchronous to
                                only provide servant_proxy macros with all their arguments to avoid
                                accidental forgetting. Precisely:<itemizedlist>
                                    <listitem>
                                        <para>BOOST_ASYNC_FUTURE_MEMBER /BOOST_ASYNC_POST_MEMBER
                                            require priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>BOOST_ASYNC_FUTURE_MEMBER_LOG /
                                            BOOST_ASYNC_POST_MEMBER_LOG require task name and
                                            priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>make_safe_callback requires name and priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>make_lambda_continuation_wrapper requires task
                                            name</para>
                                    </listitem>
                                    <listitem>
                                        <para>parallel algorithms require task name and
                                            priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>asynchronous::vector requires as last arguments name
                                            and priority</para>
                                    </listitem>
                                </itemizedlist></para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_NO_SAVING_CPU_LOAD: overrides default of
                                Asynchronous: schedulers will run at full speed. This can slightly
                                increase speed, at the cost of high CPU load.</para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_PRCTL_SUPPORT: Allows naming of threads if
                                sys/prctl is supported (Linux).</para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_USE_BOOST_SPREADSORT: in older Boost versions,
                                Spreasort was not included. This switch will provide
                                parallel_spreadsort, parallel_quick_spreadsort and
                                parallel_spreadsort_inplace</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
        </chapter>
    </part>
</book>
